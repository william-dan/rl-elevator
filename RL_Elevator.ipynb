{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYurBw6aWzG8"
      },
      "source": [
        "# RL in Elevator Dispatch\n",
        "*Course Project Final Report*\n",
        "\n",
        "*Reinforcement Learning and Decision Making Under Uncertainty,*\n",
        "\n",
        "*University of Neuchatel, Spring 2025*\n",
        "\n",
        "Author: William Dan\n",
        "\n",
        "Student Number:  24-122-749\n",
        "\n",
        "Github URL: https://github.com/william-dan/rl-elevator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s59RnlzKYGub"
      },
      "source": [
        "## Clone the project from Github and install the gym environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0-_tpdKZkfA",
        "outputId": "f159a440-5052-496a-ca85-27caf7e728ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7gYUpDSPHa_",
        "outputId": "a5e86e0e-b767-47ec-a7a0-2c6b8e29bb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rl-elevator'...\n",
            "remote: Enumerating objects: 348, done.\u001b[K\n",
            "remote: Counting objects: 100% (348/348), done.\u001b[K\n",
            "remote: Compressing objects: 100% (243/243), done.\u001b[K\n",
            "remote: Total 348 (delta 151), reused 294 (delta 100), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (348/348), 8.10 MiB | 14.76 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n",
            "/content/rl-elevator\n",
            "Obtaining file:///content/rl-elevator\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from Elevators==0.0.1) (1.1.1)\n",
            "Collecting pre-commit (from Elevators==0.0.1)\n",
            "  Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from Elevators==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->Elevators==0.0.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->Elevators==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->Elevators==0.0.1) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->Elevators==0.0.1) (0.0.4)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->Elevators==0.0.1)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->Elevators==0.0.1)\n",
            "  Downloading identify-2.6.10-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->Elevators==0.0.1)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from pre-commit->Elevators==0.0.1) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->Elevators==0.0.1)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->Elevators==0.0.1)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv>=20.10.0->pre-commit->Elevators==0.0.1) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv>=20.10.0->pre-commit->Elevators==0.0.1) (4.3.8)\n",
            "Downloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading identify-2.6.10-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: Elevators\n",
            "  Building editable for Elevators (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Elevators: filename=elevators-0.0.1-py2.py3-none-any.whl size=1825 sha256=26487b35afdf6b0c339b3cae01824605fec36a73efd8c024e8721092975f06f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1mb9vi4v/wheels/59/1f/7a/34e5adbfd31255fa344ab8d51103ebee0418aabe8c050bcdad\n",
            "Successfully built Elevators\n",
            "Installing collected packages: distlib, virtualenv, nodeenv, identify, cfgv, pre-commit, Elevators\n",
            "Successfully installed Elevators-0.0.1 cfgv-3.4.0 distlib-0.3.9 identify-2.6.10 nodeenv-1.9.1 pre-commit-4.2.0 virtualenv-20.31.2\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/william-dan/rl-elevator.git\n",
        "%cd /content/rl-elevator\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIl68ZBCYrUG"
      },
      "source": [
        "## Prepare for the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NBuqoRCrPUlo"
      },
      "outputs": [],
      "source": [
        "# define all the libs here\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from Solver.LOOK import LOOKSolver\n",
        "import random\n",
        "import Elevators\n",
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ez3Uwa4FQa0c"
      },
      "outputs": [],
      "source": [
        "# some utils functions\n",
        "def get_device(device_id):\n",
        "    return torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def make_reproducible(seed=0):\n",
        "    \"\"\"Sets random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def running_mean(x, N):\n",
        "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
        "\n",
        "def action_flatten_to_matrix(action, num_floors, num_cars):\n",
        "    return (action % (num_floors+1), action // (num_floors+1))\n",
        "\n",
        "def action_matrix_to_flatten(action, num_floors, num_cars):\n",
        "    return action[1] * (num_floors+1) + action[0]\n",
        "\n",
        "def render_subplot(figList):\n",
        "  \"\"\"\n",
        "  figList is a list of tuples (name, data)\n",
        "  name is the name of the subplot\n",
        "  data is the data to be plotted\n",
        "  data: (x, [(y, label, xlabel, ylabel, title)])\n",
        "  \"\"\"\n",
        "  WIDTH = 3\n",
        "\n",
        "  # fig.delaxes(axes[1][2])\n",
        "  # Initialize a figure and axes for the subplots\n",
        "  fig, axes = plt.subplots(len(figList) // WIDTH + 1, WIDTH, figsize=(WIDTH * 6, (len(figList) // WIDTH + 1) * 6))\n",
        "  axes = axes.flatten()\n",
        "  for i, (name, data) in enumerate(figList):\n",
        "    x, yList = data\n",
        "    for y, label, xlabel, ylabel, title in yList:\n",
        "      axes[i].plot(x, y, label=label)\n",
        "      axes[i].set_xlabel(xlabel)\n",
        "      axes[i].set_ylabel(ylabel)\n",
        "      axes[i].set_title(title)\n",
        "      axes[i].legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "device = get_device(0)\n",
        "make_reproducible(0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, state_size, n_outputs, hidden_dim_size):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.n_outputs = n_outputs\n",
        "        self.hidden_dim_size = hidden_dim_size\n",
        "\n",
        "        # Shared backbone - fully connected layers for the preprocessed state\n",
        "        self.shared_backbone = nn.Sequential(\n",
        "            nn.Linear(state_size, self.hidden_dim_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim_size, self.hidden_dim_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim_size, self.hidden_dim_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim_size, self.n_outputs)\n",
        "        )\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        # Pass through the shared backbone\n",
        "        shared_features = self.shared_backbone(state)\n",
        "\n",
        "        # Policy head\n",
        "        logits = self.policy_head(shared_features)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # Value head\n",
        "        value = self.value_head(shared_features)\n",
        "\n",
        "        return probs, log_probs, value"
      ],
      "metadata": {
        "id": "HyGhZH3gbQTg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHyHAUzXX62o"
      },
      "source": [
        "## Policy Gradient with baseline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import isnan\n",
        "\n",
        "# prompt: Use curriculum learning in RL\n",
        "\n",
        "class PolicyGradientWithBaseline(object):\n",
        "  def __init__(self, env, N, M):\n",
        "    self.N = N\n",
        "    self.M = M\n",
        "    self.env = env\n",
        "    # self.net = CNN().to(device)\n",
        "    self.ac = ActorCriticNetwork(state_size=N*M*5, n_outputs=(N+1)*M, hidden_dim_size=256).to(device)\n",
        "    self.optimizer = torch.optim.Adam(params=self.ac.parameters(), lr=2e-5)\n",
        "    # self.policy = PolicyNetwork(n_inputs=N*M*5, n_outputs=(N+1)*M, hidden_dim_size=256).to(device)\n",
        "    # self.value = ValueNetwork(num_states=N*M*5, hidden_dim=256).to(device)\n",
        "    # self.policy_optimizer = torch.optim.Adam(params=self.policy.parameters(), lr=5e-5)\n",
        "    # self.value_optimizer = torch.optim.Adam(params=self.value.parameters(), lr=1e-3)\n",
        "    # placeholders for rewards for each episode\n",
        "    self.rewards = []\n",
        "    self.losses =  []\n",
        "    self.steps = []\n",
        "    # self.policy_losses = []\n",
        "    # self.value_losses = []\n",
        "    self.teacher_model = LOOKSolver(self.env)\n",
        "\n",
        "  def save_model(self, ac_path=\"/content/drive/MyDrive/PG-ac.pth\", optimizer_path=\"/content/drive/MyDrive/PG-optimizer.pth\"):\n",
        "    torch.save(self.ac.state_dict(), ac_path)\n",
        "    torch.save(self.optimizer.state_dict(), optimizer_path)\n",
        "    # torch.save(self.policy.state_dict(), \"/content/drive/MyDrive/PG-policy.pth\")\n",
        "    # torch.save(self.value.state_dict(), \"/content/drive/MyDrive/PG-value.pth\")\n",
        "    # torch.save(self.policy_optimizer.state_dict(), \"/content/drive/MyDrive/PG-policy-optimizer.pth\")\n",
        "    # torch.save(self.value_optimizer.state_dict(), \"/content/drive/MyDrive/PG-value-optimizer.pth\")\n",
        "\n",
        "  def load_model(self, ac_path=\"/content/drive/MyDrive/PG-ac.pth\", optimizer_path=\"/content/drive/MyDrive/PG-optimizer.pth\"):\n",
        "    # self.policy.load_state_dict(torch.load(\"/content/drive/MyDrive/PG-policy.pth\"))\n",
        "    # self.value.load_state_dict(torch.load(\"/content/drive/MyDrive/PG-value.pth\"))\n",
        "    # self.policy_optimizer.load_state_dict(torch.load(\"/content/drive/MyDrive/PG-policy-optimizer.pth\"))\n",
        "    # self.value_optimizer.load_state_dict(torch.load(\"/content/drive/MyDrive/PG-value-optimizer.pth\"))\n",
        "    self.ac.load_state_dict(torch.load(ac_path))\n",
        "    self.optimizer.load_state_dict(torch.load(optimizer_path))\n",
        "\n",
        "  def train(self, num_trajectories=600, max_episode_length=400, gamma=0.99, is_curriculum=True, curriculum_start_prob=1.0, curriculum_end_prob=0.1):\n",
        "    # iterating through trajectories\n",
        "    for tau in tqdm(range(num_trajectories)):\n",
        "      # Calculate the current teacher probability\n",
        "      teacher_prob = curriculum_start_prob - (tau / (num_trajectories - 1)) * (curriculum_start_prob - curriculum_end_prob)\n",
        "      teacher_prob = max(curriculum_end_prob, teacher_prob) # Ensure it doesn't go below the end probability\n",
        "\n",
        "      # resetting the environment\n",
        "      state, info = self.env.reset()\n",
        "      self.teacher_model.reset(info)\n",
        "\n",
        "      # setting done to False for while loop\n",
        "      done = False\n",
        "      # storing trajectory and logπ(a_t|s_t, θ)\n",
        "      transition_buffer = []\n",
        "      log_probs = []\n",
        "      state_values =[]\n",
        "\n",
        "      t = 0\n",
        "      clk = 0.0\n",
        "      while done == False and info[\"time\"] < max_episode_length:\n",
        "          # features = self.net(torch.tensor(state).to(device).permute(2, 0, 1).unsqueeze(0))\n",
        "          # retrieving π and logπ\n",
        "          action_probs, action_log_probs, state_value = self.ac(torch.tensor(state).to(device).flatten())\n",
        "          # action_probs, action_log_probs = self.policy(preprocess_state(torch.tensor(state).to(device).flatten()))\n",
        "          if np.isnan(action_log_probs.detach().cpu().sum()):\n",
        "            print(\"action_log_probs.sum() is nan\")\n",
        "            raise ValueError(\"action_log_probs.sum() is nan\")\n",
        "          # retrieving value of the state we're currently in\n",
        "          # state_value = self.value(preprocess_state(torch.tensor(state).to(device).flatten()))\n",
        "\n",
        "          if is_curriculum and random.random() < teacher_prob:\n",
        "            # Use the teacher's action\n",
        "            action = self.teacher_model.get_next_action((state, info))\n",
        "          else:\n",
        "            # Sample action from the learned policy\n",
        "            action_probs_np = action_probs.flatten().detach().cpu().numpy()\n",
        "            action = np.random.choice(np.arange(len(action_probs_np)), p=action_probs_np)\n",
        "            action = action_flatten_to_matrix(action, self.N, self.M)\n",
        "\n",
        "          # keeping track of previous state\n",
        "          prev_state = state\n",
        "          # environment step\n",
        "\n",
        "          state, reward, done, truncation, info = self.env.step(action)\n",
        "          # storing rewards, logπ and state values\n",
        "          transition_buffer.append(reward)\n",
        "          log_probs.append(action_log_probs[action_matrix_to_flatten(action, self.N, self.M)])\n",
        "          state_values.append(state_value)\n",
        "          t += 1\n",
        "      # logging the episode length as a cumulative reward\n",
        "      self.rewards.append(sum(transition_buffer))\n",
        "      returns = []\n",
        "      for t_prime in range(t):\n",
        "          # computing discounted rewards in future for every timestep\n",
        "          G = 0\n",
        "          for i, tick in enumerate(transition_buffer[t_prime:]):\n",
        "              G += (gamma ** i) * tick\n",
        "          returns.append(G)\n",
        "\n",
        "      # turning the returns vector into a tensor\n",
        "      returns = torch.tensor(returns).to(device)\n",
        "      # creating the advantage term δ\n",
        "      deltas = returns - torch.tensor(state_values).to(device)\n",
        "\n",
        "      # computing the gradients for each timestep\n",
        "      policy_gradients = []\n",
        "      value_gradients = []\n",
        "\n",
        "      # policy_deltas = deltas\n",
        "      # value_deltas = deltas.clone()\n",
        "      # using Sutton & Barto losses for value function and policy\n",
        "      for t, (log_prob, d) in enumerate(zip(log_probs, deltas)):\n",
        "          policy_gradients.append(- log_prob * d)\n",
        "      for d, V in zip(deltas, state_values):\n",
        "          value_gradients.append( - d * V)\n",
        "      self.steps.append(t)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss = torch.stack(policy_gradients).sum() + torch.stack(value_gradients).sum()\n",
        "      self.losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(self.ac.parameters(), 0.5)\n",
        "      self.optimizer.step()\n",
        "      # # updating the policy network\n",
        "      # self.policy_optimizer.zero_grad()\n",
        "      # # summing all gradients for one batch update instead of update at each timestep\n",
        "      # policy_loss = torch.stack(policy_gradients).sum()\n",
        "      # self.policy_losses.append(policy_loss.item())\n",
        "      # policy_loss.backward()\n",
        "      # torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
        "      # self.policy_optimizer.step()\n",
        "      # # updating the value network\n",
        "      # self.value_optimizer.zero_grad()\n",
        "      # value_loss = torch.stack(value_gradients).sum()\n",
        "      # self.value_losses.append(value_loss.item())\n",
        "      # value_loss.backward()\n",
        "      # torch.nn.utils.clip_grad_norm_(self.value.parameters(), 0.5)\n",
        "\n",
        "      # self.value_optimizer.step()\n",
        "\n",
        "  def plot_rewards_and_losses(self, interval):\n",
        "    # plot the results of the training\n",
        "    plt.figure(figsize=(12,9))\n",
        "    plt.plot(running_mean(self.rewards,interval))\n",
        "    plt.grid()\n",
        "    plt.title(\"REINFORCE cumulative rewards\")\n",
        "    plt.figure(figsize=(12,9))\n",
        "    plt.plot(running_mean(self.losses,interval))\n",
        "    plt.grid()\n",
        "    plt.title(\"REINFORCE losses\")\n",
        "    # plt.figure(figsize=(12,9))\n",
        "    # plt.plot(running_mean(self.policy_losses,interval))\n",
        "    # plt.grid()\n",
        "    # plt.title(\"REINFORCE policy losses\")\n",
        "    # plt.figure(figsize=(12,9))\n",
        "    # plt.plot(running_mean(self.value_losses,interval))\n",
        "    # plt.grid()\n",
        "    # plt.title(\"REINFORCE value losses\")\n",
        "\n",
        "  def export_rewards_to_csv(self):\n",
        "    # Export rewards to CSV\n",
        "    with open(f'/content/drive/MyDrive/PG_Baseline_rewards_{self.N}_{self.M}.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Episode', 'Reward'])\n",
        "        for episode, reward in enumerate(self.rewards):\n",
        "            writer.writerow([episode, reward])\n",
        "\n",
        "  def export_losses_to_csv(self):\n",
        "    # Export losses to CSV\n",
        "    with open(f'/content/drive/MyDrive/PG_Baseline_losses_{self.N}_{self.M}.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Episode', 'Loss'])\n",
        "        for episode, loss in enumerate(self.losses):\n",
        "            writer.writerow([episode, loss])\n",
        "\n",
        "\n",
        "  def export_steps_to_csv(self):\n",
        "    # Export losses to CSV\n",
        "    with open(f'/content/drive/MyDrive/PG_Baseline_steps_{self.N}_{self.M}.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Episode', 'Steps'])\n",
        "        for episode, steps in enumerate(self.steps):\n",
        "            writer.writerow([episode, steps])\n",
        "\n",
        "\n",
        "\n",
        "  def get_next_action(self, obs):\n",
        "    action_probs, _, _ = self.ac(torch.tensor(obs).to(device).flatten())\n",
        "    # action_probs, _ = self.policy(torch.tensor(obs).to(device).flatten())\n",
        "    action_probs_np = action_probs.flatten().detach().cpu().numpy()\n",
        "    action = np.random.choice(np.arange(len(action_probs_np)), p=action_probs_np)\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "R7yMlutmnea5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver():\n",
        "    def __init__(self, env: gym.Env, model, N, M):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def get_next_action(self, obs):\n",
        "        return self.model.get_next_action(obs)\n",
        "\n",
        "\n",
        "    def run_episode(self, max_steps=100, render=False):\n",
        "        obs, info = self.env.reset()\n",
        "        self.reset()\n",
        "        total_reward = 0\n",
        "        # for _ in range(max_steps):\n",
        "        while info[\"time\"] < max_steps:\n",
        "            action = self.get_next_action(obs)\n",
        "            obs, reward, done, truncated, info = self.env.step(action_flatten_to_matrix(action, self.N, self.M))\n",
        "            if render:\n",
        "              self.env.render()\n",
        "            total_reward += reward\n",
        "            if done or truncated:\n",
        "                break\n",
        "        print(f\"info[\\\"done\\\"]: {info['done']}\")\n",
        "        # print(f\"info[\\\"waiting\\\"]: {info['waiting']}\")\n",
        "        # print(f\"info[\\\"time\\\"]: {info['time']}\")\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def plot(self, rewards):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(rewards)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Reward')\n",
        "        plt.title('PGWithBaselineSolver Performance')\n",
        "        plt.savefig(\"PGWithBaseline_performance.png\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def benchmark(self, num_episodes=100, render=False):\n",
        "        rewards = []\n",
        "        for _ in range(num_episodes):\n",
        "            total_reward = self.run_episode(render=render)\n",
        "            rewards.append(total_reward)\n",
        "        return rewards"
      ],
      "metadata": {
        "id": "k2HaBJxSbFoZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_test(model_class, N, M, seed=0):\n",
        "  env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                   num_floors=N,\n",
        "                   num_cars=M,\n",
        "                   avg_passengers_spawning_time=5,\n",
        "                   total_passengers=1000000,\n",
        "                   seed=seed)\n",
        "  model = model_class(env, N, M)\n",
        "  model.load_model(ac_path=f\"/content/drive/MyDrive/{model_class.__name__}-ac-bad-{N}-{M}.pth\", optimizer_path=f\"/content/drive/MyDrive/{model_class.__name__}-optimizer-bad-{N}-{M}.pth\")\n",
        "  solver = Solver(env, model, N, M)\n",
        "  rewards = solver.benchmark(num_episodes=1, render=True)\n",
        "  print(rewards)"
      ],
      "metadata": {
        "id": "SR3WY-hsJmYg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rewards_test(model_class, N, M):\n",
        "  env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                   num_floors=N,\n",
        "                   num_cars=M,\n",
        "                   avg_passengers_spawning_time=5,\n",
        "                   total_passengers=1000000,\n",
        "                   seed=0)\n",
        "  model = model_class(env, N, M)\n",
        "  model.load_model(ac_path=f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\", optimizer_path=f\"/content/drive/MyDrive/{model_class.__name__}-optimizer-{N}-{M}.pth\")\n",
        "  solver = Solver(env, model, N, M)\n",
        "  rewards = solver.benchmark(num_episodes=1000)\n",
        "  with open(f'/content/drive/MyDrive/{model_class.__name__}_test_rewards_{N}_{M}.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow(['Episode', 'Reward'])\n",
        "      for episode, reward in enumerate(rewards):\n",
        "          writer.writerow([episode, reward])\n"
      ],
      "metadata": {
        "id": "SK05fdo0F6Q9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark(model_class, N, M, epochs=2000):\n",
        "  env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                   num_floors=N,\n",
        "                   num_cars=M,\n",
        "                   avg_passengers_spawning_time=5,\n",
        "                   total_passengers=1000000,\n",
        "                   seed=0)\n",
        "  # make_reproducible(30)\n",
        "  model = model_class(env, N, M)\n",
        "\n",
        "  # if os.path.exists(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\"):\n",
        "  #   model.load_model(ac_path=f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\", optimizer_path=f\"/content/drive/MyDrive/{model_class.__name__}-optimizer-{N}-{M}.pth\")\n",
        "\n",
        "\n",
        "  model.train(num_trajectories=epochs, max_episode_length=100, is_curriculum=False)\n",
        "  model.save_model(ac_path=f\"/content/drive/MyDrive/{model_class.__name__}-ac-bad-{N}-{M}.pth\", optimizer_path=f\"/content/drive/MyDrive/{model_class.__name__}-optimizer-bad-{N}-{M}.pth\")\n",
        "  model.plot_rewards_and_losses(100)\n",
        "  model.export_losses_to_csv()\n",
        "  model.export_rewards_to_csv()\n",
        "  model.export_steps_to_csv()\n",
        "\n",
        "  env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                   num_floors=N,\n",
        "                   num_cars=M,\n",
        "                   avg_passengers_spawning_time=5,\n",
        "                   total_passengers=1000000,\n",
        "                   seed=0)\n",
        "  # Initialize the LOOK solver\n",
        "  look_solver = LOOKSolver(env)\n",
        "\n",
        "  # Run a simulation using the LOOK solver\n",
        "  num_steps = 100\n",
        "  total_reward_look = 0\n",
        "  state, info = env.reset()\n",
        "  look_solver.reset(info)\n",
        "\n",
        "  print(\"Running LOOK benchmark...\")\n",
        "  while info[\"time\"] < num_steps:\n",
        "      action = look_solver.get_next_action((state, info))\n",
        "      state, reward, done, truncation, info = env.step(action)\n",
        "      # look_solver.env.render()\n",
        "      total_reward_look += reward\n",
        "      if done or truncation:\n",
        "          break\n",
        "\n",
        "  print(info[\"done\"])\n",
        "  print(f\"\\nTotal reward for LOOK benchmark over {num_steps} steps: {total_reward_look}\")\n",
        "\n",
        "  avg_reward = -1e9\n",
        "  t = 0\n",
        "  while avg_reward < total_reward_look and t < 100:\n",
        "    t += 1\n",
        "    env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                    num_floors=N,\n",
        "                    num_cars=M,\n",
        "                    avg_passengers_spawning_time=5,\n",
        "                    total_passengers=1000000,\n",
        "                    #  capacity=4,\n",
        "                    seed=0)\n",
        "    model = model_class(env, N, M)\n",
        "    model.load_model(ac_path=f\"/content/drive/MyDrive/{model_class.__name__}-ac-bad-{N}-{M}.pth\", optimizer_path=f\"/content/drive/MyDrive/{model_class.__name__}-optimizer-bad-{N}-{M}.pth\")\n",
        "    solver = Solver(env, model, N, M)\n",
        "    rewards = solver.benchmark(num_episodes=1)\n",
        "    avg_reward = sum(rewards) / len(rewards)\n",
        "\n",
        "  print(f\"Average reward: {avg_reward}\")\n",
        "  solver.plot(rewards)"
      ],
      "metadata": {
        "id": "0dffDDbaaKcQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ECgK5-U357e"
      },
      "outputs": [],
      "source": [
        "benchmark(PolicyGradientWithBaseline,3,1,100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(PolicyGradientWithBaseline,6,1,100)"
      ],
      "metadata": {
        "id": "vzTX5oD7oYCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_test(PolicyGradientWithBaseline,3,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WGGvtyaoGoJX",
        "outputId": "906f246b-70e1-4ed1-fb5f-dc12fd5d44b3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 122\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 100\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 82\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 82\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 92\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 86\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 100\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 102\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 85\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 106\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 82\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 104\n",
            "info[\"done\"]: 87\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 97\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 102\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 91\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 80\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 87\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 97\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 104\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 101\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 105\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 111\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 80\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 100\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 85\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 86\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 84\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 103\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 84\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 84\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 88\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 97\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 132\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 82\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 99\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 95\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 92\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 106\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 99\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 85\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 97\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 80\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 84\n",
            "info[\"done\"]: 111\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 92\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 98\n",
            "info[\"done\"]: 95\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 90\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 93\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 107\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 108\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 96\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 92\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 101\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 93\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 93\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 111\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_test(PolicyGradientWithBaseline,6,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uIZhsTK7Gkzp",
        "outputId": "04520a60-2dfc-4de0-8d65-86a67baba3e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "render_test(PolicyGradientWithBaseline,3,1,0)"
      ],
      "metadata": {
        "id": "yFCcvUChJ6Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "render_test(PolicyGradientWithBaseline,6,1,0)"
      ],
      "metadata": {
        "id": "M40HrBVKJ7xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYMKK3AS9ppj"
      },
      "source": [
        "## 1 step Actor Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "a_a3vLyykW6h"
      },
      "outputs": [],
      "source": [
        "class ActorCriticOneStep(object):\n",
        "  def __init__(self, env, N, M):\n",
        "    self.N = N\n",
        "    self.M = M\n",
        "    self.ac = ActorCriticNetwork(state_size=N*M*5, n_outputs=(N+1)*M, hidden_dim_size=256).to(device)\n",
        "    self.optimizer = torch.optim.Adam(params=self.ac.parameters(), lr=2e-5)\n",
        "    self.env = env\n",
        "    self.rewards = []\n",
        "    self.losses = []\n",
        "    self.steps = []\n",
        "\n",
        "  def save_model(self, ac_path, optimizer_path):\n",
        "    torch.save(self.ac.state_dict(), ac_path)\n",
        "    torch.save(self.optimizer.state_dict(), optimizer_path)\n",
        "\n",
        "  def load_model(self, ac_path, optimizer_path):\n",
        "    self.ac.load_state_dict(torch.load(ac_path))\n",
        "    self.optimizer.load_state_dict(torch.load(optimizer_path))\n",
        "\n",
        "  def train(self, num_trajectories=500, max_episode_length=100, gamma=0.99, is_curriculum=False):\n",
        "    # iterating through trajectories\n",
        "    for tau in tqdm(range(num_trajectories)):\n",
        "        # resetting the environment\n",
        "        state, info = self.env.reset(seed=123)\n",
        "        # setting done to False for while loop\n",
        "        done = False\n",
        "\n",
        "        t = 0\n",
        "        rewardSum = 0.0\n",
        "        losses_per_episode = []\n",
        "        while done == False and info[\"time\"] < max_episode_length:\n",
        "            # retrieving π and logπ\n",
        "            action_probs, action_log_probs, state_value = self.ac(torch.tensor(state).to(device).flatten())\n",
        "            if np.isnan(action_log_probs.detach().cpu().sum()):\n",
        "              print(\"action_log_probs.sum() is nan\")\n",
        "              raise ValueError(\"action_log_probs.sum() is nan\")\n",
        "\n",
        "            # sampling the action according to the distribution given by π\n",
        "            action_probs_np = action_probs.flatten().detach().cpu().numpy()\n",
        "            action = np.random.choice(np.arange(len(action_probs_np)), p=action_probs_np)\n",
        "            # print(action, action_formatted)\n",
        "            # keeping track of previous state\n",
        "            prev_state = state\n",
        "            # environment step\n",
        "            state, reward, done, truncation, info = self.env.step(action_flatten_to_matrix(action, self.N, self.M))\n",
        "            # getting the value of next state - no value if the state is terminal\n",
        "            if done == False:\n",
        "                _, _, next_state_value = self.ac(torch.tensor(state).to(device).flatten())\n",
        "            else:\n",
        "                next_state_value = torch.tensor(0.).to(device)\n",
        "\n",
        "            # delta computation\n",
        "            reward = torch.tensor(reward).to(device)\n",
        "            rewardSum += reward.item()\n",
        "            # detaching the next state gradients for the value function update\n",
        "            delta = reward + gamma * next_state_value.detach() - state_value\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = delta ** 2 - delta * action_log_probs[action]\n",
        "            # loss = delta\n",
        "            losses_per_episode.append(loss.item())\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.ac.parameters(), 0.5)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            t += 1\n",
        "        self.losses.append(torch.Tensor(losses_per_episode).mean().item())\n",
        "        self.steps.append(t)\n",
        "        self.rewards.append(rewardSum)\n",
        "\n",
        "  def plot_rewards_and_losses(self, interval):\n",
        "    # plot the results of the training\n",
        "    plt.figure(figsize=(12,9))\n",
        "    plt.plot(running_mean(self.rewards,interval))\n",
        "    plt.grid()\n",
        "    plt.title(\"REINFORCE cumulative rewards\")\n",
        "    plt.figure(figsize=(12,9))\n",
        "    plt.plot(running_mean(self.losses,interval))\n",
        "    plt.grid()\n",
        "    plt.title(\"REINFORCE losses\")\n",
        "\n",
        "  def export_rewards_to_csv(self):\n",
        "    # Export rewards to CSV\n",
        "    with open(f'/content/drive/MyDrive/AC_rewards_{self.N}_{self.M}.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Episode', 'Reward'])\n",
        "        for episode, reward in enumerate(self.rewards):\n",
        "            writer.writerow([episode, reward])\n",
        "\n",
        "  def export_losses_to_csv(self):\n",
        "    # Export losses to CSV\n",
        "    with open(f'/content/drive/MyDrive/AC_losses_{self.N}_{self.M}.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Episode', 'Loss'])\n",
        "        for episode, loss in enumerate(self.losses):\n",
        "            writer.writerow([episode, loss])\n",
        "\n",
        "  def export_steps_to_csv(self):\n",
        "    # Export losses to CSV\n",
        "    with open(f'/content/drive/MyDrive/AC_steps_{self.N}_{self.M}.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Episode', 'Steps'])\n",
        "        for episode, steps in enumerate(self.steps):\n",
        "            writer.writerow([episode, steps])\n",
        "\n",
        "\n",
        "\n",
        "  def get_next_action(self, obs):\n",
        "    action_probs, action_log_probs, state_value = self.ac(torch.tensor(obs).to(device).flatten())\n",
        "    action_probs_np = action_probs.flatten().detach().cpu().numpy()\n",
        "    action = np.random.choice(np.arange(len(action_probs_np)), p=action_probs_np)\n",
        "    return action\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_K2t_xgnWCq"
      },
      "outputs": [],
      "source": [
        "benchmark(ActorCriticOneStep, 3, 1, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(ActorCriticOneStep, 6, 1, 100)"
      ],
      "metadata": {
        "id": "duPE7WFhp9ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_test(ActorCriticOneStep, 3, 1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OVQVmzQ9GrfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_test(ActorCriticOneStep, 6, 1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nRstb0CXGuWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "render_test(ActorCriticOneStep, 3, 1,0)"
      ],
      "metadata": {
        "id": "A5UgENCtKAjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "render_test(ActorCriticOneStep, 6, 1, 0)"
      ],
      "metadata": {
        "id": "EffnvRBAKFdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]\n",
        "from stable_baselines3 import PPO as SB3_PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "# from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.logger import configure\n"
      ],
      "metadata": {
        "id": "vzvoxZ6Es__X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SB3PPOSolver():\n",
        "    def __init__(self, env: gym.Env, model):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def get_next_action(self, obs):\n",
        "        # SB3 model expects a batch of observations, even for a single environment step during evaluation\n",
        "        action, _states = self.model.predict(obs, deterministic=False)\n",
        "        # print(action)\n",
        "        # SB3 predicts flattened actions for MultiDiscrete. We need to convert it back\n",
        "        # return action_flatten_to_matrix(action.item(), N, M)\n",
        "        return action\n",
        "\n",
        "    def run_episode(self, max_steps=100, render=False):\n",
        "        obs, info = self.env.reset()\n",
        "        self.reset()\n",
        "        total_reward = 0\n",
        "        while info[\"time\"] < max_steps:\n",
        "            action = self.get_next_action(obs)\n",
        "            obs, reward, done, truncated, info = self.env.step(action) # SB3 model directly gives the action in the environment's action space\n",
        "            total_reward += reward\n",
        "            if render:\n",
        "              self.env.render()\n",
        "            if done or truncated:\n",
        "                break\n",
        "        print(f\"info[\\\"done\\\"]: {info['done']}\")\n",
        "        return total_reward\n",
        "\n",
        "    def plot(self, rewards):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(rewards)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Reward')\n",
        "        plt.title('SB3 PPOSolver Performance')\n",
        "        plt.savefig(\"SB3_PPO_performance.png\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def benchmark(self, num_episodes=100, render=False):\n",
        "        rewards = []\n",
        "        for _ in range(num_episodes):\n",
        "            total_reward = self.run_episode(max_steps=100, render=render)\n",
        "            rewards.append(total_reward)\n",
        "        return rewards"
      ],
      "metadata": {
        "id": "kBkZPEr_m69Z"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpisodeRewardCallback(BaseCallback):\n",
        "    \"\"\"Collect per‑episode rewards and push them wherever you like.\"\"\"\n",
        "    def __init__(self, verbose: int = 0, n_steps = 2048, N=3, M=1):\n",
        "        super().__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.n_steps = n_steps\n",
        "        self.current_step = 0\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # print(f\"on_step\")\n",
        "        # `infos` is a list (one per env if vectorized)\n",
        "        self.current_step += 1\n",
        "        if self.current_step % self.n_steps == 0:\n",
        "\n",
        "          for info in self.locals[\"infos\"]:\n",
        "\n",
        "              if \"reward\" in info:               # signals an episode just ended\n",
        "                  r = info[\"reward\"]        # total reward\n",
        "                  self.episode_rewards.append(r)   # stash locally\n",
        "\n",
        "                  # 👉 Log to SB3's logger so it shows up in TensorBoard\n",
        "                  self.logger.record(\"episode_reward\", r)\n",
        "\n",
        "                  # 👉 Or: write to disk / DB / live plot here\n",
        "                  if self.verbose:\n",
        "                      print(f\"Episode done — reward = {r}\")\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        save the rewards to csv file\n",
        "        \"\"\"\n",
        "        # Export rewards to CSV\n",
        "        with open(f'/content/drive/MyDrive/PPO_rewards_{self.N}_{self.M}.csv', 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['Episode', 'Reward'])\n",
        "            for episode, reward in enumerate(self.episode_rewards):\n",
        "                writer.writerow([episode, reward])\n"
      ],
      "metadata": {
        "id": "tUKfMksfsM9r"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_test_ppo(model_class, N, M, seed=0):\n",
        "  threshold = 1200\n",
        "  max_times = 100\n",
        "  for i in range(max_times):\n",
        "    env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                    num_floors=N,\n",
        "                    num_cars=M,\n",
        "                    avg_passengers_spawning_time=5,\n",
        "                    total_passengers=1000000,\n",
        "                    seed=seed)\n",
        "    model = model_class(\"MlpPolicy\", env, verbose=1, device=device)\n",
        "    model.load(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\")\n",
        "    sb3_ppo_solver = SB3PPOSolver(env, model)\n",
        "\n",
        "\n",
        "    rewards = sb3_ppo_solver.benchmark(num_episodes=1, render=True)\n",
        "    assert(len(rewards) == 1)\n",
        "    if rewards[0] >= threshold:\n",
        "      break\n",
        "  print(rewards)\n",
        "\n"
      ],
      "metadata": {
        "id": "I9CexpKZNNXY"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rewards_test_ppo(model_class, N, M):\n",
        "  env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                   num_floors=N,\n",
        "                   num_cars=M,\n",
        "                   avg_passengers_spawning_time=5,\n",
        "                   total_passengers=1000000,\n",
        "                   seed=0)\n",
        "  model = model_class(\"MlpPolicy\", env, verbose=1, device=device)\n",
        "  model.load(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\")\n",
        "  sb3_ppo_solver = SB3PPOSolver(env, model)\n",
        "  rewards = sb3_ppo_solver.benchmark(num_episodes=1000)\n",
        "  with open(f'/content/drive/MyDrive/{model_class.__name__}_test_rewards_{N}_{M}.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow(['Episode', 'Reward'])\n",
        "      for episode, reward in enumerate(rewards):\n",
        "          writer.writerow([episode, reward])\n"
      ],
      "metadata": {
        "id": "PB8vsas5H370"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def benchmark_ppo(model_class, N, M):\n",
        "  n_steps = 100\n",
        "  env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                   num_floors=N,\n",
        "                   num_cars=M,\n",
        "                   avg_passengers_spawning_time=5,\n",
        "                   total_passengers=1000000,\n",
        "                   seed=0)\n",
        "\n",
        "\n",
        "  new_logger = configure(f\"/content/drive/MyDrive/ppo-{N}-{M}-log\", [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "\n",
        "  model = model_class(\"MlpPolicy\", env, verbose=1, device=device, n_steps = n_steps,\n",
        "                      tensorboard_log=f\"/content/drive/MyDrive/ppo-tensorboard-{N}-{M}/\")\n",
        "\n",
        "  # if os.path.exists(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\"):\n",
        "  #   model.load(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\")\n",
        "\n",
        "\n",
        "  model.set_logger(new_logger)\n",
        "  # # Adjust the total_timesteps as needed\n",
        "  model.learn(\n",
        "      total_timesteps=2e5,\n",
        "      callback=EpisodeRewardCallback(verbose=1, n_steps=n_steps, N=N, M=M)\n",
        "  )\n",
        "\n",
        "  # Save the trained model\n",
        "  model.save(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\")\n",
        "\n",
        "  # solver = SB3PPOSolver(vec_env, model)\n",
        "  # rewards = solver.benchmark(num_episodes=100)\n",
        "\n",
        "\n",
        "\n",
        "  env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                   num_floors=N,\n",
        "                   num_cars=M,\n",
        "                   avg_passengers_spawning_time=5,\n",
        "                   total_passengers=1000000,\n",
        "                   seed=0)\n",
        "  # Initialize the LOOK solver\n",
        "  look_solver = LOOKSolver(env)\n",
        "\n",
        "  # Run a simulation using the LOOK solver\n",
        "  num_steps = 100\n",
        "  total_reward_look = 0\n",
        "  state, info = env.reset()\n",
        "  look_solver.reset(info)\n",
        "\n",
        "  print(\"Running LOOK benchmark...\")\n",
        "  while info[\"time\"] < num_steps:\n",
        "      action = look_solver.get_next_action((state, info))\n",
        "      state, reward, done, truncation, info = env.step(action)\n",
        "      # look_solver.env.render()\n",
        "      total_reward_look += reward\n",
        "      if done or truncation:\n",
        "          break\n",
        "\n",
        "  print(info[\"done\"])\n",
        "  print(f\"\\nTotal reward for LOOK benchmark over {num_steps} steps: {total_reward_look}\")\n",
        "\n",
        "  avg_reward = 0\n",
        "  t = 0\n",
        "  while avg_reward < total_reward_look and t < 100:\n",
        "    t += 1\n",
        "    env = gym.make(\"Elevators/Elevators-v0\",\n",
        "                    num_floors=N,\n",
        "                    num_cars=M,\n",
        "                    avg_passengers_spawning_time=5,\n",
        "                    total_passengers=1000000,\n",
        "                    #  capacity=4,\n",
        "                    seed=0)\n",
        "    assert os.path.exists(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\")\n",
        "    model = model_class.load(f\"/content/drive/MyDrive/{model_class.__name__}-ac-{N}-{M}.pth\", optimizer_path=f\"/content/drive/MyDrive/{model_class.__name__}-optimizer-{N}-{M}.pth\")\n",
        "    sb3_ppo_solver = SB3PPOSolver(env, model)\n",
        "    sb3_ppo_rewards = sb3_ppo_solver.benchmark(num_episodes=1)\n",
        "    avg_reward = sum(sb3_ppo_rewards) / len(sb3_ppo_rewards)\n",
        "\n",
        "  print(f\"Average reward: {avg_reward}\")\n",
        "  sb3_ppo_solver.plot(sb3_ppo_rewards)"
      ],
      "metadata": {
        "id": "-ZO6KVffssLR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_ppo(SB3_PPO, 3, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MZGmy1KOnOqH",
        "outputId": "ff428f0b-454d-493b-b6d7-4fd00af819b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/drive/MyDrive/ppo-3-1-log\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 100`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
            "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
            "Info: (n_steps=100 and n_envs=1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "|    time_elapsed         | 616           |\n",
            "|    total_timesteps      | 175100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.0574643e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.383        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.38e+04      |\n",
            "|    n_updates            | 17500         |\n",
            "|    policy_gradient_loss | -0.000862     |\n",
            "|    value_loss           | 1.03e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4954341.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.95e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1752          |\n",
            "|    time_elapsed         | 617           |\n",
            "|    total_timesteps      | 175200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2084814e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.422        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.28e+04      |\n",
            "|    n_updates            | 17510         |\n",
            "|    policy_gradient_loss | -0.000169     |\n",
            "|    value_loss           | 8.84e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4956889.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 4.96e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1753         |\n",
            "|    time_elapsed         | 617          |\n",
            "|    total_timesteps      | 175300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008820673 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.389       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.94e+04     |\n",
            "|    n_updates            | 17520        |\n",
            "|    policy_gradient_loss | -0.00373     |\n",
            "|    value_loss           | 5.74e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4960284.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.96e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1754          |\n",
            "|    time_elapsed         | 617           |\n",
            "|    total_timesteps      | 175400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029728003 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.408        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.31e+04      |\n",
            "|    n_updates            | 17530         |\n",
            "|    policy_gradient_loss | -0.00202      |\n",
            "|    value_loss           | 1.21e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4963266.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 4.96e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1755         |\n",
            "|    time_elapsed         | 618          |\n",
            "|    total_timesteps      | 175500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.134275e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.336       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.22e+04     |\n",
            "|    n_updates            | 17540        |\n",
            "|    policy_gradient_loss | 0.000195     |\n",
            "|    value_loss           | 1.75e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4965285.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.97e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1756          |\n",
            "|    time_elapsed         | 618           |\n",
            "|    total_timesteps      | 175600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2974331e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.433        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.03e+04      |\n",
            "|    n_updates            | 17550         |\n",
            "|    policy_gradient_loss | -0.00029      |\n",
            "|    value_loss           | 1.37e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4968068.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.97e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1757          |\n",
            "|    time_elapsed         | 618           |\n",
            "|    total_timesteps      | 175700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7637283e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.37         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.8e+04       |\n",
            "|    n_updates            | 17560         |\n",
            "|    policy_gradient_loss | -0.00042      |\n",
            "|    value_loss           | 5.4e+04       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4970364.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 4.97e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1758         |\n",
            "|    time_elapsed         | 619          |\n",
            "|    total_timesteps      | 175800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.761079e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.373       |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7e+04        |\n",
            "|    n_updates            | 17570        |\n",
            "|    policy_gradient_loss | -0.000162    |\n",
            "|    value_loss           | 1.24e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4972288.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.97e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1759          |\n",
            "|    time_elapsed         | 619           |\n",
            "|    total_timesteps      | 175900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00051041204 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.402        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.64e+04      |\n",
            "|    n_updates            | 17580         |\n",
            "|    policy_gradient_loss | -0.00336      |\n",
            "|    value_loss           | 7.04e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4974882.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 4.97e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1760         |\n",
            "|    time_elapsed         | 620          |\n",
            "|    total_timesteps      | 176000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.519959e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.332       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.28e+04     |\n",
            "|    n_updates            | 17590        |\n",
            "|    policy_gradient_loss | -0.000411    |\n",
            "|    value_loss           | 4.17e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4977358.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 4.98e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1761         |\n",
            "|    time_elapsed         | 620          |\n",
            "|    total_timesteps      | 176100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.167314e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.492       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.78e+04     |\n",
            "|    n_updates            | 17600        |\n",
            "|    policy_gradient_loss | -0.0012      |\n",
            "|    value_loss           | 1.05e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4978533.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 4.98e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1762         |\n",
            "|    time_elapsed         | 620          |\n",
            "|    total_timesteps      | 176200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006366621 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.438       |\n",
            "|    explained_variance   | 5.13e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.96e+04     |\n",
            "|    n_updates            | 17610        |\n",
            "|    policy_gradient_loss | -0.00273     |\n",
            "|    value_loss           | 7.94e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4980787.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.98e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1763          |\n",
            "|    time_elapsed         | 621           |\n",
            "|    total_timesteps      | 176300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020068149 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.535        |\n",
            "|    explained_variance   | -8.34e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.71e+03      |\n",
            "|    n_updates            | 17620         |\n",
            "|    policy_gradient_loss | -0.0023       |\n",
            "|    value_loss           | 1.19e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4983720.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.98e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1764          |\n",
            "|    time_elapsed         | 621           |\n",
            "|    total_timesteps      | 176400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030287524 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.456        |\n",
            "|    explained_variance   | 5.96e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.12e+04      |\n",
            "|    n_updates            | 17630         |\n",
            "|    policy_gradient_loss | -0.00153      |\n",
            "|    value_loss           | 1.07e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4985788.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.99e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1765          |\n",
            "|    time_elapsed         | 621           |\n",
            "|    total_timesteps      | 176500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022130692 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.351        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.4e+04       |\n",
            "|    n_updates            | 17640         |\n",
            "|    policy_gradient_loss | -0.00195      |\n",
            "|    value_loss           | 9.66e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4988277.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.99e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1766          |\n",
            "|    time_elapsed         | 622           |\n",
            "|    total_timesteps      | 176600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5324085e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.457        |\n",
            "|    explained_variance   | 4.77e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.26e+04      |\n",
            "|    n_updates            | 17650         |\n",
            "|    policy_gradient_loss | 5.81e-05      |\n",
            "|    value_loss           | 4.46e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4990047.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 4.99e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1767         |\n",
            "|    time_elapsed         | 622          |\n",
            "|    total_timesteps      | 176700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.887785e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.359       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.09e+04     |\n",
            "|    n_updates            | 17660        |\n",
            "|    policy_gradient_loss | -0.000113    |\n",
            "|    value_loss           | 8.96e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4992435.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 4.99e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1768          |\n",
            "|    time_elapsed         | 622           |\n",
            "|    total_timesteps      | 176800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038385257 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.384        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2e+04         |\n",
            "|    n_updates            | 17670         |\n",
            "|    policy_gradient_loss | -0.0024       |\n",
            "|    value_loss           | 3.9e+04       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4995124.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5e+06         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1769          |\n",
            "|    time_elapsed         | 623           |\n",
            "|    total_timesteps      | 176900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00041936344 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.44         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.1e+04       |\n",
            "|    n_updates            | 17680         |\n",
            "|    policy_gradient_loss | -0.00236      |\n",
            "|    value_loss           | 7.3e+04       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 4996677.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5e+06        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1770         |\n",
            "|    time_elapsed         | 623          |\n",
            "|    total_timesteps      | 177000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.522947e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.372       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.3e+04      |\n",
            "|    n_updates            | 17690        |\n",
            "|    policy_gradient_loss | -0.00121     |\n",
            "|    value_loss           | 1.04e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 4999323.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5e+06         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1771          |\n",
            "|    time_elapsed         | 623           |\n",
            "|    total_timesteps      | 177100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017748479 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.364        |\n",
            "|    explained_variance   | 5.96e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.16e+04      |\n",
            "|    n_updates            | 17700         |\n",
            "|    policy_gradient_loss | -0.00236      |\n",
            "|    value_loss           | 2.19e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5002171.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5e+06         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1772          |\n",
            "|    time_elapsed         | 624           |\n",
            "|    total_timesteps      | 177200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2571923e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.406        |\n",
            "|    explained_variance   | 4.17e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.61e+04      |\n",
            "|    n_updates            | 17710         |\n",
            "|    policy_gradient_loss | 0.000129      |\n",
            "|    value_loss           | 1.01e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5005346.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.01e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1773          |\n",
            "|    time_elapsed         | 624           |\n",
            "|    total_timesteps      | 177300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038578914 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.446        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.82e+04      |\n",
            "|    n_updates            | 17720         |\n",
            "|    policy_gradient_loss | -0.00243      |\n",
            "|    value_loss           | 1.39e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5008504.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.01e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1774          |\n",
            "|    time_elapsed         | 624           |\n",
            "|    total_timesteps      | 177400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016998706 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.338        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.33e+04      |\n",
            "|    n_updates            | 17730         |\n",
            "|    policy_gradient_loss | -0.000983     |\n",
            "|    value_loss           | 1.43e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5010623.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.01e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1775         |\n",
            "|    time_elapsed         | 625          |\n",
            "|    total_timesteps      | 177500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.099532e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.313       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.72e+04     |\n",
            "|    n_updates            | 17740        |\n",
            "|    policy_gradient_loss | -0.000454    |\n",
            "|    value_loss           | 1.49e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5013278.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.01e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 283         |\n",
            "|    iterations           | 1776        |\n",
            "|    time_elapsed         | 625         |\n",
            "|    total_timesteps      | 177600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000535955 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.457      |\n",
            "|    explained_variance   | 7.15e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.76e+04    |\n",
            "|    n_updates            | 17750       |\n",
            "|    policy_gradient_loss | -0.00339    |\n",
            "|    value_loss           | 6.93e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5015326.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.02e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 283         |\n",
            "|    iterations           | 1777        |\n",
            "|    time_elapsed         | 625         |\n",
            "|    total_timesteps      | 177700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001132879 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.374      |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.47e+04    |\n",
            "|    n_updates            | 17760       |\n",
            "|    policy_gradient_loss | -0.00488    |\n",
            "|    value_loss           | 1.11e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5017930.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.02e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1778         |\n",
            "|    time_elapsed         | 626          |\n",
            "|    total_timesteps      | 177800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.776828e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.354       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.07e+04     |\n",
            "|    n_updates            | 17770        |\n",
            "|    policy_gradient_loss | -0.000212    |\n",
            "|    value_loss           | 3.72e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5021643.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.02e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1779          |\n",
            "|    time_elapsed         | 626           |\n",
            "|    total_timesteps      | 177900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2849534e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.374        |\n",
            "|    explained_variance   | 1.31e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.6e+04       |\n",
            "|    n_updates            | 17780         |\n",
            "|    policy_gradient_loss | -0.000587     |\n",
            "|    value_loss           | 9.94e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5024749.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.02e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1780          |\n",
            "|    time_elapsed         | 626           |\n",
            "|    total_timesteps      | 178000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6718586e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.334        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.51e+05      |\n",
            "|    n_updates            | 17790         |\n",
            "|    policy_gradient_loss | -0.000581     |\n",
            "|    value_loss           | 2.69e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5027330.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.03e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1781         |\n",
            "|    time_elapsed         | 627          |\n",
            "|    total_timesteps      | 178100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.304855e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.335       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.28e+04     |\n",
            "|    n_updates            | 17800        |\n",
            "|    policy_gradient_loss | -0.00132     |\n",
            "|    value_loss           | 1.22e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5030445.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.03e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1782         |\n",
            "|    time_elapsed         | 627          |\n",
            "|    total_timesteps      | 178200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002156837 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.429       |\n",
            "|    explained_variance   | 2.15e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.13e+04     |\n",
            "|    n_updates            | 17810        |\n",
            "|    policy_gradient_loss | -0.00161     |\n",
            "|    value_loss           | 9e+04        |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5032734.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.03e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1783         |\n",
            "|    time_elapsed         | 628          |\n",
            "|    total_timesteps      | 178300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.834483e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.343       |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.06e+04     |\n",
            "|    n_updates            | 17820        |\n",
            "|    policy_gradient_loss | 0.000153     |\n",
            "|    value_loss           | 1.38e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5034890.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.03e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1784          |\n",
            "|    time_elapsed         | 628           |\n",
            "|    total_timesteps      | 178400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8131868e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.347        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.68e+04      |\n",
            "|    n_updates            | 17830         |\n",
            "|    policy_gradient_loss | -0.000358     |\n",
            "|    value_loss           | 7.22e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5037538.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.04e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1785          |\n",
            "|    time_elapsed         | 628           |\n",
            "|    total_timesteps      | 178500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.3959585e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.388        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.81e+04      |\n",
            "|    n_updates            | 17840         |\n",
            "|    policy_gradient_loss | -0.0011       |\n",
            "|    value_loss           | 5.79e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5040341.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.04e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1786         |\n",
            "|    time_elapsed         | 629          |\n",
            "|    total_timesteps      | 178600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.180055e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.375       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.31e+04     |\n",
            "|    n_updates            | 17850        |\n",
            "|    policy_gradient_loss | -0.000852    |\n",
            "|    value_loss           | 8.72e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5043048.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.04e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1787          |\n",
            "|    time_elapsed         | 629           |\n",
            "|    total_timesteps      | 178700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.8549279e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.331        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.33e+04      |\n",
            "|    n_updates            | 17860         |\n",
            "|    policy_gradient_loss | -0.000153     |\n",
            "|    value_loss           | 1.03e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5045346.0\n",
            "--------------------------------------------\n",
            "| episode_reward          | 5.05e+06       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 283            |\n",
            "|    iterations           | 1788           |\n",
            "|    time_elapsed         | 629            |\n",
            "|    total_timesteps      | 178800         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000105097475 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -0.412         |\n",
            "|    explained_variance   | 1.19e-07       |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 5.7e+04        |\n",
            "|    n_updates            | 17870          |\n",
            "|    policy_gradient_loss | -0.00103       |\n",
            "|    value_loss           | 1.24e+05       |\n",
            "--------------------------------------------\n",
            "Episode done — reward = 5048169.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.05e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1789         |\n",
            "|    time_elapsed         | 630          |\n",
            "|    total_timesteps      | 178900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.622208e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.321       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.69e+04     |\n",
            "|    n_updates            | 17880        |\n",
            "|    policy_gradient_loss | -0.000641    |\n",
            "|    value_loss           | 7.35e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5049891.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.05e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1790          |\n",
            "|    time_elapsed         | 630           |\n",
            "|    total_timesteps      | 179000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00063757267 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.339        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.94e+04      |\n",
            "|    n_updates            | 17890         |\n",
            "|    policy_gradient_loss | -0.00354      |\n",
            "|    value_loss           | 1.07e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5053356.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.05e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1791         |\n",
            "|    time_elapsed         | 630          |\n",
            "|    total_timesteps      | 179100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.546007e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.319       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.01e+04     |\n",
            "|    n_updates            | 17900        |\n",
            "|    policy_gradient_loss | -0.000335    |\n",
            "|    value_loss           | 4.13e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5056317.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.06e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1792         |\n",
            "|    time_elapsed         | 631          |\n",
            "|    total_timesteps      | 179200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.516956e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.285       |\n",
            "|    explained_variance   | 4.77e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.92e+04     |\n",
            "|    n_updates            | 17910        |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    value_loss           | 1.7e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5058774.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.06e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 283         |\n",
            "|    iterations           | 1793        |\n",
            "|    time_elapsed         | 631         |\n",
            "|    total_timesteps      | 179300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 3.92984e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.39       |\n",
            "|    explained_variance   | 4.77e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.01e+05    |\n",
            "|    n_updates            | 17920       |\n",
            "|    policy_gradient_loss | -0.000451   |\n",
            "|    value_loss           | 1.78e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5062113.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.06e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1794          |\n",
            "|    time_elapsed         | 631           |\n",
            "|    total_timesteps      | 179400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5688361e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.334        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.97e+04      |\n",
            "|    n_updates            | 17930         |\n",
            "|    policy_gradient_loss | -0.000462     |\n",
            "|    value_loss           | 8.75e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5064528.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.06e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1795         |\n",
            "|    time_elapsed         | 632          |\n",
            "|    total_timesteps      | 179500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.707473e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.376       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.56e+04     |\n",
            "|    n_updates            | 17940        |\n",
            "|    policy_gradient_loss | -0.000868    |\n",
            "|    value_loss           | 1.57e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5067468.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.07e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1796         |\n",
            "|    time_elapsed         | 632          |\n",
            "|    total_timesteps      | 179600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.490573e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.303       |\n",
            "|    explained_variance   | 3.58e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.55e+04     |\n",
            "|    n_updates            | 17950        |\n",
            "|    policy_gradient_loss | -0.000768    |\n",
            "|    value_loss           | 6.85e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5070408.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.07e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1797          |\n",
            "|    time_elapsed         | 632           |\n",
            "|    total_timesteps      | 179700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1669882e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.356        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.88e+04      |\n",
            "|    n_updates            | 17960         |\n",
            "|    policy_gradient_loss | -0.000378     |\n",
            "|    value_loss           | 1.49e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5073394.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.07e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1798          |\n",
            "|    time_elapsed         | 633           |\n",
            "|    total_timesteps      | 179800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.2557432e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.368        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.34e+04      |\n",
            "|    n_updates            | 17970         |\n",
            "|    policy_gradient_loss | -0.00048      |\n",
            "|    value_loss           | 1.21e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5075459.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.08e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1799          |\n",
            "|    time_elapsed         | 633           |\n",
            "|    total_timesteps      | 179900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.3996246e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.327        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.91e+04      |\n",
            "|    n_updates            | 17980         |\n",
            "|    policy_gradient_loss | -0.00109      |\n",
            "|    value_loss           | 1.1e+05       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5077065.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.08e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1800          |\n",
            "|    time_elapsed         | 633           |\n",
            "|    total_timesteps      | 180000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.3218875e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.434        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.19e+04      |\n",
            "|    n_updates            | 17990         |\n",
            "|    policy_gradient_loss | -0.000527     |\n",
            "|    value_loss           | 6.45e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5079416.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.08e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1801         |\n",
            "|    time_elapsed         | 634          |\n",
            "|    total_timesteps      | 180100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005031426 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.313       |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.55e+04     |\n",
            "|    n_updates            | 18000        |\n",
            "|    policy_gradient_loss | -0.00158     |\n",
            "|    value_loss           | 2.44e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5082751.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.08e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1802          |\n",
            "|    time_elapsed         | 634           |\n",
            "|    total_timesteps      | 180200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010221234 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.405        |\n",
            "|    explained_variance   | 9.54e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.34e+04      |\n",
            "|    n_updates            | 18010         |\n",
            "|    policy_gradient_loss | -0.00108      |\n",
            "|    value_loss           | 8.62e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5086620.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.09e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1803          |\n",
            "|    time_elapsed         | 634           |\n",
            "|    total_timesteps      | 180300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3972995e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.305        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.46e+04      |\n",
            "|    n_updates            | 18020         |\n",
            "|    policy_gradient_loss | -0.000346     |\n",
            "|    value_loss           | 1.61e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5089182.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.09e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1804          |\n",
            "|    time_elapsed         | 635           |\n",
            "|    total_timesteps      | 180400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8708977e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.336        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.24e+05      |\n",
            "|    n_updates            | 18030         |\n",
            "|    policy_gradient_loss | -0.000314     |\n",
            "|    value_loss           | 2.54e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5091611.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.09e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1805          |\n",
            "|    time_elapsed         | 635           |\n",
            "|    total_timesteps      | 180500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0822794e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.338        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.79e+04      |\n",
            "|    n_updates            | 18040         |\n",
            "|    policy_gradient_loss | -0.000402     |\n",
            "|    value_loss           | 8.84e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5093749.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.09e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1806          |\n",
            "|    time_elapsed         | 636           |\n",
            "|    total_timesteps      | 180600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5981339e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.31         |\n",
            "|    explained_variance   | 1.97e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.27e+04      |\n",
            "|    n_updates            | 18050         |\n",
            "|    policy_gradient_loss | -0.000238     |\n",
            "|    value_loss           | 9.34e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5097088.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.1e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1807          |\n",
            "|    time_elapsed         | 636           |\n",
            "|    total_timesteps      | 180700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024832028 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.374        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.88e+04      |\n",
            "|    n_updates            | 18060         |\n",
            "|    policy_gradient_loss | -0.00214      |\n",
            "|    value_loss           | 3.89e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5099740.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.1e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1808         |\n",
            "|    time_elapsed         | 636          |\n",
            "|    total_timesteps      | 180800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004891362 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.321       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.09e+04     |\n",
            "|    n_updates            | 18070        |\n",
            "|    policy_gradient_loss | -0.00121     |\n",
            "|    value_loss           | 1.6e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5102573.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.1e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1809          |\n",
            "|    time_elapsed         | 637           |\n",
            "|    total_timesteps      | 180900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9296843e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.341        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.42e+04      |\n",
            "|    n_updates            | 18080         |\n",
            "|    policy_gradient_loss | -0.000596     |\n",
            "|    value_loss           | 1.03e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5105226.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.11e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1810         |\n",
            "|    time_elapsed         | 637          |\n",
            "|    total_timesteps      | 181000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.464861e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.291       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.07e+04     |\n",
            "|    n_updates            | 18090        |\n",
            "|    policy_gradient_loss | -0.00072     |\n",
            "|    value_loss           | 1.06e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5106654.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.11e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1811          |\n",
            "|    time_elapsed         | 637           |\n",
            "|    total_timesteps      | 181100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.3749445e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.345        |\n",
            "|    explained_variance   | 2.03e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.2e+04       |\n",
            "|    n_updates            | 18100         |\n",
            "|    policy_gradient_loss | -0.000676     |\n",
            "|    value_loss           | 8.95e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5109118.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.11e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1812          |\n",
            "|    time_elapsed         | 638           |\n",
            "|    total_timesteps      | 181200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00082468917 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.482        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.91e+03      |\n",
            "|    n_updates            | 18110         |\n",
            "|    policy_gradient_loss | -0.00301      |\n",
            "|    value_loss           | 2.11e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5111597.0\n",
            "--------------------------------------------\n",
            "| episode_reward          | 5.11e+06       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 283            |\n",
            "|    iterations           | 1813           |\n",
            "|    time_elapsed         | 638            |\n",
            "|    total_timesteps      | 181300         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000111559304 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -0.414         |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 3.82e+04       |\n",
            "|    n_updates            | 18120          |\n",
            "|    policy_gradient_loss | -8.96e-05      |\n",
            "|    value_loss           | 8.27e+04       |\n",
            "--------------------------------------------\n",
            "Episode done — reward = 5114641.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.11e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1814          |\n",
            "|    time_elapsed         | 638           |\n",
            "|    total_timesteps      | 181400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00058249116 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.37         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.39e+04      |\n",
            "|    n_updates            | 18130         |\n",
            "|    policy_gradient_loss | -0.0026       |\n",
            "|    value_loss           | 1.07e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5117722.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.12e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1815          |\n",
            "|    time_elapsed         | 639           |\n",
            "|    total_timesteps      | 181500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033974388 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.355        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.83e+04      |\n",
            "|    n_updates            | 18140         |\n",
            "|    policy_gradient_loss | -0.00147      |\n",
            "|    value_loss           | 1.44e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5120514.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.12e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1816          |\n",
            "|    time_elapsed         | 639           |\n",
            "|    total_timesteps      | 181600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1838403e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.446        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.3e+04       |\n",
            "|    n_updates            | 18150         |\n",
            "|    policy_gradient_loss | 8.13e-05      |\n",
            "|    value_loss           | 1.44e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5123139.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.12e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1817          |\n",
            "|    time_elapsed         | 639           |\n",
            "|    total_timesteps      | 181700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.6595982e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.32         |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.58e+04      |\n",
            "|    n_updates            | 18160         |\n",
            "|    policy_gradient_loss | -0.000355     |\n",
            "|    value_loss           | 1.29e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5126079.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.13e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1818          |\n",
            "|    time_elapsed         | 640           |\n",
            "|    total_timesteps      | 181800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9340775e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.361        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.82e+04      |\n",
            "|    n_updates            | 18170         |\n",
            "|    policy_gradient_loss | -0.000456     |\n",
            "|    value_loss           | 1.03e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5128231.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.13e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1819          |\n",
            "|    time_elapsed         | 640           |\n",
            "|    total_timesteps      | 181900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018966364 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.318        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.48e+04      |\n",
            "|    n_updates            | 18180         |\n",
            "|    policy_gradient_loss | -0.0018       |\n",
            "|    value_loss           | 1.64e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5130288.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.13e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1820         |\n",
            "|    time_elapsed         | 640          |\n",
            "|    total_timesteps      | 182000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001512579 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.393       |\n",
            "|    explained_variance   | 1.07e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.56e+04     |\n",
            "|    n_updates            | 18190        |\n",
            "|    policy_gradient_loss | -0.0014      |\n",
            "|    value_loss           | 8.19e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5133513.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.13e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1821         |\n",
            "|    time_elapsed         | 641          |\n",
            "|    total_timesteps      | 182100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005296808 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.434       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.62e+04     |\n",
            "|    n_updates            | 18200        |\n",
            "|    policy_gradient_loss | -0.00289     |\n",
            "|    value_loss           | 4.2e+04      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5135350.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.14e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1822         |\n",
            "|    time_elapsed         | 641          |\n",
            "|    total_timesteps      | 182200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.205055e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.386       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.13e+04     |\n",
            "|    n_updates            | 18210        |\n",
            "|    policy_gradient_loss | -0.000324    |\n",
            "|    value_loss           | 1.74e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5138680.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.14e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1823          |\n",
            "|    time_elapsed         | 641           |\n",
            "|    total_timesteps      | 182300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9462775e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.425        |\n",
            "|    explained_variance   | 2.74e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.7e+04       |\n",
            "|    n_updates            | 18220         |\n",
            "|    policy_gradient_loss | -0.000673     |\n",
            "|    value_loss           | 3.71e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5141335.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.14e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 283          |\n",
            "|    iterations           | 1824         |\n",
            "|    time_elapsed         | 642          |\n",
            "|    total_timesteps      | 182400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.468242e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.36        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.17e+04     |\n",
            "|    n_updates            | 18230        |\n",
            "|    policy_gradient_loss | 2.71e-05     |\n",
            "|    value_loss           | 2.01e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5144457.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.14e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1825          |\n",
            "|    time_elapsed         | 642           |\n",
            "|    total_timesteps      | 182500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.8900853e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.351        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.99e+04      |\n",
            "|    n_updates            | 18240         |\n",
            "|    policy_gradient_loss | -0.000588     |\n",
            "|    value_loss           | 1.38e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5147205.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.15e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1826          |\n",
            "|    time_elapsed         | 642           |\n",
            "|    total_timesteps      | 182600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017365062 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.373        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.05e+04      |\n",
            "|    n_updates            | 18250         |\n",
            "|    policy_gradient_loss | -0.00159      |\n",
            "|    value_loss           | 1.44e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5148913.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.15e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 283           |\n",
            "|    iterations           | 1827          |\n",
            "|    time_elapsed         | 643           |\n",
            "|    total_timesteps      | 182700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00048276747 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.499        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.85e+04      |\n",
            "|    n_updates            | 18260         |\n",
            "|    policy_gradient_loss | -0.00402      |\n",
            "|    value_loss           | 1.28e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5151623.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.15e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1828         |\n",
            "|    time_elapsed         | 643          |\n",
            "|    total_timesteps      | 182800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020315605 |\n",
            "|    clip_fraction        | 0.00729      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.378       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.47e+04     |\n",
            "|    n_updates            | 18270        |\n",
            "|    policy_gradient_loss | -0.00365     |\n",
            "|    value_loss           | 2.43e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5154135.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.15e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1829          |\n",
            "|    time_elapsed         | 644           |\n",
            "|    total_timesteps      | 182900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00055275846 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.4          |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.2e+04       |\n",
            "|    n_updates            | 18280         |\n",
            "|    policy_gradient_loss | -0.0021       |\n",
            "|    value_loss           | 1.03e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5157244.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.16e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1830          |\n",
            "|    time_elapsed         | 644           |\n",
            "|    total_timesteps      | 183000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.9782455e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.361        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.93e+04      |\n",
            "|    n_updates            | 18290         |\n",
            "|    policy_gradient_loss | -6.93e-05     |\n",
            "|    value_loss           | 8.63e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5159871.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.16e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1831          |\n",
            "|    time_elapsed         | 644           |\n",
            "|    total_timesteps      | 183100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2957205e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.504        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.07e+05      |\n",
            "|    n_updates            | 18300         |\n",
            "|    policy_gradient_loss | -0.000583     |\n",
            "|    value_loss           | 1.96e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5161422.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.16e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1832          |\n",
            "|    time_elapsed         | 645           |\n",
            "|    total_timesteps      | 183200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.7882782e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.452        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.89e+04      |\n",
            "|    n_updates            | 18310         |\n",
            "|    policy_gradient_loss | -0.00053      |\n",
            "|    value_loss           | 8.96e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5164139.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.16e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1833         |\n",
            "|    time_elapsed         | 645          |\n",
            "|    total_timesteps      | 183300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017139495 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.488       |\n",
            "|    explained_variance   | 1.37e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.82e+03     |\n",
            "|    n_updates            | 18320        |\n",
            "|    policy_gradient_loss | -0.00395     |\n",
            "|    value_loss           | 2.34e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5168214.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.17e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1834          |\n",
            "|    time_elapsed         | 645           |\n",
            "|    total_timesteps      | 183400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023022029 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.485        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.48e+04      |\n",
            "|    n_updates            | 18330         |\n",
            "|    policy_gradient_loss | 0.000396      |\n",
            "|    value_loss           | 1.08e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5170623.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.17e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1835          |\n",
            "|    time_elapsed         | 646           |\n",
            "|    total_timesteps      | 183500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9621256e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.339        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.25e+05      |\n",
            "|    n_updates            | 18340         |\n",
            "|    policy_gradient_loss | -0.0006       |\n",
            "|    value_loss           | 3.13e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5172530.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.17e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1836          |\n",
            "|    time_elapsed         | 646           |\n",
            "|    total_timesteps      | 183600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00040227253 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.506        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.03e+04      |\n",
            "|    n_updates            | 18350         |\n",
            "|    policy_gradient_loss | -0.00255      |\n",
            "|    value_loss           | 9.11e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5176378.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.18e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1837          |\n",
            "|    time_elapsed         | 646           |\n",
            "|    total_timesteps      | 183700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.1501546e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.464        |\n",
            "|    explained_variance   | 5.36e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.36e+04      |\n",
            "|    n_updates            | 18360         |\n",
            "|    policy_gradient_loss | -0.000317     |\n",
            "|    value_loss           | 4.15e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5178906.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.18e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1838          |\n",
            "|    time_elapsed         | 647           |\n",
            "|    total_timesteps      | 183800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.3548153e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.36         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.08e+05      |\n",
            "|    n_updates            | 18370         |\n",
            "|    policy_gradient_loss | -0.000568     |\n",
            "|    value_loss           | 1.97e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5181267.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.18e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1839         |\n",
            "|    time_elapsed         | 647          |\n",
            "|    total_timesteps      | 183900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005274422 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.459       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.72e+04     |\n",
            "|    n_updates            | 18380        |\n",
            "|    policy_gradient_loss | -0.00258     |\n",
            "|    value_loss           | 9.05e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5183984.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.18e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1840         |\n",
            "|    time_elapsed         | 647          |\n",
            "|    total_timesteps      | 184000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007617235 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.444       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.03e+04     |\n",
            "|    n_updates            | 18390        |\n",
            "|    policy_gradient_loss | -0.00318     |\n",
            "|    value_loss           | 5.11e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5186588.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.19e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1841         |\n",
            "|    time_elapsed         | 648          |\n",
            "|    total_timesteps      | 184100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016151353 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.395       |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.75e+04     |\n",
            "|    n_updates            | 18400        |\n",
            "|    policy_gradient_loss | -0.00162     |\n",
            "|    value_loss           | 1.32e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5189469.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.19e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1842         |\n",
            "|    time_elapsed         | 648          |\n",
            "|    total_timesteps      | 184200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005252614 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.393       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.5e+04      |\n",
            "|    n_updates            | 18410        |\n",
            "|    policy_gradient_loss | -0.00336     |\n",
            "|    value_loss           | 9.11e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5192295.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.19e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1843          |\n",
            "|    time_elapsed         | 648           |\n",
            "|    total_timesteps      | 184300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00047958156 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.407        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.16e+04      |\n",
            "|    n_updates            | 18420         |\n",
            "|    policy_gradient_loss | -0.00329      |\n",
            "|    value_loss           | 1.25e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5195214.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.2e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1844         |\n",
            "|    time_elapsed         | 649          |\n",
            "|    total_timesteps      | 184400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.587236e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.35        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.05e+04     |\n",
            "|    n_updates            | 18430        |\n",
            "|    policy_gradient_loss | -2.89e-05    |\n",
            "|    value_loss           | 1.09e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5198165.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.2e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1845         |\n",
            "|    time_elapsed         | 649          |\n",
            "|    total_timesteps      | 184500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003400492 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.355       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.75e+04     |\n",
            "|    n_updates            | 18440        |\n",
            "|    policy_gradient_loss | -0.00248     |\n",
            "|    value_loss           | 1.21e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5200986.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.2e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1846          |\n",
            "|    time_elapsed         | 649           |\n",
            "|    total_timesteps      | 184600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023541818 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.444        |\n",
            "|    explained_variance   | 1.49e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.58e+04      |\n",
            "|    n_updates            | 18450         |\n",
            "|    policy_gradient_loss | -0.00117      |\n",
            "|    value_loss           | 1.08e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5203040.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.2e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1847          |\n",
            "|    time_elapsed         | 650           |\n",
            "|    total_timesteps      | 184700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010349291 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.409        |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.35e+04      |\n",
            "|    n_updates            | 18460         |\n",
            "|    policy_gradient_loss | -0.000896     |\n",
            "|    value_loss           | 8e+04         |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5205903.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.21e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1848         |\n",
            "|    time_elapsed         | 650          |\n",
            "|    total_timesteps      | 184800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002596476 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.367       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.7e+04      |\n",
            "|    n_updates            | 18470        |\n",
            "|    policy_gradient_loss | -0.00134     |\n",
            "|    value_loss           | 5.22e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5207885.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.21e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1849         |\n",
            "|    time_elapsed         | 650          |\n",
            "|    total_timesteps      | 184900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013396263 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.382       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.74e+04     |\n",
            "|    n_updates            | 18480        |\n",
            "|    policy_gradient_loss | -0.0031      |\n",
            "|    value_loss           | 1.05e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5211030.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.21e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1850         |\n",
            "|    time_elapsed         | 651          |\n",
            "|    total_timesteps      | 185000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010814759 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.516       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.72e+04     |\n",
            "|    n_updates            | 18490        |\n",
            "|    policy_gradient_loss | -0.00345     |\n",
            "|    value_loss           | 5.32e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5213040.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.21e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1851          |\n",
            "|    time_elapsed         | 651           |\n",
            "|    total_timesteps      | 185100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.9244663e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.451        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.49e+04      |\n",
            "|    n_updates            | 18500         |\n",
            "|    policy_gradient_loss | -0.000361     |\n",
            "|    value_loss           | 1.76e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5216043.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.22e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1852          |\n",
            "|    time_elapsed         | 652           |\n",
            "|    total_timesteps      | 185200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00019429531 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.47         |\n",
            "|    explained_variance   | 1.37e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.82e+04      |\n",
            "|    n_updates            | 18510         |\n",
            "|    policy_gradient_loss | -0.00125      |\n",
            "|    value_loss           | 4.77e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5219403.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.22e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1853          |\n",
            "|    time_elapsed         | 652           |\n",
            "|    total_timesteps      | 185300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.1210067e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.457        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.9e+04       |\n",
            "|    n_updates            | 18520         |\n",
            "|    policy_gradient_loss | -0.000464     |\n",
            "|    value_loss           | 1.67e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5221735.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.22e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1854          |\n",
            "|    time_elapsed         | 652           |\n",
            "|    total_timesteps      | 185400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00025351055 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.344        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.16e+04      |\n",
            "|    n_updates            | 18530         |\n",
            "|    policy_gradient_loss | -0.00193      |\n",
            "|    value_loss           | 1.27e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5224078.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.22e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1855          |\n",
            "|    time_elapsed         | 653           |\n",
            "|    total_timesteps      | 185500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015902965 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.44         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.52e+04      |\n",
            "|    n_updates            | 18540         |\n",
            "|    policy_gradient_loss | -0.000813     |\n",
            "|    value_loss           | 7.23e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5227152.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.23e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1856          |\n",
            "|    time_elapsed         | 653           |\n",
            "|    total_timesteps      | 185600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034063222 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.366        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.89e+04      |\n",
            "|    n_updates            | 18550         |\n",
            "|    policy_gradient_loss | -0.00297      |\n",
            "|    value_loss           | 9.03e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5229861.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.23e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1857         |\n",
            "|    time_elapsed         | 653          |\n",
            "|    total_timesteps      | 185700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010952414 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.435       |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.78e+04     |\n",
            "|    n_updates            | 18560        |\n",
            "|    policy_gradient_loss | -0.0049      |\n",
            "|    value_loss           | 1.37e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5232822.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.23e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1858          |\n",
            "|    time_elapsed         | 654           |\n",
            "|    total_timesteps      | 185800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.7714416e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.436        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.35e+04      |\n",
            "|    n_updates            | 18570         |\n",
            "|    policy_gradient_loss | 7.58e-05      |\n",
            "|    value_loss           | 1.38e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5235792.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.24e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 1859        |\n",
            "|    time_elapsed         | 654         |\n",
            "|    total_timesteps      | 185900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 6.64895e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.36       |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.19e+04    |\n",
            "|    n_updates            | 18580       |\n",
            "|    policy_gradient_loss | -0.000751   |\n",
            "|    value_loss           | 1.03e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5238351.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.24e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1860          |\n",
            "|    time_elapsed         | 654           |\n",
            "|    total_timesteps      | 186000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015921549 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.364        |\n",
            "|    explained_variance   | 5.96e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.3e+04       |\n",
            "|    n_updates            | 18590         |\n",
            "|    policy_gradient_loss | -0.000987     |\n",
            "|    value_loss           | 1.15e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5241290.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.24e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1861          |\n",
            "|    time_elapsed         | 655           |\n",
            "|    total_timesteps      | 186100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00041971775 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.401        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.8e+04       |\n",
            "|    n_updates            | 18600         |\n",
            "|    policy_gradient_loss | -0.00195      |\n",
            "|    value_loss           | 9.48e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5244059.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.24e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1862          |\n",
            "|    time_elapsed         | 655           |\n",
            "|    total_timesteps      | 186200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.9706604e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.423        |\n",
            "|    explained_variance   | 2.32e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.99e+04      |\n",
            "|    n_updates            | 18610         |\n",
            "|    policy_gradient_loss | -8.8e-05      |\n",
            "|    value_loss           | 1.41e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5246544.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.25e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1863          |\n",
            "|    time_elapsed         | 655           |\n",
            "|    total_timesteps      | 186300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030846114 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.413        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.06e+05      |\n",
            "|    n_updates            | 18620         |\n",
            "|    policy_gradient_loss | -0.0028       |\n",
            "|    value_loss           | 1.72e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5249501.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.25e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1864         |\n",
            "|    time_elapsed         | 656          |\n",
            "|    total_timesteps      | 186400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001812657 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.423       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.89e+04     |\n",
            "|    n_updates            | 18630        |\n",
            "|    policy_gradient_loss | -0.00187     |\n",
            "|    value_loss           | 1.22e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5252377.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.25e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1865         |\n",
            "|    time_elapsed         | 656          |\n",
            "|    total_timesteps      | 186500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001072835 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.348       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.17e+04     |\n",
            "|    n_updates            | 18640        |\n",
            "|    policy_gradient_loss | -0.000442    |\n",
            "|    value_loss           | 1.45e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5254957.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.25e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1866         |\n",
            "|    time_elapsed         | 656          |\n",
            "|    total_timesteps      | 186600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001615053 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.318       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.02e+04     |\n",
            "|    n_updates            | 18650        |\n",
            "|    policy_gradient_loss | -0.00137     |\n",
            "|    value_loss           | 1.16e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5258085.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.26e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1867         |\n",
            "|    time_elapsed         | 657          |\n",
            "|    total_timesteps      | 186700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004055546 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.317       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.49e+04     |\n",
            "|    n_updates            | 18660        |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    value_loss           | 9.49e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5260910.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.26e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1868          |\n",
            "|    time_elapsed         | 657           |\n",
            "|    total_timesteps      | 186800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.9013357e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.335        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.22e+04      |\n",
            "|    n_updates            | 18670         |\n",
            "|    policy_gradient_loss | -0.0008       |\n",
            "|    value_loss           | 1.39e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5264280.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.26e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 1869        |\n",
            "|    time_elapsed         | 658         |\n",
            "|    total_timesteps      | 186900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000648758 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.356      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.22e+04    |\n",
            "|    n_updates            | 18680       |\n",
            "|    policy_gradient_loss | -0.00166    |\n",
            "|    value_loss           | 1.24e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5266212.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.27e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1870          |\n",
            "|    time_elapsed         | 658           |\n",
            "|    total_timesteps      | 187000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016705805 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.361        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.56e+04      |\n",
            "|    n_updates            | 18690         |\n",
            "|    policy_gradient_loss | -0.00104      |\n",
            "|    value_loss           | 1.83e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5268396.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.27e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1871         |\n",
            "|    time_elapsed         | 658          |\n",
            "|    total_timesteps      | 187100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013597915 |\n",
            "|    clip_fraction        | 0.00356      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.422       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.46e+04     |\n",
            "|    n_updates            | 18700        |\n",
            "|    policy_gradient_loss | -0.00325     |\n",
            "|    value_loss           | 5.04e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5271630.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.27e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1872         |\n",
            "|    time_elapsed         | 659          |\n",
            "|    total_timesteps      | 187200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004144631 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.466       |\n",
            "|    explained_variance   | 5.96e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.63e+04     |\n",
            "|    n_updates            | 18710        |\n",
            "|    policy_gradient_loss | -0.00226     |\n",
            "|    value_loss           | 7.55e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5273335.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.27e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1873          |\n",
            "|    time_elapsed         | 659           |\n",
            "|    total_timesteps      | 187300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033818526 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.334        |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.24e+04      |\n",
            "|    n_updates            | 18720         |\n",
            "|    policy_gradient_loss | -0.00117      |\n",
            "|    value_loss           | 1.61e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5276817.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.28e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1874         |\n",
            "|    time_elapsed         | 659          |\n",
            "|    total_timesteps      | 187400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023099082 |\n",
            "|    clip_fraction        | 0.0161       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.442       |\n",
            "|    explained_variance   | 1.37e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.35e+04     |\n",
            "|    n_updates            | 18730        |\n",
            "|    policy_gradient_loss | -0.00686     |\n",
            "|    value_loss           | 2.94e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5278887.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.28e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1875         |\n",
            "|    time_elapsed         | 660          |\n",
            "|    total_timesteps      | 187500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.563005e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.306       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2e+05      |\n",
            "|    n_updates            | 18740        |\n",
            "|    policy_gradient_loss | -0.000124    |\n",
            "|    value_loss           | 2.33e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5281468.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.28e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1876         |\n",
            "|    time_elapsed         | 660          |\n",
            "|    total_timesteps      | 187600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032955282 |\n",
            "|    clip_fraction        | 0.0115       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.392       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.66e+04     |\n",
            "|    n_updates            | 18750        |\n",
            "|    policy_gradient_loss | -0.00753     |\n",
            "|    value_loss           | 4.4e+04      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5284791.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.28e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1877         |\n",
            "|    time_elapsed         | 660          |\n",
            "|    total_timesteps      | 187700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004584613 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.381       |\n",
            "|    explained_variance   | 2.21e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.15e+04     |\n",
            "|    n_updates            | 18760        |\n",
            "|    policy_gradient_loss | -0.00115     |\n",
            "|    value_loss           | 8.12e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5288052.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.29e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1878         |\n",
            "|    time_elapsed         | 661          |\n",
            "|    total_timesteps      | 187800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.571668e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.295       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.46e+04     |\n",
            "|    n_updates            | 18770        |\n",
            "|    policy_gradient_loss | -0.00021     |\n",
            "|    value_loss           | 1.47e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5290257.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.29e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1879          |\n",
            "|    time_elapsed         | 661           |\n",
            "|    total_timesteps      | 187900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0310776e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.337        |\n",
            "|    explained_variance   | 3.76e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.28e+04      |\n",
            "|    n_updates            | 18780         |\n",
            "|    policy_gradient_loss | -0.000726     |\n",
            "|    value_loss           | 1.46e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5293470.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.29e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1880          |\n",
            "|    time_elapsed         | 661           |\n",
            "|    total_timesteps      | 188000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033586874 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.406        |\n",
            "|    explained_variance   | 1.61e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.4e+04       |\n",
            "|    n_updates            | 18790         |\n",
            "|    policy_gradient_loss | -0.00149      |\n",
            "|    value_loss           | 7.24e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5295591.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.3e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1881          |\n",
            "|    time_elapsed         | 662           |\n",
            "|    total_timesteps      | 188100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017121388 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.269        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.74e+04      |\n",
            "|    n_updates            | 18800         |\n",
            "|    policy_gradient_loss | -0.00166      |\n",
            "|    value_loss           | 1.78e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5300259.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.3e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1882          |\n",
            "|    time_elapsed         | 662           |\n",
            "|    total_timesteps      | 188200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035525172 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.339        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.35e+04      |\n",
            "|    n_updates            | 18810         |\n",
            "|    policy_gradient_loss | -0.00187      |\n",
            "|    value_loss           | 4.65e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5302773.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.3e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1883          |\n",
            "|    time_elapsed         | 662           |\n",
            "|    total_timesteps      | 188300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1222073e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.308        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.37e+05      |\n",
            "|    n_updates            | 18820         |\n",
            "|    policy_gradient_loss | 0.000208      |\n",
            "|    value_loss           | 3.08e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5305509.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.31e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1884          |\n",
            "|    time_elapsed         | 663           |\n",
            "|    total_timesteps      | 188400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017819736 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.409        |\n",
            "|    explained_variance   | 3.4e-06       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.5e+04       |\n",
            "|    n_updates            | 18830         |\n",
            "|    policy_gradient_loss | -0.00137      |\n",
            "|    value_loss           | 6.27e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5309222.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.31e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1885          |\n",
            "|    time_elapsed         | 663           |\n",
            "|    total_timesteps      | 188500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030272396 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.384        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.33e+04      |\n",
            "|    n_updates            | 18840         |\n",
            "|    policy_gradient_loss | -0.00177      |\n",
            "|    value_loss           | 1.28e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5311933.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.31e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 1886        |\n",
            "|    time_elapsed         | 663         |\n",
            "|    total_timesteps      | 188600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 7.53485e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.323      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.39e+05    |\n",
            "|    n_updates            | 18850       |\n",
            "|    policy_gradient_loss | -0.000135   |\n",
            "|    value_loss           | 2.93e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5314124.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.31e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1887         |\n",
            "|    time_elapsed         | 664          |\n",
            "|    total_timesteps      | 188700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002059845 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.321       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.49e+04     |\n",
            "|    n_updates            | 18860        |\n",
            "|    policy_gradient_loss | -0.00224     |\n",
            "|    value_loss           | 1.01e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5316990.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.32e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1888          |\n",
            "|    time_elapsed         | 664           |\n",
            "|    total_timesteps      | 188800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00041231216 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.364        |\n",
            "|    explained_variance   | 7.15e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.28e+04      |\n",
            "|    n_updates            | 18870         |\n",
            "|    policy_gradient_loss | -0.00275      |\n",
            "|    value_loss           | 7.27e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5319868.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.32e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1889          |\n",
            "|    time_elapsed         | 665           |\n",
            "|    total_timesteps      | 188900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013239299 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.385        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.02e+04      |\n",
            "|    n_updates            | 18880         |\n",
            "|    policy_gradient_loss | -0.000974     |\n",
            "|    value_loss           | 1.15e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5322265.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.32e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1890          |\n",
            "|    time_elapsed         | 665           |\n",
            "|    total_timesteps      | 189000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.2213076e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.301        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.62e+04      |\n",
            "|    n_updates            | 18890         |\n",
            "|    policy_gradient_loss | -0.000711     |\n",
            "|    value_loss           | 1.07e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5324658.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.32e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1891         |\n",
            "|    time_elapsed         | 665          |\n",
            "|    total_timesteps      | 189100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012987558 |\n",
            "|    clip_fraction        | 0.00929      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.332       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.99e+04     |\n",
            "|    n_updates            | 18900        |\n",
            "|    policy_gradient_loss | -0.00469     |\n",
            "|    value_loss           | 5.69e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5328162.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.33e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1892          |\n",
            "|    time_elapsed         | 666           |\n",
            "|    total_timesteps      | 189200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011412924 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.388        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.99e+04      |\n",
            "|    n_updates            | 18910         |\n",
            "|    policy_gradient_loss | -0.00105      |\n",
            "|    value_loss           | 8.65e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5330671.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.33e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1893          |\n",
            "|    time_elapsed         | 666           |\n",
            "|    total_timesteps      | 189300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012811263 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.311        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.34e+04      |\n",
            "|    n_updates            | 18920         |\n",
            "|    policy_gradient_loss | -0.00165      |\n",
            "|    value_loss           | 2.03e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5333254.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.33e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1894          |\n",
            "|    time_elapsed         | 666           |\n",
            "|    total_timesteps      | 189400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011049066 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.264        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.68e+04      |\n",
            "|    n_updates            | 18930         |\n",
            "|    policy_gradient_loss | -0.000466     |\n",
            "|    value_loss           | 8.25e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5335305.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.34e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1895          |\n",
            "|    time_elapsed         | 667           |\n",
            "|    total_timesteps      | 189500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016477032 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.369        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.07e+04      |\n",
            "|    n_updates            | 18940         |\n",
            "|    policy_gradient_loss | -0.00167      |\n",
            "|    value_loss           | 7.6e+04       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5337654.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.34e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1896          |\n",
            "|    time_elapsed         | 667           |\n",
            "|    total_timesteps      | 189600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018178174 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.43         |\n",
            "|    explained_variance   | 1.85e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.97e+04      |\n",
            "|    n_updates            | 18950         |\n",
            "|    policy_gradient_loss | -0.00116      |\n",
            "|    value_loss           | 3.77e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5339964.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.34e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1897          |\n",
            "|    time_elapsed         | 667           |\n",
            "|    total_timesteps      | 189700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011669964 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.357        |\n",
            "|    explained_variance   | 1.97e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.51e+04      |\n",
            "|    n_updates            | 18960         |\n",
            "|    policy_gradient_loss | -0.00107      |\n",
            "|    value_loss           | 8.95e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5342553.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.34e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 1898        |\n",
            "|    time_elapsed         | 668         |\n",
            "|    total_timesteps      | 189800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000979054 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.444      |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.84e+04    |\n",
            "|    n_updates            | 18970       |\n",
            "|    policy_gradient_loss | -0.00291    |\n",
            "|    value_loss           | 1.04e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5346243.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.35e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1899          |\n",
            "|    time_elapsed         | 668           |\n",
            "|    total_timesteps      | 189900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010218425 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.399        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.82e+04      |\n",
            "|    n_updates            | 18980         |\n",
            "|    policy_gradient_loss | 3.39e-05      |\n",
            "|    value_loss           | 7.83e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5349939.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.35e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1900          |\n",
            "|    time_elapsed         | 668           |\n",
            "|    total_timesteps      | 190000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4203239e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.335        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.02e+05      |\n",
            "|    n_updates            | 18990         |\n",
            "|    policy_gradient_loss | -0.000328     |\n",
            "|    value_loss           | 1.99e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5352123.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.35e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1901         |\n",
            "|    time_elapsed         | 669          |\n",
            "|    total_timesteps      | 190100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.904534e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.317       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.31e+05     |\n",
            "|    n_updates            | 19000        |\n",
            "|    policy_gradient_loss | -0.000356    |\n",
            "|    value_loss           | 2.31e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5355536.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.36e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1902          |\n",
            "|    time_elapsed         | 669           |\n",
            "|    total_timesteps      | 190200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.2975648e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.436        |\n",
            "|    explained_variance   | 3.99e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.19e+04      |\n",
            "|    n_updates            | 19010         |\n",
            "|    policy_gradient_loss | -0.00079      |\n",
            "|    value_loss           | 5.29e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5357992.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.36e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1903          |\n",
            "|    time_elapsed         | 669           |\n",
            "|    total_timesteps      | 190300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.3339095e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.392        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.72e+04      |\n",
            "|    n_updates            | 19020         |\n",
            "|    policy_gradient_loss | -0.0012       |\n",
            "|    value_loss           | 1.77e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5361411.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.36e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1904          |\n",
            "|    time_elapsed         | 670           |\n",
            "|    total_timesteps      | 190400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015335028 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.377        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.87e+04      |\n",
            "|    n_updates            | 19030         |\n",
            "|    policy_gradient_loss | -0.00105      |\n",
            "|    value_loss           | 6.87e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5364216.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.36e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1905         |\n",
            "|    time_elapsed         | 670          |\n",
            "|    total_timesteps      | 190500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.940123e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.314       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.6e+04      |\n",
            "|    n_updates            | 19040        |\n",
            "|    policy_gradient_loss | -0.000206    |\n",
            "|    value_loss           | 1.89e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5367037.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.37e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1906         |\n",
            "|    time_elapsed         | 670          |\n",
            "|    total_timesteps      | 190600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.046272e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.301       |\n",
            "|    explained_variance   | 1.25e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.4e+04      |\n",
            "|    n_updates            | 19050        |\n",
            "|    policy_gradient_loss | -0.000829    |\n",
            "|    value_loss           | 1.16e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5369501.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.37e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1907          |\n",
            "|    time_elapsed         | 671           |\n",
            "|    total_timesteps      | 190700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1396956e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.311        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.74e+04      |\n",
            "|    n_updates            | 19060         |\n",
            "|    policy_gradient_loss | -0.000163     |\n",
            "|    value_loss           | 7.5e+04       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5371782.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.37e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1908          |\n",
            "|    time_elapsed         | 671           |\n",
            "|    total_timesteps      | 190800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9189643e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.379        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.05e+04      |\n",
            "|    n_updates            | 19070         |\n",
            "|    policy_gradient_loss | -0.000567     |\n",
            "|    value_loss           | 8.32e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5374207.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.37e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1909         |\n",
            "|    time_elapsed         | 672          |\n",
            "|    total_timesteps      | 190900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003948369 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.388       |\n",
            "|    explained_variance   | 3.46e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.15e+04     |\n",
            "|    n_updates            | 19080        |\n",
            "|    policy_gradient_loss | -0.0027      |\n",
            "|    value_loss           | 5.45e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5377197.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.38e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1910          |\n",
            "|    time_elapsed         | 672           |\n",
            "|    total_timesteps      | 191000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013547334 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.376        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.52e+04      |\n",
            "|    n_updates            | 19090         |\n",
            "|    policy_gradient_loss | -0.000887     |\n",
            "|    value_loss           | 9.53e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5379171.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.38e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1911          |\n",
            "|    time_elapsed         | 672           |\n",
            "|    total_timesteps      | 191100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014424826 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.356        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.62e+04      |\n",
            "|    n_updates            | 19100         |\n",
            "|    policy_gradient_loss | -0.00106      |\n",
            "|    value_loss           | 1.19e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5381229.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.38e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1912         |\n",
            "|    time_elapsed         | 673          |\n",
            "|    total_timesteps      | 191200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010020277 |\n",
            "|    clip_fraction        | 0.00278      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.416       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.14e+04     |\n",
            "|    n_updates            | 19110        |\n",
            "|    policy_gradient_loss | -0.0037      |\n",
            "|    value_loss           | 4.69e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5383959.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.38e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1913          |\n",
            "|    time_elapsed         | 673           |\n",
            "|    total_timesteps      | 191300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029968878 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.438        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.4e+04       |\n",
            "|    n_updates            | 19120         |\n",
            "|    policy_gradient_loss | -0.000532     |\n",
            "|    value_loss           | 6.6e+04       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5385618.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.39e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1914         |\n",
            "|    time_elapsed         | 673          |\n",
            "|    total_timesteps      | 191400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007962066 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.416       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.93e+04     |\n",
            "|    n_updates            | 19130        |\n",
            "|    policy_gradient_loss | -0.00286     |\n",
            "|    value_loss           | 9.55e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5387778.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.39e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1915         |\n",
            "|    time_elapsed         | 674          |\n",
            "|    total_timesteps      | 191500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025224707 |\n",
            "|    clip_fraction        | 0.00825      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.417       |\n",
            "|    explained_variance   | 1.07e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.74e+04     |\n",
            "|    n_updates            | 19140        |\n",
            "|    policy_gradient_loss | -0.00839     |\n",
            "|    value_loss           | 2.81e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5390899.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.39e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1916         |\n",
            "|    time_elapsed         | 674          |\n",
            "|    total_timesteps      | 191600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013770617 |\n",
            "|    clip_fraction        | 0.00356      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.373       |\n",
            "|    explained_variance   | 5.36e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.59e+04     |\n",
            "|    n_updates            | 19150        |\n",
            "|    policy_gradient_loss | -0.00337     |\n",
            "|    value_loss           | 4.96e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5393724.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.39e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1917          |\n",
            "|    time_elapsed         | 674           |\n",
            "|    total_timesteps      | 191700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.6343386e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.296        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.07e+04      |\n",
            "|    n_updates            | 19160         |\n",
            "|    policy_gradient_loss | -0.00111      |\n",
            "|    value_loss           | 1.55e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5396267.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.4e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1918          |\n",
            "|    time_elapsed         | 675           |\n",
            "|    total_timesteps      | 191800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012691153 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.323        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.3e+04       |\n",
            "|    n_updates            | 19170         |\n",
            "|    policy_gradient_loss | -0.00107      |\n",
            "|    value_loss           | 1.25e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5398451.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.4e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1919          |\n",
            "|    time_elapsed         | 675           |\n",
            "|    total_timesteps      | 191900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00019686985 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.465        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.36e+04      |\n",
            "|    n_updates            | 19180         |\n",
            "|    policy_gradient_loss | -0.00146      |\n",
            "|    value_loss           | 7.25e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5400885.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.4e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1920          |\n",
            "|    time_elapsed         | 675           |\n",
            "|    total_timesteps      | 192000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013007823 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.345        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.35e+04      |\n",
            "|    n_updates            | 19190         |\n",
            "|    policy_gradient_loss | -0.00092      |\n",
            "|    value_loss           | 6.14e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5404119.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.4e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1921          |\n",
            "|    time_elapsed         | 676           |\n",
            "|    total_timesteps      | 192100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00048536682 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.379        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.38e+04      |\n",
            "|    n_updates            | 19200         |\n",
            "|    policy_gradient_loss | -0.00323      |\n",
            "|    value_loss           | 8.07e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5407420.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.41e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1922         |\n",
            "|    time_elapsed         | 676          |\n",
            "|    total_timesteps      | 192200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.720372e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.323       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.71e+04     |\n",
            "|    n_updates            | 19210        |\n",
            "|    policy_gradient_loss | -0.000632    |\n",
            "|    value_loss           | 1.89e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5409981.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.41e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1923         |\n",
            "|    time_elapsed         | 676          |\n",
            "|    total_timesteps      | 192300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.258053e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.386       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.9e+04      |\n",
            "|    n_updates            | 19220        |\n",
            "|    policy_gradient_loss | -0.000427    |\n",
            "|    value_loss           | 1.75e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5412335.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.41e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1924         |\n",
            "|    time_elapsed         | 677          |\n",
            "|    total_timesteps      | 192400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027731454 |\n",
            "|    clip_fraction        | 0.0155       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.417       |\n",
            "|    explained_variance   | 1.37e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.7e+04      |\n",
            "|    n_updates            | 19230        |\n",
            "|    policy_gradient_loss | -0.00896     |\n",
            "|    value_loss           | 9.09e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5414818.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.41e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1925          |\n",
            "|    time_elapsed         | 677           |\n",
            "|    total_timesteps      | 192500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00052433764 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.347        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.47e+04      |\n",
            "|    n_updates            | 19240         |\n",
            "|    policy_gradient_loss | -0.00195      |\n",
            "|    value_loss           | 6.51e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5418926.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.42e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1926          |\n",
            "|    time_elapsed         | 677           |\n",
            "|    total_timesteps      | 192600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038724826 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.388        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.34e+04      |\n",
            "|    n_updates            | 19250         |\n",
            "|    policy_gradient_loss | -0.00308      |\n",
            "|    value_loss           | 1.16e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5421216.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 5.42e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 284        |\n",
            "|    iterations           | 1927       |\n",
            "|    time_elapsed         | 678        |\n",
            "|    total_timesteps      | 192700     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 5.5176e-05 |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.297     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.53e+05   |\n",
            "|    n_updates            | 19260      |\n",
            "|    policy_gradient_loss | 5.73e-05   |\n",
            "|    value_loss           | 3.13e+05   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 5424038.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.42e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1928          |\n",
            "|    time_elapsed         | 678           |\n",
            "|    total_timesteps      | 192800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010683046 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.39         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.42e+04      |\n",
            "|    n_updates            | 19270         |\n",
            "|    policy_gradient_loss | -0.000807     |\n",
            "|    value_loss           | 7.31e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5427240.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.43e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1929         |\n",
            "|    time_elapsed         | 679          |\n",
            "|    total_timesteps      | 192900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.996385e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.364       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.14e+04     |\n",
            "|    n_updates            | 19280        |\n",
            "|    policy_gradient_loss | -0.00129     |\n",
            "|    value_loss           | 1.17e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5429949.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.43e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1930          |\n",
            "|    time_elapsed         | 679           |\n",
            "|    total_timesteps      | 193000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029296242 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.4          |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.47e+04      |\n",
            "|    n_updates            | 19290         |\n",
            "|    policy_gradient_loss | -0.00175      |\n",
            "|    value_loss           | 1.52e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5432427.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.43e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1931         |\n",
            "|    time_elapsed         | 679          |\n",
            "|    total_timesteps      | 193100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.342847e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.346       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.05e+04     |\n",
            "|    n_updates            | 19300        |\n",
            "|    policy_gradient_loss | -0.000606    |\n",
            "|    value_loss           | 1.27e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5435167.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.44e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1932         |\n",
            "|    time_elapsed         | 680          |\n",
            "|    total_timesteps      | 193200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.886861e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.406       |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.73e+04     |\n",
            "|    n_updates            | 19310        |\n",
            "|    policy_gradient_loss | -0.00155     |\n",
            "|    value_loss           | 9.42e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5438412.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.44e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1933          |\n",
            "|    time_elapsed         | 680           |\n",
            "|    total_timesteps      | 193300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00032618016 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.359        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.37e+04      |\n",
            "|    n_updates            | 19320         |\n",
            "|    policy_gradient_loss | -0.00226      |\n",
            "|    value_loss           | 1.15e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5440449.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.44e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1934          |\n",
            "|    time_elapsed         | 680           |\n",
            "|    total_timesteps      | 193400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00050540944 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.432        |\n",
            "|    explained_variance   | 2.98e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.5e+04       |\n",
            "|    n_updates            | 19330         |\n",
            "|    policy_gradient_loss | -0.0018       |\n",
            "|    value_loss           | 1.66e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5443096.0\n",
            "--------------------------------------------\n",
            "| episode_reward          | 5.44e+06       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 284            |\n",
            "|    iterations           | 1935           |\n",
            "|    time_elapsed         | 681            |\n",
            "|    total_timesteps      | 193500         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000109100765 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -0.356         |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 3.68e+04       |\n",
            "|    n_updates            | 19340          |\n",
            "|    policy_gradient_loss | -0.000673      |\n",
            "|    value_loss           | 5.8e+04        |\n",
            "--------------------------------------------\n",
            "Episode done — reward = 5445846.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.45e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1936          |\n",
            "|    time_elapsed         | 681           |\n",
            "|    total_timesteps      | 193600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020594383 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.412        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.35e+04      |\n",
            "|    n_updates            | 19350         |\n",
            "|    policy_gradient_loss | -0.00085      |\n",
            "|    value_loss           | 1.12e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5448241.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.45e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1937          |\n",
            "|    time_elapsed         | 681           |\n",
            "|    total_timesteps      | 193700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024309703 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.327        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.81e+04      |\n",
            "|    n_updates            | 19360         |\n",
            "|    policy_gradient_loss | -0.00185      |\n",
            "|    value_loss           | 1.22e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5451495.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.45e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1938          |\n",
            "|    time_elapsed         | 682           |\n",
            "|    total_timesteps      | 193800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.5466254e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.337        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.1e+04       |\n",
            "|    n_updates            | 19370         |\n",
            "|    policy_gradient_loss | -0.000826     |\n",
            "|    value_loss           | 7.81e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5453910.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.45e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1939          |\n",
            "|    time_elapsed         | 682           |\n",
            "|    total_timesteps      | 193900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6256985e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.309        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.38e+04      |\n",
            "|    n_updates            | 19380         |\n",
            "|    policy_gradient_loss | -0.000708     |\n",
            "|    value_loss           | 2.05e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5457165.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.46e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1940          |\n",
            "|    time_elapsed         | 682           |\n",
            "|    total_timesteps      | 194000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017312495 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.338        |\n",
            "|    explained_variance   | 2.98e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.7e+04       |\n",
            "|    n_updates            | 19390         |\n",
            "|    policy_gradient_loss | -0.00129      |\n",
            "|    value_loss           | 8.03e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5458980.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.46e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 1941        |\n",
            "|    time_elapsed         | 683         |\n",
            "|    total_timesteps      | 194100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000508443 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.352      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.58e+04    |\n",
            "|    n_updates            | 19400       |\n",
            "|    policy_gradient_loss | -0.00319    |\n",
            "|    value_loss           | 1.43e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5462478.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.46e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1942         |\n",
            "|    time_elapsed         | 683          |\n",
            "|    total_timesteps      | 194200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016703451 |\n",
            "|    clip_fraction        | 0.00651      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.403       |\n",
            "|    explained_variance   | 2.56e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.85e+04     |\n",
            "|    n_updates            | 19410        |\n",
            "|    policy_gradient_loss | -0.00677     |\n",
            "|    value_loss           | 3.87e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5464254.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.46e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1943          |\n",
            "|    time_elapsed         | 683           |\n",
            "|    total_timesteps      | 194300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.3837303e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.334        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.4e+04       |\n",
            "|    n_updates            | 19420         |\n",
            "|    policy_gradient_loss | 0.000727      |\n",
            "|    value_loss           | 1.55e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5466596.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.47e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1944          |\n",
            "|    time_elapsed         | 684           |\n",
            "|    total_timesteps      | 194400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023708266 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.417        |\n",
            "|    explained_variance   | 1.97e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.74e+04      |\n",
            "|    n_updates            | 19430         |\n",
            "|    policy_gradient_loss | -0.0013       |\n",
            "|    value_loss           | 3.15e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5469163.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.47e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1945         |\n",
            "|    time_elapsed         | 684          |\n",
            "|    total_timesteps      | 194500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010393922 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.406       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.98e+04     |\n",
            "|    n_updates            | 19440        |\n",
            "|    policy_gradient_loss | -0.00237     |\n",
            "|    value_loss           | 7.05e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5471732.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.47e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1946         |\n",
            "|    time_elapsed         | 684          |\n",
            "|    total_timesteps      | 194600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010503309 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.364       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.15e+04     |\n",
            "|    n_updates            | 19450        |\n",
            "|    policy_gradient_loss | -0.00457     |\n",
            "|    value_loss           | 1.1e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5474233.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.47e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1947         |\n",
            "|    time_elapsed         | 685          |\n",
            "|    total_timesteps      | 194700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005418056 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.336       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.55e+04     |\n",
            "|    n_updates            | 19460        |\n",
            "|    policy_gradient_loss | -0.00393     |\n",
            "|    value_loss           | 8.31e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5477731.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.48e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1948         |\n",
            "|    time_elapsed         | 685          |\n",
            "|    total_timesteps      | 194800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005149697 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.45        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.35e+04     |\n",
            "|    n_updates            | 19470        |\n",
            "|    policy_gradient_loss | -0.000995    |\n",
            "|    value_loss           | 6.36e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5481111.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.48e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1949          |\n",
            "|    time_elapsed         | 686           |\n",
            "|    total_timesteps      | 194900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5142187e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.352        |\n",
            "|    explained_variance   | 5.96e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.56e+04      |\n",
            "|    n_updates            | 19480         |\n",
            "|    policy_gradient_loss | -0.000497     |\n",
            "|    value_loss           | 1.56e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5483817.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.48e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1950          |\n",
            "|    time_elapsed         | 686           |\n",
            "|    total_timesteps      | 195000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3388435e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.3          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.54e+04      |\n",
            "|    n_updates            | 19490         |\n",
            "|    policy_gradient_loss | -0.000271     |\n",
            "|    value_loss           | 1.56e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5487008.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.49e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1951          |\n",
            "|    time_elapsed         | 686           |\n",
            "|    total_timesteps      | 195100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00067910727 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.313        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.84e+04      |\n",
            "|    n_updates            | 19500         |\n",
            "|    policy_gradient_loss | -0.00321      |\n",
            "|    value_loss           | 8.24e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5490083.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.49e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1952          |\n",
            "|    time_elapsed         | 687           |\n",
            "|    total_timesteps      | 195200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021485606 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.232        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.19e+04      |\n",
            "|    n_updates            | 19510         |\n",
            "|    policy_gradient_loss | -0.000471     |\n",
            "|    value_loss           | 1.59e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5491857.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.49e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1953         |\n",
            "|    time_elapsed         | 687          |\n",
            "|    total_timesteps      | 195300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.672675e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.353       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.15e+04     |\n",
            "|    n_updates            | 19520        |\n",
            "|    policy_gradient_loss | -0.00111     |\n",
            "|    value_loss           | 1.33e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5494575.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.49e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1954         |\n",
            "|    time_elapsed         | 687          |\n",
            "|    total_timesteps      | 195400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014813924 |\n",
            "|    clip_fraction        | 0.00668      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.373       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.74e+04     |\n",
            "|    n_updates            | 19530        |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 4.15e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5497149.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.5e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1955          |\n",
            "|    time_elapsed         | 688           |\n",
            "|    total_timesteps      | 195500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038127432 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.402        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.15e+04      |\n",
            "|    n_updates            | 19540         |\n",
            "|    policy_gradient_loss | -0.000628     |\n",
            "|    value_loss           | 1.18e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5499356.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.5e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1956         |\n",
            "|    time_elapsed         | 688          |\n",
            "|    total_timesteps      | 195600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004199198 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.351       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.14e+04     |\n",
            "|    n_updates            | 19550        |\n",
            "|    policy_gradient_loss | -0.00144     |\n",
            "|    value_loss           | 8.5e+04      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5501433.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.5e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1957          |\n",
            "|    time_elapsed         | 688           |\n",
            "|    total_timesteps      | 195700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.6117096e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.368        |\n",
            "|    explained_variance   | 1.25e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.96e+04      |\n",
            "|    n_updates            | 19560         |\n",
            "|    policy_gradient_loss | -0.000851     |\n",
            "|    value_loss           | 6e+04         |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5504114.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.5e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1958          |\n",
            "|    time_elapsed         | 689           |\n",
            "|    total_timesteps      | 195800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020491745 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.492        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.76e+04      |\n",
            "|    n_updates            | 19570         |\n",
            "|    policy_gradient_loss | -0.00238      |\n",
            "|    value_loss           | 7.84e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5506539.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.51e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1959         |\n",
            "|    time_elapsed         | 689          |\n",
            "|    total_timesteps      | 195900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005585876 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.329       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.15e+05     |\n",
            "|    n_updates            | 19580        |\n",
            "|    policy_gradient_loss | -0.0035      |\n",
            "|    value_loss           | 1.56e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5509159.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.51e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1960         |\n",
            "|    time_elapsed         | 689          |\n",
            "|    total_timesteps      | 196000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010690715 |\n",
            "|    clip_fraction        | 0.00434      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.336       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.43e+04     |\n",
            "|    n_updates            | 19590        |\n",
            "|    policy_gradient_loss | -0.00405     |\n",
            "|    value_loss           | 5.85e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5513025.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.51e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1961          |\n",
            "|    time_elapsed         | 690           |\n",
            "|    total_timesteps      | 196100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018445216 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.307        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.64e+04      |\n",
            "|    n_updates            | 19600         |\n",
            "|    policy_gradient_loss | -0.00062      |\n",
            "|    value_loss           | 9.57e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5515503.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.52e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1962         |\n",
            "|    time_elapsed         | 690          |\n",
            "|    total_timesteps      | 196200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002546673 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.313       |\n",
            "|    explained_variance   | 1.01e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.13e+05     |\n",
            "|    n_updates            | 19610        |\n",
            "|    policy_gradient_loss | -0.00258     |\n",
            "|    value_loss           | 2.22e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5518468.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.52e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1963          |\n",
            "|    time_elapsed         | 690           |\n",
            "|    total_timesteps      | 196300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024870818 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.343        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.29e+04      |\n",
            "|    n_updates            | 19620         |\n",
            "|    policy_gradient_loss | -0.00107      |\n",
            "|    value_loss           | 1.31e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5521026.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.52e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1964          |\n",
            "|    time_elapsed         | 691           |\n",
            "|    total_timesteps      | 196400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00032077424 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.3          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.17e+04      |\n",
            "|    n_updates            | 19630         |\n",
            "|    policy_gradient_loss | -0.00265      |\n",
            "|    value_loss           | 1.28e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5523788.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.52e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1965          |\n",
            "|    time_elapsed         | 691           |\n",
            "|    total_timesteps      | 196500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014096576 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.347        |\n",
            "|    explained_variance   | 2.32e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.99e+04      |\n",
            "|    n_updates            | 19640         |\n",
            "|    policy_gradient_loss | -0.000911     |\n",
            "|    value_loss           | 9.29e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5526213.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.53e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1966          |\n",
            "|    time_elapsed         | 691           |\n",
            "|    total_timesteps      | 196600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.6677087e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.353        |\n",
            "|    explained_variance   | 9.54e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.13e+04      |\n",
            "|    n_updates            | 19650         |\n",
            "|    policy_gradient_loss | -0.00064      |\n",
            "|    value_loss           | 1.36e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5528590.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.53e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1967         |\n",
            "|    time_elapsed         | 692          |\n",
            "|    total_timesteps      | 196700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016883593 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.386       |\n",
            "|    explained_variance   | 3.76e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.96e+04     |\n",
            "|    n_updates            | 19660        |\n",
            "|    policy_gradient_loss | -0.00673     |\n",
            "|    value_loss           | 5.65e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5531400.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.53e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1968         |\n",
            "|    time_elapsed         | 692          |\n",
            "|    total_timesteps      | 196800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009238136 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.353       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.26e+04     |\n",
            "|    n_updates            | 19670        |\n",
            "|    policy_gradient_loss | -0.00251     |\n",
            "|    value_loss           | 7.62e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5533906.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.53e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1969          |\n",
            "|    time_elapsed         | 692           |\n",
            "|    total_timesteps      | 196900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010495043 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.365        |\n",
            "|    explained_variance   | 2.86e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.49e+04      |\n",
            "|    n_updates            | 19680         |\n",
            "|    policy_gradient_loss | -0.00112      |\n",
            "|    value_loss           | 1.13e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5537574.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.54e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1970          |\n",
            "|    time_elapsed         | 693           |\n",
            "|    total_timesteps      | 197000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00054141856 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.313        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.27e+04      |\n",
            "|    n_updates            | 19690         |\n",
            "|    policy_gradient_loss | -0.00243      |\n",
            "|    value_loss           | 8.62e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5539619.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.54e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1971         |\n",
            "|    time_elapsed         | 693          |\n",
            "|    total_timesteps      | 197100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026519569 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.32        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.72e+04     |\n",
            "|    n_updates            | 19700        |\n",
            "|    policy_gradient_loss | -0.0117      |\n",
            "|    value_loss           | 2.51e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5543542.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.54e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 1972        |\n",
            "|    time_elapsed         | 694         |\n",
            "|    total_timesteps      | 197200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004979304 |\n",
            "|    clip_fraction        | 0.0189      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.409      |\n",
            "|    explained_variance   | 9.54e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.69e+04    |\n",
            "|    n_updates            | 19710       |\n",
            "|    policy_gradient_loss | -0.00837    |\n",
            "|    value_loss           | 5.2e+04     |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5547297.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.55e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1973          |\n",
            "|    time_elapsed         | 694           |\n",
            "|    total_timesteps      | 197300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.4464498e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.261        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.2e+05       |\n",
            "|    n_updates            | 19720         |\n",
            "|    policy_gradient_loss | 0.000992      |\n",
            "|    value_loss           | 2.13e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5549425.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.55e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1974          |\n",
            "|    time_elapsed         | 694           |\n",
            "|    total_timesteps      | 197400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.1012293e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.29         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.28e+05      |\n",
            "|    n_updates            | 19730         |\n",
            "|    policy_gradient_loss | 0.000107      |\n",
            "|    value_loss           | 2.49e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5552701.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.55e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1975         |\n",
            "|    time_elapsed         | 695          |\n",
            "|    total_timesteps      | 197500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004547672 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.354       |\n",
            "|    explained_variance   | 3.58e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.86e+04     |\n",
            "|    n_updates            | 19740        |\n",
            "|    policy_gradient_loss | -0.00215     |\n",
            "|    value_loss           | 6.39e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5555555.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.56e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1976          |\n",
            "|    time_elapsed         | 695           |\n",
            "|    total_timesteps      | 197600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00072457275 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.333        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.86e+04      |\n",
            "|    n_updates            | 19750         |\n",
            "|    policy_gradient_loss | -0.00158      |\n",
            "|    value_loss           | 1.56e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5557702.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.56e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1977         |\n",
            "|    time_elapsed         | 695          |\n",
            "|    total_timesteps      | 197700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005722218 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.299       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.38e+04     |\n",
            "|    n_updates            | 19760        |\n",
            "|    policy_gradient_loss | -0.00252     |\n",
            "|    value_loss           | 1.23e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5561514.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.56e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1978          |\n",
            "|    time_elapsed         | 696           |\n",
            "|    total_timesteps      | 197800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00026502227 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.352        |\n",
            "|    explained_variance   | 1.07e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.42e+04      |\n",
            "|    n_updates            | 19770         |\n",
            "|    policy_gradient_loss | -0.00107      |\n",
            "|    value_loss           | 7.26e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5563572.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.56e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1979         |\n",
            "|    time_elapsed         | 696          |\n",
            "|    total_timesteps      | 197900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.737271e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.276       |\n",
            "|    explained_variance   | 9.54e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.11e+05     |\n",
            "|    n_updates            | 19780        |\n",
            "|    policy_gradient_loss | -0.000494    |\n",
            "|    value_loss           | 2.05e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5565966.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 5.57e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 1980        |\n",
            "|    time_elapsed         | 696         |\n",
            "|    total_timesteps      | 198000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 9.90743e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.321      |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.76e+04    |\n",
            "|    n_updates            | 19790       |\n",
            "|    policy_gradient_loss | -0.000611   |\n",
            "|    value_loss           | 9.01e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 5567962.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.57e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1981          |\n",
            "|    time_elapsed         | 697           |\n",
            "|    total_timesteps      | 198100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018602223 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.391        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.28e+04      |\n",
            "|    n_updates            | 19800         |\n",
            "|    policy_gradient_loss | -0.00179      |\n",
            "|    value_loss           | 6.08e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5570613.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.57e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1982          |\n",
            "|    time_elapsed         | 697           |\n",
            "|    total_timesteps      | 198200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022711647 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.316        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.5e+04       |\n",
            "|    n_updates            | 19810         |\n",
            "|    policy_gradient_loss | -0.00218      |\n",
            "|    value_loss           | 4.09e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5573909.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.57e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1983          |\n",
            "|    time_elapsed         | 697           |\n",
            "|    total_timesteps      | 198300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0236268e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.259        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.39e+04      |\n",
            "|    n_updates            | 19820         |\n",
            "|    policy_gradient_loss | -3.84e-05     |\n",
            "|    value_loss           | 8.87e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5576298.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.58e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1984         |\n",
            "|    time_elapsed         | 698          |\n",
            "|    total_timesteps      | 198400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019847304 |\n",
            "|    clip_fraction        | 0.00434      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.325       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.42e+04     |\n",
            "|    n_updates            | 19830        |\n",
            "|    policy_gradient_loss | -0.00573     |\n",
            "|    value_loss           | 1.78e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5577575.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.58e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1985         |\n",
            "|    time_elapsed         | 698          |\n",
            "|    total_timesteps      | 198500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010353825 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.303       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.36e+04     |\n",
            "|    n_updates            | 19840        |\n",
            "|    policy_gradient_loss | -0.00264     |\n",
            "|    value_loss           | 6.09e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5579902.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.58e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1986         |\n",
            "|    time_elapsed         | 698          |\n",
            "|    total_timesteps      | 198600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028192378 |\n",
            "|    clip_fraction        | 0.00807      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.49        |\n",
            "|    explained_variance   | 2.92e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.42e+03     |\n",
            "|    n_updates            | 19850        |\n",
            "|    policy_gradient_loss | -0.00621     |\n",
            "|    value_loss           | 1.7e+04      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5583657.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.58e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1987          |\n",
            "|    time_elapsed         | 699           |\n",
            "|    total_timesteps      | 198700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00069065054 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.332        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.27e+04      |\n",
            "|    n_updates            | 19860         |\n",
            "|    policy_gradient_loss | -0.00426      |\n",
            "|    value_loss           | 3.95e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5586195.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.59e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1988          |\n",
            "|    time_elapsed         | 699           |\n",
            "|    total_timesteps      | 198800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016509798 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.343        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.04e+05      |\n",
            "|    n_updates            | 19870         |\n",
            "|    policy_gradient_loss | -0.000319     |\n",
            "|    value_loss           | 2.03e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5589016.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.59e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1989         |\n",
            "|    time_elapsed         | 699          |\n",
            "|    total_timesteps      | 198900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.674347e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.283       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.69e+04     |\n",
            "|    n_updates            | 19880        |\n",
            "|    policy_gradient_loss | -0.000281    |\n",
            "|    value_loss           | 1.02e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5591045.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.59e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1990         |\n",
            "|    time_elapsed         | 700          |\n",
            "|    total_timesteps      | 199000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015477659 |\n",
            "|    clip_fraction        | 0.00356      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.307       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.04e+04     |\n",
            "|    n_updates            | 19890        |\n",
            "|    policy_gradient_loss | -0.00457     |\n",
            "|    value_loss           | 8.46e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5594197.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.59e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1991         |\n",
            "|    time_elapsed         | 700          |\n",
            "|    total_timesteps      | 199100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018317632 |\n",
            "|    clip_fraction        | 0.00625      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.351       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.89e+04     |\n",
            "|    n_updates            | 19900        |\n",
            "|    policy_gradient_loss | -0.00575     |\n",
            "|    value_loss           | 3.5e+04      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5596683.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.6e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1992          |\n",
            "|    time_elapsed         | 700           |\n",
            "|    total_timesteps      | 199200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029122026 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.319        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.64e+04      |\n",
            "|    n_updates            | 19910         |\n",
            "|    policy_gradient_loss | 0.000645      |\n",
            "|    value_loss           | 1.29e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5599146.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.6e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1993          |\n",
            "|    time_elapsed         | 701           |\n",
            "|    total_timesteps      | 199300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027492346 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.359        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.1e+04       |\n",
            "|    n_updates            | 19920         |\n",
            "|    policy_gradient_loss | -0.00197      |\n",
            "|    value_loss           | 6.98e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5601712.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.6e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1994         |\n",
            "|    time_elapsed         | 701          |\n",
            "|    total_timesteps      | 199400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004924676 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.424       |\n",
            "|    explained_variance   | 1.55e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.65e+04     |\n",
            "|    n_updates            | 19930        |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    value_loss           | 5.45e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5604503.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.6e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1995          |\n",
            "|    time_elapsed         | 702           |\n",
            "|    total_timesteps      | 199500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00036568462 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.314        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.25e+04      |\n",
            "|    n_updates            | 19940         |\n",
            "|    policy_gradient_loss | -0.00208      |\n",
            "|    value_loss           | 9.56e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5606311.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.61e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1996         |\n",
            "|    time_elapsed         | 702          |\n",
            "|    total_timesteps      | 199600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013474313 |\n",
            "|    clip_fraction        | 0.00217      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.395       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.44e+04     |\n",
            "|    n_updates            | 19950        |\n",
            "|    policy_gradient_loss | -0.00532     |\n",
            "|    value_loss           | 1.1e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5608932.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.61e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 1997         |\n",
            "|    time_elapsed         | 702          |\n",
            "|    total_timesteps      | 199700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011921821 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.412       |\n",
            "|    explained_variance   | 9.54e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.92e+04     |\n",
            "|    n_updates            | 19960        |\n",
            "|    policy_gradient_loss | -0.00301     |\n",
            "|    value_loss           | 3.9e+04      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 5611620.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.61e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1998          |\n",
            "|    time_elapsed         | 703           |\n",
            "|    total_timesteps      | 199800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00041340722 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.324        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.59e+04      |\n",
            "|    n_updates            | 19970         |\n",
            "|    policy_gradient_loss | -0.00178      |\n",
            "|    value_loss           | 7.31e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5613512.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 5.61e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 284           |\n",
            "|    iterations           | 1999          |\n",
            "|    time_elapsed         | 703           |\n",
            "|    total_timesteps      | 199900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015464208 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.438        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.16e+04      |\n",
            "|    n_updates            | 19980         |\n",
            "|    policy_gradient_loss | -0.000507     |\n",
            "|    value_loss           | 8.97e+04      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 5615673.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 5.62e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 284          |\n",
            "|    iterations           | 2000         |\n",
            "|    time_elapsed         | 703          |\n",
            "|    total_timesteps      | 200000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028989986 |\n",
            "|    clip_fraction        | 0.00729      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.363       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.24e+04     |\n",
            "|    n_updates            | 19990        |\n",
            "|    policy_gradient_loss | -0.0098      |\n",
            "|    value_loss           | 3.55e+04     |\n",
            "------------------------------------------\n",
            "Running LOOK benchmark...\n",
            "42\n",
            "\n",
            "Total reward for LOOK benchmark over 100 steps: 882.0\n",
            "info[\"done\"]: 53\n",
            "Average reward: 1114.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_ppo(SB3_PPO, 6, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kkmoT1eTutqu",
        "outputId": "6338b44e-46a7-46c6-cbdf-86805b162793"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "|    time_elapsed         | 673         |\n",
            "|    total_timesteps      | 175100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004702367 |\n",
            "|    clip_fraction        | 0.0053      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.23e+05    |\n",
            "|    n_updates            | 17500       |\n",
            "|    policy_gradient_loss | -0.00983    |\n",
            "|    value_loss           | 1.9e+05     |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6458988.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.46e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1752         |\n",
            "|    time_elapsed         | 674          |\n",
            "|    total_timesteps      | 175200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038955885 |\n",
            "|    clip_fraction        | 0.00868      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.58e+04     |\n",
            "|    n_updates            | 17510        |\n",
            "|    policy_gradient_loss | -0.0105      |\n",
            "|    value_loss           | 1.92e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6463428.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.46e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1753         |\n",
            "|    time_elapsed         | 674          |\n",
            "|    total_timesteps      | 175300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011642633 |\n",
            "|    clip_fraction        | 0.00217      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.57e+04     |\n",
            "|    n_updates            | 17520        |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    value_loss           | 1.91e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6465198.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.47e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1754         |\n",
            "|    time_elapsed         | 675          |\n",
            "|    total_timesteps      | 175400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005367701 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.912       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.5e+05      |\n",
            "|    n_updates            | 17530        |\n",
            "|    policy_gradient_loss | -0.00301     |\n",
            "|    value_loss           | 3.2e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6469683.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.47e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 259         |\n",
            "|    iterations           | 1755        |\n",
            "|    time_elapsed         | 675         |\n",
            "|    total_timesteps      | 175500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009068708 |\n",
            "|    clip_fraction        | 0.0424      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.36e+04    |\n",
            "|    n_updates            | 17540       |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    value_loss           | 3.11e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6473041.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.47e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1756         |\n",
            "|    time_elapsed         | 675          |\n",
            "|    total_timesteps      | 175600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009762851 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.985       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.94e+05     |\n",
            "|    n_updates            | 17550        |\n",
            "|    policy_gradient_loss | -0.00316     |\n",
            "|    value_loss           | 3.67e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6476438.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.48e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 259           |\n",
            "|    iterations           | 1757          |\n",
            "|    time_elapsed         | 676           |\n",
            "|    total_timesteps      | 175700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00079866755 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.01         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.27e+04      |\n",
            "|    n_updates            | 17560         |\n",
            "|    policy_gradient_loss | -0.00328      |\n",
            "|    value_loss           | 1.23e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6480633.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.48e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1758         |\n",
            "|    time_elapsed         | 676          |\n",
            "|    total_timesteps      | 175800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014776577 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.04e+04     |\n",
            "|    n_updates            | 17570        |\n",
            "|    policy_gradient_loss | -0.00515     |\n",
            "|    value_loss           | 1.71e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6484991.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.48e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 259           |\n",
            "|    iterations           | 1759          |\n",
            "|    time_elapsed         | 676           |\n",
            "|    total_timesteps      | 175900        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00091956015 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1            |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.72e+05      |\n",
            "|    n_updates            | 17580         |\n",
            "|    policy_gradient_loss | -0.00197      |\n",
            "|    value_loss           | 3.72e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6488325.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.49e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 259           |\n",
            "|    iterations           | 1760          |\n",
            "|    time_elapsed         | 677           |\n",
            "|    total_timesteps      | 176000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012824839 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.04         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.07e+05      |\n",
            "|    n_updates            | 17590         |\n",
            "|    policy_gradient_loss | -0.000511     |\n",
            "|    value_loss           | 4.23e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6491488.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.49e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1761         |\n",
            "|    time_elapsed         | 677          |\n",
            "|    total_timesteps      | 176100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017312171 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 2.56e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.35e+05     |\n",
            "|    n_updates            | 17600        |\n",
            "|    policy_gradient_loss | -0.00622     |\n",
            "|    value_loss           | 2.33e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6495074.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.5e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1762         |\n",
            "|    time_elapsed         | 678          |\n",
            "|    total_timesteps      | 176200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010463122 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.965       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.51e+04     |\n",
            "|    n_updates            | 17610        |\n",
            "|    policy_gradient_loss | -0.00315     |\n",
            "|    value_loss           | 1.91e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6497915.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.5e+06     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 259         |\n",
            "|    iterations           | 1763        |\n",
            "|    time_elapsed         | 678         |\n",
            "|    total_timesteps      | 176300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001368905 |\n",
            "|    clip_fraction        | 0.00373     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 1.01e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.6e+05     |\n",
            "|    n_updates            | 17620       |\n",
            "|    policy_gradient_loss | -0.00652    |\n",
            "|    value_loss           | 3.01e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6500989.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.5e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1764         |\n",
            "|    time_elapsed         | 678          |\n",
            "|    total_timesteps      | 176400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026977528 |\n",
            "|    clip_fraction        | 0.00434      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.89e+04     |\n",
            "|    n_updates            | 17630        |\n",
            "|    policy_gradient_loss | -0.00854     |\n",
            "|    value_loss           | 1.51e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6504511.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.5e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1765         |\n",
            "|    time_elapsed         | 679          |\n",
            "|    total_timesteps      | 176500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007622638 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.997       |\n",
            "|    explained_variance   | 3.58e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.97e+04     |\n",
            "|    n_updates            | 17640        |\n",
            "|    policy_gradient_loss | -0.00228     |\n",
            "|    value_loss           | 1.37e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6509916.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.51e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1766         |\n",
            "|    time_elapsed         | 679          |\n",
            "|    total_timesteps      | 176600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006929826 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.9e+04      |\n",
            "|    n_updates            | 17650        |\n",
            "|    policy_gradient_loss | -0.0027      |\n",
            "|    value_loss           | 1.99e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6512968.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.51e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1767         |\n",
            "|    time_elapsed         | 679          |\n",
            "|    total_timesteps      | 176700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021715628 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.69e+05     |\n",
            "|    n_updates            | 17660        |\n",
            "|    policy_gradient_loss | -0.00868     |\n",
            "|    value_loss           | 5.53e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6516526.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.52e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 259           |\n",
            "|    iterations           | 1768          |\n",
            "|    time_elapsed         | 680           |\n",
            "|    total_timesteps      | 176800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00094490673 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.984        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.42e+04      |\n",
            "|    n_updates            | 17670         |\n",
            "|    policy_gradient_loss | -0.00436      |\n",
            "|    value_loss           | 1.53e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6520052.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.52e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1769         |\n",
            "|    time_elapsed         | 680          |\n",
            "|    total_timesteps      | 176900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003467874 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.968       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.25e+05     |\n",
            "|    n_updates            | 17680        |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 2.31e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6522440.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.52e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 259         |\n",
            "|    iterations           | 1770        |\n",
            "|    time_elapsed         | 680         |\n",
            "|    total_timesteps      | 177000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001050348 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.45e+05    |\n",
            "|    n_updates            | 17690       |\n",
            "|    policy_gradient_loss | -0.00402    |\n",
            "|    value_loss           | 2.4e+05     |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6526048.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.53e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1771         |\n",
            "|    time_elapsed         | 681          |\n",
            "|    total_timesteps      | 177100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031921784 |\n",
            "|    clip_fraction        | 0.0102       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.79e+04     |\n",
            "|    n_updates            | 17700        |\n",
            "|    policy_gradient_loss | -0.0119      |\n",
            "|    value_loss           | 7.49e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6530166.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.53e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1772         |\n",
            "|    time_elapsed         | 681          |\n",
            "|    total_timesteps      | 177200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020692807 |\n",
            "|    clip_fraction        | 0.00946      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.965       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.5e+04      |\n",
            "|    n_updates            | 17710        |\n",
            "|    policy_gradient_loss | -0.0109      |\n",
            "|    value_loss           | 2.1e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6533042.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.53e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 259         |\n",
            "|    iterations           | 1773        |\n",
            "|    time_elapsed         | 682         |\n",
            "|    total_timesteps      | 177300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001159958 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 3.58e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.37e+05    |\n",
            "|    n_updates            | 17720       |\n",
            "|    policy_gradient_loss | -0.00516    |\n",
            "|    value_loss           | 2.69e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6536964.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.54e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 259         |\n",
            "|    iterations           | 1774        |\n",
            "|    time_elapsed         | 682         |\n",
            "|    total_timesteps      | 177400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008522399 |\n",
            "|    clip_fraction        | 0.0299      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.81e+04    |\n",
            "|    n_updates            | 17730       |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    value_loss           | 1.17e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6540033.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.54e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 259         |\n",
            "|    iterations           | 1775        |\n",
            "|    time_elapsed         | 682         |\n",
            "|    total_timesteps      | 177500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002967031 |\n",
            "|    clip_fraction        | 0.0118      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 2.38e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.24e+05    |\n",
            "|    n_updates            | 17740       |\n",
            "|    policy_gradient_loss | -0.00429    |\n",
            "|    value_loss           | 2.48e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6542547.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.54e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1776         |\n",
            "|    time_elapsed         | 683          |\n",
            "|    total_timesteps      | 177600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002708893 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.14e+04     |\n",
            "|    n_updates            | 17750        |\n",
            "|    policy_gradient_loss | -0.00188     |\n",
            "|    value_loss           | 2.47e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6545563.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.55e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1777         |\n",
            "|    time_elapsed         | 683          |\n",
            "|    total_timesteps      | 177700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010954685 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.47e+04     |\n",
            "|    n_updates            | 17760        |\n",
            "|    policy_gradient_loss | -0.00444     |\n",
            "|    value_loss           | 1.18e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6548895.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.55e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1778         |\n",
            "|    time_elapsed         | 683          |\n",
            "|    total_timesteps      | 177800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017141722 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.31e+04     |\n",
            "|    n_updates            | 17770        |\n",
            "|    policy_gradient_loss | -0.00439     |\n",
            "|    value_loss           | 1.99e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6552033.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.55e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1779         |\n",
            "|    time_elapsed         | 684          |\n",
            "|    total_timesteps      | 177900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017633482 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.85e+04     |\n",
            "|    n_updates            | 17780        |\n",
            "|    policy_gradient_loss | -0.00631     |\n",
            "|    value_loss           | 1.31e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6555207.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.56e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1780         |\n",
            "|    time_elapsed         | 684          |\n",
            "|    total_timesteps      | 178000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013655359 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.25e+04     |\n",
            "|    n_updates            | 17790        |\n",
            "|    policy_gradient_loss | -0.0072      |\n",
            "|    value_loss           | 1.71e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6559228.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.56e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 259           |\n",
            "|    iterations           | 1781          |\n",
            "|    time_elapsed         | 685           |\n",
            "|    total_timesteps      | 178100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00078655686 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.05         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.29e+04      |\n",
            "|    n_updates            | 17800         |\n",
            "|    policy_gradient_loss | -0.00228      |\n",
            "|    value_loss           | 2.07e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6563160.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.56e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1782         |\n",
            "|    time_elapsed         | 685          |\n",
            "|    total_timesteps      | 178200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010638082 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.66e+05     |\n",
            "|    n_updates            | 17810        |\n",
            "|    policy_gradient_loss | -0.00477     |\n",
            "|    value_loss           | 3.06e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6566595.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.57e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 259           |\n",
            "|    iterations           | 1783          |\n",
            "|    time_elapsed         | 685           |\n",
            "|    total_timesteps      | 178300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027677944 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.04         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.28e+05      |\n",
            "|    n_updates            | 17820         |\n",
            "|    policy_gradient_loss | -0.00231      |\n",
            "|    value_loss           | 3.89e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6569596.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.57e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1784         |\n",
            "|    time_elapsed         | 686          |\n",
            "|    total_timesteps      | 178400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007490121 |\n",
            "|    clip_fraction        | 0.00278      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16e+05     |\n",
            "|    n_updates            | 17830        |\n",
            "|    policy_gradient_loss | -0.00425     |\n",
            "|    value_loss           | 1.83e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6571712.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.57e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1785         |\n",
            "|    time_elapsed         | 686          |\n",
            "|    total_timesteps      | 178500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025593513 |\n",
            "|    clip_fraction        | 0.00295      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.86e+04     |\n",
            "|    n_updates            | 17840        |\n",
            "|    policy_gradient_loss | -0.00647     |\n",
            "|    value_loss           | 1.22e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6576112.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.58e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1786         |\n",
            "|    time_elapsed         | 686          |\n",
            "|    total_timesteps      | 178600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021007184 |\n",
            "|    clip_fraction        | 0.00451      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.61e+04     |\n",
            "|    n_updates            | 17850        |\n",
            "|    policy_gradient_loss | -0.00692     |\n",
            "|    value_loss           | 4.91e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6579715.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.58e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 259           |\n",
            "|    iterations           | 1787          |\n",
            "|    time_elapsed         | 687           |\n",
            "|    total_timesteps      | 178700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00093909353 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.07         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.51e+05      |\n",
            "|    n_updates            | 17860         |\n",
            "|    policy_gradient_loss | -0.00199      |\n",
            "|    value_loss           | 3.47e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6583719.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.58e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 1788         |\n",
            "|    time_elapsed         | 687          |\n",
            "|    total_timesteps      | 178800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021688037 |\n",
            "|    clip_fraction        | 0.00373      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.12        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.21e+05     |\n",
            "|    n_updates            | 17870        |\n",
            "|    policy_gradient_loss | -0.00954     |\n",
            "|    value_loss           | 2.63e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6586092.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.59e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1789        |\n",
            "|    time_elapsed         | 688         |\n",
            "|    total_timesteps      | 178900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001756107 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.17e+05    |\n",
            "|    n_updates            | 17880       |\n",
            "|    policy_gradient_loss | -0.00484    |\n",
            "|    value_loss           | 2.39e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6590904.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 6.59e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1790       |\n",
            "|    time_elapsed         | 688        |\n",
            "|    total_timesteps      | 179000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00807656 |\n",
            "|    clip_fraction        | 0.0427     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.04      |\n",
            "|    explained_variance   | 2.38e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.15e+04   |\n",
            "|    n_updates            | 17890      |\n",
            "|    policy_gradient_loss | -0.0188    |\n",
            "|    value_loss           | 7.06e+04   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 6593287.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.59e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1791         |\n",
            "|    time_elapsed         | 688          |\n",
            "|    total_timesteps      | 179100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014529533 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.919       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.54e+05     |\n",
            "|    n_updates            | 17900        |\n",
            "|    policy_gradient_loss | -0.00437     |\n",
            "|    value_loss           | 3.42e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6596720.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 6.6e+06    |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1792       |\n",
            "|    time_elapsed         | 689        |\n",
            "|    total_timesteps      | 179200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01379869 |\n",
            "|    clip_fraction        | 0.0563     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.01      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.37e+04   |\n",
            "|    n_updates            | 17910      |\n",
            "|    policy_gradient_loss | -0.0259    |\n",
            "|    value_loss           | 7.37e+04   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 6600773.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.6e+06     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1793        |\n",
            "|    time_elapsed         | 689         |\n",
            "|    total_timesteps      | 179300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003720115 |\n",
            "|    clip_fraction        | 0.00868     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.947      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.29e+04    |\n",
            "|    n_updates            | 17920       |\n",
            "|    policy_gradient_loss | -0.00769    |\n",
            "|    value_loss           | 1.67e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6603921.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.6e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1794          |\n",
            "|    time_elapsed         | 689           |\n",
            "|    total_timesteps      | 179400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031561637 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.955        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.84e+05      |\n",
            "|    n_updates            | 17930         |\n",
            "|    policy_gradient_loss | -0.00173      |\n",
            "|    value_loss           | 3.4e+05       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6606754.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.61e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1795        |\n",
            "|    time_elapsed         | 690         |\n",
            "|    total_timesteps      | 179500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004291777 |\n",
            "|    clip_fraction        | 0.0146      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.01e+05    |\n",
            "|    n_updates            | 17940       |\n",
            "|    policy_gradient_loss | -0.00812    |\n",
            "|    value_loss           | 1.78e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6610217.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.61e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1796         |\n",
            "|    time_elapsed         | 690          |\n",
            "|    total_timesteps      | 179600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029137272 |\n",
            "|    clip_fraction        | 0.00712      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.877       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.54e+04     |\n",
            "|    n_updates            | 17950        |\n",
            "|    policy_gradient_loss | -0.00692     |\n",
            "|    value_loss           | 1.71e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6613727.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.61e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1797         |\n",
            "|    time_elapsed         | 691          |\n",
            "|    total_timesteps      | 179700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015646573 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.51e+04     |\n",
            "|    n_updates            | 17960        |\n",
            "|    policy_gradient_loss | -0.00513     |\n",
            "|    value_loss           | 2.05e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6617317.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.62e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1798         |\n",
            "|    time_elapsed         | 691          |\n",
            "|    total_timesteps      | 179800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010889443 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.977       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.05e+04     |\n",
            "|    n_updates            | 17970        |\n",
            "|    policy_gradient_loss | -0.00194     |\n",
            "|    value_loss           | 1.83e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6620953.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.62e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1799         |\n",
            "|    time_elapsed         | 691          |\n",
            "|    total_timesteps      | 179900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012506765 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.987       |\n",
            "|    explained_variance   | 6.56e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.11e+04     |\n",
            "|    n_updates            | 17980        |\n",
            "|    policy_gradient_loss | -0.00322     |\n",
            "|    value_loss           | 2e+05        |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6623515.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.62e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1800         |\n",
            "|    time_elapsed         | 692          |\n",
            "|    total_timesteps      | 180000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015130255 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.52e+05     |\n",
            "|    n_updates            | 17990        |\n",
            "|    policy_gradient_loss | -0.00484     |\n",
            "|    value_loss           | 2.23e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6625709.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.63e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1801        |\n",
            "|    time_elapsed         | 692         |\n",
            "|    total_timesteps      | 180100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003927849 |\n",
            "|    clip_fraction        | 0.00434     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.971      |\n",
            "|    explained_variance   | 8.34e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.44e+04    |\n",
            "|    n_updates            | 18000       |\n",
            "|    policy_gradient_loss | -0.00927    |\n",
            "|    value_loss           | 8.17e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6629678.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 6.63e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1802       |\n",
            "|    time_elapsed         | 692        |\n",
            "|    total_timesteps      | 180200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00215501 |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.952     |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.22e+04   |\n",
            "|    n_updates            | 18010      |\n",
            "|    policy_gradient_loss | -0.00351   |\n",
            "|    value_loss           | 7.24e+04   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 6632748.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.63e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1803          |\n",
            "|    time_elapsed         | 693           |\n",
            "|    total_timesteps      | 180300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00057121273 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.02         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+05      |\n",
            "|    n_updates            | 18020         |\n",
            "|    policy_gradient_loss | -0.00283      |\n",
            "|    value_loss           | 3.12e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6637225.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.64e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1804         |\n",
            "|    time_elapsed         | 693          |\n",
            "|    total_timesteps      | 180400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054814015 |\n",
            "|    clip_fraction        | 0.0255       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.11e+04     |\n",
            "|    n_updates            | 18030        |\n",
            "|    policy_gradient_loss | -0.016       |\n",
            "|    value_loss           | 1.45e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6641314.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.64e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1805         |\n",
            "|    time_elapsed         | 693          |\n",
            "|    total_timesteps      | 180500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017586483 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.965       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.49e+05     |\n",
            "|    n_updates            | 18040        |\n",
            "|    policy_gradient_loss | -0.00534     |\n",
            "|    value_loss           | 3.08e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6643733.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.64e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1806         |\n",
            "|    time_elapsed         | 694          |\n",
            "|    total_timesteps      | 180600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041277367 |\n",
            "|    clip_fraction        | 0.0181       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 3.28e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.07e+05     |\n",
            "|    n_updates            | 18050        |\n",
            "|    policy_gradient_loss | -0.0118      |\n",
            "|    value_loss           | 2.26e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6646422.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.65e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1807        |\n",
            "|    time_elapsed         | 694         |\n",
            "|    total_timesteps      | 180700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003673749 |\n",
            "|    clip_fraction        | 0.00295     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.951      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.46e+04    |\n",
            "|    n_updates            | 18060       |\n",
            "|    policy_gradient_loss | -0.0098     |\n",
            "|    value_loss           | 9.38e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6649940.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.65e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1808         |\n",
            "|    time_elapsed         | 695          |\n",
            "|    total_timesteps      | 180800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018755128 |\n",
            "|    clip_fraction        | 0.00356      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.967       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.93e+04     |\n",
            "|    n_updates            | 18070        |\n",
            "|    policy_gradient_loss | -0.00188     |\n",
            "|    value_loss           | 1.07e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6653638.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.65e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1809         |\n",
            "|    time_elapsed         | 695          |\n",
            "|    total_timesteps      | 180900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012118549 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.07e+05     |\n",
            "|    n_updates            | 18080        |\n",
            "|    policy_gradient_loss | -0.00483     |\n",
            "|    value_loss           | 2.25e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6655625.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.66e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1810        |\n",
            "|    time_elapsed         | 695         |\n",
            "|    total_timesteps      | 181000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003010818 |\n",
            "|    clip_fraction        | 0.00295     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.29e+04    |\n",
            "|    n_updates            | 18090       |\n",
            "|    policy_gradient_loss | -0.00938    |\n",
            "|    value_loss           | 1.78e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6659680.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.66e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1811         |\n",
            "|    time_elapsed         | 696          |\n",
            "|    total_timesteps      | 181100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012151448 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.85e+04     |\n",
            "|    n_updates            | 18100        |\n",
            "|    policy_gradient_loss | -0.00558     |\n",
            "|    value_loss           | 4.4e+04      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6662600.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.66e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1812          |\n",
            "|    time_elapsed         | 696           |\n",
            "|    total_timesteps      | 181200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00036320311 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.07         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.75e+05      |\n",
            "|    n_updates            | 18110         |\n",
            "|    policy_gradient_loss | -0.000694     |\n",
            "|    value_loss           | 2.85e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6665266.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.67e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1813         |\n",
            "|    time_elapsed         | 696          |\n",
            "|    total_timesteps      | 181300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008674762 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.29e+04     |\n",
            "|    n_updates            | 18120        |\n",
            "|    policy_gradient_loss | -0.00271     |\n",
            "|    value_loss           | 1.51e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6668511.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.67e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1814         |\n",
            "|    time_elapsed         | 697          |\n",
            "|    total_timesteps      | 181400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050145434 |\n",
            "|    clip_fraction        | 0.00729      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.992       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.89e+04     |\n",
            "|    n_updates            | 18130        |\n",
            "|    policy_gradient_loss | -0.0133      |\n",
            "|    value_loss           | 8.98e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6671859.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.67e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1815          |\n",
            "|    time_elapsed         | 697           |\n",
            "|    total_timesteps      | 181500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00060992443 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1            |\n",
            "|    explained_variance   | 5.96e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.21e+05      |\n",
            "|    n_updates            | 18140         |\n",
            "|    policy_gradient_loss | 0.000968      |\n",
            "|    value_loss           | 2.19e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6675715.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.68e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1816          |\n",
            "|    time_elapsed         | 698           |\n",
            "|    total_timesteps      | 181600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00051804644 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.976        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.68e+04      |\n",
            "|    n_updates            | 18150         |\n",
            "|    policy_gradient_loss | -0.00223      |\n",
            "|    value_loss           | 1.74e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6678693.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.68e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1817         |\n",
            "|    time_elapsed         | 698          |\n",
            "|    total_timesteps      | 181700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010848938 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.92        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.28e+05     |\n",
            "|    n_updates            | 18160        |\n",
            "|    policy_gradient_loss | -0.00606     |\n",
            "|    value_loss           | 2.54e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6681434.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.68e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1818         |\n",
            "|    time_elapsed         | 698          |\n",
            "|    total_timesteps      | 181800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012884196 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1e+05        |\n",
            "|    n_updates            | 18170        |\n",
            "|    policy_gradient_loss | -0.00577     |\n",
            "|    value_loss           | 1.6e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6685924.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.69e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1819         |\n",
            "|    time_elapsed         | 699          |\n",
            "|    total_timesteps      | 181900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027700071 |\n",
            "|    clip_fraction        | 0.00573      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.936       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.29e+04     |\n",
            "|    n_updates            | 18180        |\n",
            "|    policy_gradient_loss | -0.00459     |\n",
            "|    value_loss           | 1.18e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6688512.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.69e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1820          |\n",
            "|    time_elapsed         | 699           |\n",
            "|    total_timesteps      | 182000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027612637 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.98         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.26e+05      |\n",
            "|    n_updates            | 18190         |\n",
            "|    policy_gradient_loss | -0.0013       |\n",
            "|    value_loss           | 3.95e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6692601.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.69e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1821         |\n",
            "|    time_elapsed         | 699          |\n",
            "|    total_timesteps      | 182100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027881074 |\n",
            "|    clip_fraction        | 0.00434      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.6e+04      |\n",
            "|    n_updates            | 18200        |\n",
            "|    policy_gradient_loss | -0.00807     |\n",
            "|    value_loss           | 9.15e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6695505.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.7e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1822         |\n",
            "|    time_elapsed         | 700          |\n",
            "|    total_timesteps      | 182200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030682406 |\n",
            "|    clip_fraction        | 0.0114       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.941       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.64e+05     |\n",
            "|    n_updates            | 18210        |\n",
            "|    policy_gradient_loss | -0.0104      |\n",
            "|    value_loss           | 2.82e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6699756.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.7e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1823         |\n",
            "|    time_elapsed         | 700          |\n",
            "|    total_timesteps      | 182300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013468773 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.69e+04     |\n",
            "|    n_updates            | 18220        |\n",
            "|    policy_gradient_loss | -0.00289     |\n",
            "|    value_loss           | 9.96e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6701858.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.7e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1824         |\n",
            "|    time_elapsed         | 701          |\n",
            "|    total_timesteps      | 182400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034037614 |\n",
            "|    clip_fraction        | 0.00373      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.96        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.77e+05     |\n",
            "|    n_updates            | 18230        |\n",
            "|    policy_gradient_loss | -0.011       |\n",
            "|    value_loss           | 3.7e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6704207.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.7e+06     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1825        |\n",
            "|    time_elapsed         | 701         |\n",
            "|    total_timesteps      | 182500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010220194 |\n",
            "|    clip_fraction        | 0.0464      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.41e+04    |\n",
            "|    n_updates            | 18240       |\n",
            "|    policy_gradient_loss | -0.014      |\n",
            "|    value_loss           | 4.35e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6708250.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.71e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1826         |\n",
            "|    time_elapsed         | 701          |\n",
            "|    total_timesteps      | 182600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022422003 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.974       |\n",
            "|    explained_variance   | 4.71e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.38e+04     |\n",
            "|    n_updates            | 18250        |\n",
            "|    policy_gradient_loss | -0.00694     |\n",
            "|    value_loss           | 7.03e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6711465.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.71e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1827         |\n",
            "|    time_elapsed         | 702          |\n",
            "|    total_timesteps      | 182700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018330154 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.63e+05     |\n",
            "|    n_updates            | 18260        |\n",
            "|    policy_gradient_loss | -0.00443     |\n",
            "|    value_loss           | 3.4e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6713985.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.71e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1828          |\n",
            "|    time_elapsed         | 702           |\n",
            "|    total_timesteps      | 182800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00088908366 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.978        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.22e+05      |\n",
            "|    n_updates            | 18270         |\n",
            "|    policy_gradient_loss | -0.00302      |\n",
            "|    value_loss           | 1.87e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6716859.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.72e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1829        |\n",
            "|    time_elapsed         | 702         |\n",
            "|    total_timesteps      | 182900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010353854 |\n",
            "|    clip_fraction        | 0.0321      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.05e+04    |\n",
            "|    n_updates            | 18280       |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    value_loss           | 9.12e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6720141.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.72e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1830         |\n",
            "|    time_elapsed         | 703          |\n",
            "|    total_timesteps      | 183000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016085166 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.37e+04     |\n",
            "|    n_updates            | 18290        |\n",
            "|    policy_gradient_loss | -0.000717    |\n",
            "|    value_loss           | 1.17e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6723558.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.72e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1831          |\n",
            "|    time_elapsed         | 703           |\n",
            "|    total_timesteps      | 183100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018680608 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.942        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.27e+04      |\n",
            "|    n_updates            | 18300         |\n",
            "|    policy_gradient_loss | -0.00102      |\n",
            "|    value_loss           | 1.2e+05       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6728513.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.73e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1832         |\n",
            "|    time_elapsed         | 703          |\n",
            "|    total_timesteps      | 183200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011387009 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.951       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16e+05     |\n",
            "|    n_updates            | 18310        |\n",
            "|    policy_gradient_loss | -0.00593     |\n",
            "|    value_loss           | 2.32e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6731892.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.73e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1833          |\n",
            "|    time_elapsed         | 704           |\n",
            "|    total_timesteps      | 183300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00054117537 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.951        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.24e+05      |\n",
            "|    n_updates            | 18320         |\n",
            "|    policy_gradient_loss | -0.00186      |\n",
            "|    value_loss           | 4.87e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6734787.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.73e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1834         |\n",
            "|    time_elapsed         | 704          |\n",
            "|    total_timesteps      | 183400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011768683 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 1.25e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.81e+04     |\n",
            "|    n_updates            | 18330        |\n",
            "|    policy_gradient_loss | -0.00568     |\n",
            "|    value_loss           | 1.81e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6737361.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.74e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1835          |\n",
            "|    time_elapsed         | 705           |\n",
            "|    total_timesteps      | 183500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00080196373 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.89         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.41e+04      |\n",
            "|    n_updates            | 18340         |\n",
            "|    policy_gradient_loss | -0.00266      |\n",
            "|    value_loss           | 1.21e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6740878.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.74e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1836         |\n",
            "|    time_elapsed         | 705          |\n",
            "|    total_timesteps      | 183600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008844245 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.953       |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.03e+04     |\n",
            "|    n_updates            | 18350        |\n",
            "|    policy_gradient_loss | -0.00277     |\n",
            "|    value_loss           | 8.66e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6744078.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.74e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1837         |\n",
            "|    time_elapsed         | 705          |\n",
            "|    total_timesteps      | 183700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014683228 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.989       |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.12e+05     |\n",
            "|    n_updates            | 18360        |\n",
            "|    policy_gradient_loss | -0.00605     |\n",
            "|    value_loss           | 2.14e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6748241.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.75e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1838         |\n",
            "|    time_elapsed         | 706          |\n",
            "|    total_timesteps      | 183800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010285514 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.928       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.29e+05     |\n",
            "|    n_updates            | 18370        |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 1.97e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6750360.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 6.75e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1839       |\n",
            "|    time_elapsed         | 706        |\n",
            "|    total_timesteps      | 183900     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00575796 |\n",
            "|    clip_fraction        | 0.0286     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.906     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.92e+05   |\n",
            "|    n_updates            | 18380      |\n",
            "|    policy_gradient_loss | -0.0074    |\n",
            "|    value_loss           | 3.17e+05   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 6753765.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.75e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1840        |\n",
            "|    time_elapsed         | 706         |\n",
            "|    total_timesteps      | 184000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020013276 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.02e+04    |\n",
            "|    n_updates            | 18390       |\n",
            "|    policy_gradient_loss | -0.0253     |\n",
            "|    value_loss           | 6.73e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6756393.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.76e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1841         |\n",
            "|    time_elapsed         | 707          |\n",
            "|    total_timesteps      | 184100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036125453 |\n",
            "|    clip_fraction        | 0.014        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.973       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.14e+05     |\n",
            "|    n_updates            | 18400        |\n",
            "|    policy_gradient_loss | -0.00988     |\n",
            "|    value_loss           | 2.31e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6760355.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 6.76e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1842       |\n",
            "|    time_elapsed         | 707        |\n",
            "|    total_timesteps      | 184200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00569165 |\n",
            "|    clip_fraction        | 0.015      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.949     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.76e+04   |\n",
            "|    n_updates            | 18410      |\n",
            "|    policy_gradient_loss | -0.0117    |\n",
            "|    value_loss           | 1.15e+05   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 6762263.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.76e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1843        |\n",
            "|    time_elapsed         | 708         |\n",
            "|    total_timesteps      | 184300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002244802 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 4.77e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.39e+05    |\n",
            "|    n_updates            | 18420       |\n",
            "|    policy_gradient_loss | -0.00647    |\n",
            "|    value_loss           | 2.58e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6764836.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.76e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1844        |\n",
            "|    time_elapsed         | 708         |\n",
            "|    total_timesteps      | 184400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003094609 |\n",
            "|    clip_fraction        | 0.0053      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.988      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.51e+04    |\n",
            "|    n_updates            | 18430       |\n",
            "|    policy_gradient_loss | -0.00845    |\n",
            "|    value_loss           | 4.11e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6767812.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.77e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1845         |\n",
            "|    time_elapsed         | 708          |\n",
            "|    total_timesteps      | 184500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047784243 |\n",
            "|    clip_fraction        | 0.024        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.6e+04      |\n",
            "|    n_updates            | 18440        |\n",
            "|    policy_gradient_loss | -0.014       |\n",
            "|    value_loss           | 9.48e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6771897.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.77e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1846         |\n",
            "|    time_elapsed         | 709          |\n",
            "|    total_timesteps      | 184600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051731435 |\n",
            "|    clip_fraction        | 0.00668      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.953       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.2e+04      |\n",
            "|    n_updates            | 18450        |\n",
            "|    policy_gradient_loss | -0.0105      |\n",
            "|    value_loss           | 1.52e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6775480.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.78e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1847         |\n",
            "|    time_elapsed         | 709          |\n",
            "|    total_timesteps      | 184700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033740234 |\n",
            "|    clip_fraction        | 0.00451      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.879       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.54e+05     |\n",
            "|    n_updates            | 18460        |\n",
            "|    policy_gradient_loss | -0.0124      |\n",
            "|    value_loss           | 2.77e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6779327.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.78e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1848         |\n",
            "|    time_elapsed         | 709          |\n",
            "|    total_timesteps      | 184800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018504567 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1           |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.93e+04     |\n",
            "|    n_updates            | 18470        |\n",
            "|    policy_gradient_loss | -0.00539     |\n",
            "|    value_loss           | 2.21e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6781265.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.78e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1849         |\n",
            "|    time_elapsed         | 710          |\n",
            "|    total_timesteps      | 184900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010578468 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.93        |\n",
            "|    explained_variance   | 5.96e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.41e+05     |\n",
            "|    n_updates            | 18480        |\n",
            "|    policy_gradient_loss | -0.00518     |\n",
            "|    value_loss           | 2.7e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6785728.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.79e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1850         |\n",
            "|    time_elapsed         | 710          |\n",
            "|    total_timesteps      | 185000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030701505 |\n",
            "|    clip_fraction        | 0.00356      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.957       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.29e+04     |\n",
            "|    n_updates            | 18490        |\n",
            "|    policy_gradient_loss | -0.00637     |\n",
            "|    value_loss           | 4.92e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6788477.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.79e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1851         |\n",
            "|    time_elapsed         | 711          |\n",
            "|    total_timesteps      | 185100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024866636 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.03e+05     |\n",
            "|    n_updates            | 18500        |\n",
            "|    policy_gradient_loss | -0.00984     |\n",
            "|    value_loss           | 4.78e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6791522.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.79e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1852        |\n",
            "|    time_elapsed         | 711         |\n",
            "|    total_timesteps      | 185200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007498671 |\n",
            "|    clip_fraction        | 0.0168      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.931      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.36e+04    |\n",
            "|    n_updates            | 18510       |\n",
            "|    policy_gradient_loss | -0.0128     |\n",
            "|    value_loss           | 8.77e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6795379.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.8e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1853         |\n",
            "|    time_elapsed         | 711          |\n",
            "|    total_timesteps      | 185300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008949911 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.934       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.39e+04     |\n",
            "|    n_updates            | 18520        |\n",
            "|    policy_gradient_loss | -0.0029      |\n",
            "|    value_loss           | 1.88e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6800664.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.8e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1854          |\n",
            "|    time_elapsed         | 712           |\n",
            "|    total_timesteps      | 185400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00045216622 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.99         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.69e+05      |\n",
            "|    n_updates            | 18530         |\n",
            "|    policy_gradient_loss | -0.00241      |\n",
            "|    value_loss           | 3.09e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6804251.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.8e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1855          |\n",
            "|    time_elapsed         | 712           |\n",
            "|    total_timesteps      | 185500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00085146265 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.95         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.12e+05      |\n",
            "|    n_updates            | 18540         |\n",
            "|    policy_gradient_loss | -0.006        |\n",
            "|    value_loss           | 5.78e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6807995.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.81e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1856        |\n",
            "|    time_elapsed         | 712         |\n",
            "|    total_timesteps      | 185600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003068868 |\n",
            "|    clip_fraction        | 0.00807     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.945      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.04e+05    |\n",
            "|    n_updates            | 18550       |\n",
            "|    policy_gradient_loss | -0.00656    |\n",
            "|    value_loss           | 2.12e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6810578.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.81e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1857          |\n",
            "|    time_elapsed         | 713           |\n",
            "|    total_timesteps      | 185700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00088813907 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.928        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.14e+04      |\n",
            "|    n_updates            | 18560         |\n",
            "|    policy_gradient_loss | -0.00332      |\n",
            "|    value_loss           | 1.77e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6814038.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 6.81e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1858       |\n",
            "|    time_elapsed         | 713        |\n",
            "|    total_timesteps      | 185800     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00803926 |\n",
            "|    clip_fraction        | 0.0358     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.92      |\n",
            "|    explained_variance   | 2.38e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.79e+04   |\n",
            "|    n_updates            | 18570      |\n",
            "|    policy_gradient_loss | -0.0156    |\n",
            "|    value_loss           | 9.82e+04   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 6815957.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.82e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1859         |\n",
            "|    time_elapsed         | 714          |\n",
            "|    total_timesteps      | 185900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022167827 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.03e+05     |\n",
            "|    n_updates            | 18580        |\n",
            "|    policy_gradient_loss | -0.00537     |\n",
            "|    value_loss           | 1.99e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6818356.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.82e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1860         |\n",
            "|    time_elapsed         | 714          |\n",
            "|    total_timesteps      | 186000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024255184 |\n",
            "|    clip_fraction        | 0.00868      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.932       |\n",
            "|    explained_variance   | 2.32e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.35e+04     |\n",
            "|    n_updates            | 18590        |\n",
            "|    policy_gradient_loss | -0.00838     |\n",
            "|    value_loss           | 5.42e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6822090.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 6.82e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1861       |\n",
            "|    time_elapsed         | 714        |\n",
            "|    total_timesteps      | 186100     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01825571 |\n",
            "|    clip_fraction        | 0.108      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.988     |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.7e+04    |\n",
            "|    n_updates            | 18600      |\n",
            "|    policy_gradient_loss | -0.0201    |\n",
            "|    value_loss           | 8.66e+04   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 6824981.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.82e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1862        |\n",
            "|    time_elapsed         | 715         |\n",
            "|    total_timesteps      | 186200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002892246 |\n",
            "|    clip_fraction        | 0.0079      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.953      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.12e+05    |\n",
            "|    n_updates            | 18610       |\n",
            "|    policy_gradient_loss | -0.00672    |\n",
            "|    value_loss           | 2.37e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6830333.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.83e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1863         |\n",
            "|    time_elapsed         | 715          |\n",
            "|    total_timesteps      | 186300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020134842 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.952       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.5e+04      |\n",
            "|    n_updates            | 18620        |\n",
            "|    policy_gradient_loss | -0.00546     |\n",
            "|    value_loss           | 1.56e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6833049.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.83e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1864          |\n",
            "|    time_elapsed         | 715           |\n",
            "|    total_timesteps      | 186400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00056000566 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.975        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.07e+05      |\n",
            "|    n_updates            | 18630         |\n",
            "|    policy_gradient_loss | -0.00291      |\n",
            "|    value_loss           | 4.54e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6837299.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.84e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1865         |\n",
            "|    time_elapsed         | 716          |\n",
            "|    total_timesteps      | 186500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013548674 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.7e+04      |\n",
            "|    n_updates            | 18640        |\n",
            "|    policy_gradient_loss | -0.00693     |\n",
            "|    value_loss           | 1.33e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6841320.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.84e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1866         |\n",
            "|    time_elapsed         | 716          |\n",
            "|    total_timesteps      | 186600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027534685 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.943       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.84e+05     |\n",
            "|    n_updates            | 18650        |\n",
            "|    policy_gradient_loss | -0.00652     |\n",
            "|    value_loss           | 3.68e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6844342.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.84e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1867         |\n",
            "|    time_elapsed         | 717          |\n",
            "|    total_timesteps      | 186700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015049797 |\n",
            "|    clip_fraction        | 0.0053       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.884       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.83e+05     |\n",
            "|    n_updates            | 18660        |\n",
            "|    policy_gradient_loss | -0.00536     |\n",
            "|    value_loss           | 3.47e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6848520.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.85e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1868         |\n",
            "|    time_elapsed         | 717          |\n",
            "|    total_timesteps      | 186800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011214537 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.958       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.35e+04     |\n",
            "|    n_updates            | 18670        |\n",
            "|    policy_gradient_loss | -0.00415     |\n",
            "|    value_loss           | 1.44e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6850841.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.85e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1869         |\n",
            "|    time_elapsed         | 717          |\n",
            "|    total_timesteps      | 186900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023094583 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.941       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.5e+05      |\n",
            "|    n_updates            | 18680        |\n",
            "|    policy_gradient_loss | -0.00691     |\n",
            "|    value_loss           | 3.04e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6854589.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.85e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1870         |\n",
            "|    time_elapsed         | 718          |\n",
            "|    total_timesteps      | 187000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018188248 |\n",
            "|    clip_fraction        | 0.00295      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.972       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.07e+04     |\n",
            "|    n_updates            | 18690        |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    value_loss           | 8.68e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6858570.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.86e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1871         |\n",
            "|    time_elapsed         | 718          |\n",
            "|    total_timesteps      | 187100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013271413 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.999       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.4e+05      |\n",
            "|    n_updates            | 18700        |\n",
            "|    policy_gradient_loss | -0.00354     |\n",
            "|    value_loss           | 2.41e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6862035.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.86e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1872         |\n",
            "|    time_elapsed         | 718          |\n",
            "|    total_timesteps      | 187200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012456824 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.925       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.11e+05     |\n",
            "|    n_updates            | 18710        |\n",
            "|    policy_gradient_loss | -0.00504     |\n",
            "|    value_loss           | 2.07e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6864429.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.86e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1873         |\n",
            "|    time_elapsed         | 719          |\n",
            "|    total_timesteps      | 187300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033800332 |\n",
            "|    clip_fraction        | 0.0143       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.981       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.11e+05     |\n",
            "|    n_updates            | 18720        |\n",
            "|    policy_gradient_loss | -0.00862     |\n",
            "|    value_loss           | 2.23e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6867588.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.87e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1874         |\n",
            "|    time_elapsed         | 719          |\n",
            "|    total_timesteps      | 187400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030667712 |\n",
            "|    clip_fraction        | 0.0146       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.859       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.12e+04     |\n",
            "|    n_updates            | 18730        |\n",
            "|    policy_gradient_loss | -0.0077      |\n",
            "|    value_loss           | 7.91e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6869620.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.87e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1875         |\n",
            "|    time_elapsed         | 719          |\n",
            "|    total_timesteps      | 187500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022105458 |\n",
            "|    clip_fraction        | 0.00373      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.939       |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.34e+04     |\n",
            "|    n_updates            | 18740        |\n",
            "|    policy_gradient_loss | -0.00712     |\n",
            "|    value_loss           | 1.27e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6873543.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.87e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1876        |\n",
            "|    time_elapsed         | 720         |\n",
            "|    total_timesteps      | 187600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004928343 |\n",
            "|    clip_fraction        | 0.0209      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.987      |\n",
            "|    explained_variance   | 3.58e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.69e+04    |\n",
            "|    n_updates            | 18750       |\n",
            "|    policy_gradient_loss | -0.00933    |\n",
            "|    value_loss           | 5.18e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6876961.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.88e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1877        |\n",
            "|    time_elapsed         | 720         |\n",
            "|    total_timesteps      | 187700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004122966 |\n",
            "|    clip_fraction        | 0.0142      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.923      |\n",
            "|    explained_variance   | -3.58e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.73e+05    |\n",
            "|    n_updates            | 18760       |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    value_loss           | 3.12e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6879991.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.88e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1878         |\n",
            "|    time_elapsed         | 721          |\n",
            "|    total_timesteps      | 187800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011811693 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.969       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.26e+05     |\n",
            "|    n_updates            | 18770        |\n",
            "|    policy_gradient_loss | -0.00402     |\n",
            "|    value_loss           | 2.19e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6882794.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.88e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1879         |\n",
            "|    time_elapsed         | 721          |\n",
            "|    total_timesteps      | 187900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016692586 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.23e+04     |\n",
            "|    n_updates            | 18780        |\n",
            "|    policy_gradient_loss | -0.00684     |\n",
            "|    value_loss           | 1.76e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6886834.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.89e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1880         |\n",
            "|    time_elapsed         | 721          |\n",
            "|    total_timesteps      | 188000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039803144 |\n",
            "|    clip_fraction        | 0.00651      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.968       |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.99e+04     |\n",
            "|    n_updates            | 18790        |\n",
            "|    policy_gradient_loss | -0.00978     |\n",
            "|    value_loss           | 8.34e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6890611.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.89e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1881         |\n",
            "|    time_elapsed         | 722          |\n",
            "|    total_timesteps      | 188100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013784971 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.962       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2e+05      |\n",
            "|    n_updates            | 18800        |\n",
            "|    policy_gradient_loss | -0.00529     |\n",
            "|    value_loss           | 2.64e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6893800.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.89e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1882         |\n",
            "|    time_elapsed         | 722          |\n",
            "|    total_timesteps      | 188200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020888727 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.75e+05     |\n",
            "|    n_updates            | 18810        |\n",
            "|    policy_gradient_loss | -0.00606     |\n",
            "|    value_loss           | 2.82e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6897556.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.9e+06     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1883        |\n",
            "|    time_elapsed         | 722         |\n",
            "|    total_timesteps      | 188300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003571619 |\n",
            "|    clip_fraction        | 0.00313     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.09e+04    |\n",
            "|    n_updates            | 18820       |\n",
            "|    policy_gradient_loss | -0.00678    |\n",
            "|    value_loss           | 1.74e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6900565.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.9e+06     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1884        |\n",
            "|    time_elapsed         | 723         |\n",
            "|    total_timesteps      | 188400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002315368 |\n",
            "|    clip_fraction        | 0.000781    |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.94e+04    |\n",
            "|    n_updates            | 18830       |\n",
            "|    policy_gradient_loss | -0.00689    |\n",
            "|    value_loss           | 1.74e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6905199.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.91e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1885         |\n",
            "|    time_elapsed         | 723          |\n",
            "|    total_timesteps      | 188500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041890275 |\n",
            "|    clip_fraction        | 0.0172       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.989       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.59e+04     |\n",
            "|    n_updates            | 18840        |\n",
            "|    policy_gradient_loss | -0.01        |\n",
            "|    value_loss           | 1.02e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6907849.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.91e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1886         |\n",
            "|    time_elapsed         | 724          |\n",
            "|    total_timesteps      | 188600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006083811 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.994       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.45e+05     |\n",
            "|    n_updates            | 18850        |\n",
            "|    policy_gradient_loss | -0.00282     |\n",
            "|    value_loss           | 6.49e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6910723.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.91e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1887         |\n",
            "|    time_elapsed         | 724          |\n",
            "|    total_timesteps      | 188700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014915803 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.977       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.34e+04     |\n",
            "|    n_updates            | 18860        |\n",
            "|    policy_gradient_loss | -0.0062      |\n",
            "|    value_loss           | 1.21e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6915597.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.92e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1888        |\n",
            "|    time_elapsed         | 724         |\n",
            "|    total_timesteps      | 188800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005175091 |\n",
            "|    clip_fraction        | 0.00608     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.953      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.09e+04    |\n",
            "|    n_updates            | 18870       |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 9.32e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6918739.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.92e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1889         |\n",
            "|    time_elapsed         | 725          |\n",
            "|    total_timesteps      | 188900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017905061 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.988       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.62e+05     |\n",
            "|    n_updates            | 18880        |\n",
            "|    policy_gradient_loss | -0.000993    |\n",
            "|    value_loss           | 4.79e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6922346.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.92e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1890        |\n",
            "|    time_elapsed         | 725         |\n",
            "|    total_timesteps      | 189000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001122547 |\n",
            "|    clip_fraction        | 0.000781    |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.907      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.21e+04    |\n",
            "|    n_updates            | 18890       |\n",
            "|    policy_gradient_loss | -0.00562    |\n",
            "|    value_loss           | 1.59e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6924596.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.92e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1891         |\n",
            "|    time_elapsed         | 725          |\n",
            "|    total_timesteps      | 189100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024628863 |\n",
            "|    clip_fraction        | 0.0171       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.09e+04     |\n",
            "|    n_updates            | 18900        |\n",
            "|    policy_gradient_loss | -0.0102      |\n",
            "|    value_loss           | 2.04e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6928063.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.93e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1892        |\n",
            "|    time_elapsed         | 726         |\n",
            "|    total_timesteps      | 189200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002400497 |\n",
            "|    clip_fraction        | 0.000781    |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.49e+04    |\n",
            "|    n_updates            | 18910       |\n",
            "|    policy_gradient_loss | -0.00643    |\n",
            "|    value_loss           | 6.96e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6930070.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.93e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1893        |\n",
            "|    time_elapsed         | 726         |\n",
            "|    total_timesteps      | 189300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006036369 |\n",
            "|    clip_fraction        | 0.0223      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.64e+04    |\n",
            "|    n_updates            | 18920       |\n",
            "|    policy_gradient_loss | -0.00843    |\n",
            "|    value_loss           | 1.72e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6932808.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.93e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1894         |\n",
            "|    time_elapsed         | 727          |\n",
            "|    total_timesteps      | 189400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077406457 |\n",
            "|    clip_fraction        | 0.0319       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.96        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.77e+04     |\n",
            "|    n_updates            | 18930        |\n",
            "|    policy_gradient_loss | -0.0135      |\n",
            "|    value_loss           | 4.09e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6935519.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.94e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1895         |\n",
            "|    time_elapsed         | 727          |\n",
            "|    total_timesteps      | 189500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0083271675 |\n",
            "|    clip_fraction        | 0.0434       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.23e+04     |\n",
            "|    n_updates            | 18940        |\n",
            "|    policy_gradient_loss | -0.0111      |\n",
            "|    value_loss           | 1.03e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6939102.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.94e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1896         |\n",
            "|    time_elapsed         | 727          |\n",
            "|    total_timesteps      | 189600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012200184 |\n",
            "|    clip_fraction        | 0.00217      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.74e+04     |\n",
            "|    n_updates            | 18950        |\n",
            "|    policy_gradient_loss | -0.00383     |\n",
            "|    value_loss           | 9.97e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6942122.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.94e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1897          |\n",
            "|    time_elapsed         | 728           |\n",
            "|    total_timesteps      | 189700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00037198307 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.906        |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.12e+05      |\n",
            "|    n_updates            | 18960         |\n",
            "|    policy_gradient_loss | -0.00281      |\n",
            "|    value_loss           | 2.17e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6945785.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.95e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1898          |\n",
            "|    time_elapsed         | 728           |\n",
            "|    total_timesteps      | 189800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00067103875 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.977        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.72e+04      |\n",
            "|    n_updates            | 18970         |\n",
            "|    policy_gradient_loss | -0.00431      |\n",
            "|    value_loss           | 1.4e+05       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6948427.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.95e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1899         |\n",
            "|    time_elapsed         | 728          |\n",
            "|    total_timesteps      | 189900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026299949 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01e+05     |\n",
            "|    n_updates            | 18980        |\n",
            "|    policy_gradient_loss | -0.00518     |\n",
            "|    value_loss           | 2.07e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6951701.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.95e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1900         |\n",
            "|    time_elapsed         | 729          |\n",
            "|    total_timesteps      | 190000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038702283 |\n",
            "|    clip_fraction        | 0.00668      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.99e+04     |\n",
            "|    n_updates            | 18990        |\n",
            "|    policy_gradient_loss | -0.00782     |\n",
            "|    value_loss           | 1e+05        |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6954856.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.95e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1901        |\n",
            "|    time_elapsed         | 729         |\n",
            "|    total_timesteps      | 190100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003013433 |\n",
            "|    clip_fraction        | 0.0059      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 3.58e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.17e+05    |\n",
            "|    n_updates            | 19000       |\n",
            "|    policy_gradient_loss | -0.0059     |\n",
            "|    value_loss           | 2.15e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6957635.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.96e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1902         |\n",
            "|    time_elapsed         | 729          |\n",
            "|    total_timesteps      | 190200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005819171 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.42e+04     |\n",
            "|    n_updates            | 19010        |\n",
            "|    policy_gradient_loss | -0.00279     |\n",
            "|    value_loss           | 1.43e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6961270.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.96e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1903         |\n",
            "|    time_elapsed         | 730          |\n",
            "|    total_timesteps      | 190300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014878493 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.92e+04     |\n",
            "|    n_updates            | 19020        |\n",
            "|    policy_gradient_loss | -0.00438     |\n",
            "|    value_loss           | 9.48e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6964238.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.96e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1904        |\n",
            "|    time_elapsed         | 730         |\n",
            "|    total_timesteps      | 190400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003053221 |\n",
            "|    clip_fraction        | 0.00556     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.974      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.33e+05    |\n",
            "|    n_updates            | 19030       |\n",
            "|    policy_gradient_loss | -0.00665    |\n",
            "|    value_loss           | 2.29e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6966116.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.97e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1905         |\n",
            "|    time_elapsed         | 731          |\n",
            "|    total_timesteps      | 190500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013221023 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.991       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.42e+04     |\n",
            "|    n_updates            | 19040        |\n",
            "|    policy_gradient_loss | -0.00514     |\n",
            "|    value_loss           | 1.39e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6970431.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.97e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1906        |\n",
            "|    time_elapsed         | 731         |\n",
            "|    total_timesteps      | 190600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007169043 |\n",
            "|    clip_fraction        | 0.0168      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.97       |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.74e+04    |\n",
            "|    n_updates            | 19050       |\n",
            "|    policy_gradient_loss | -0.00914    |\n",
            "|    value_loss           | 3.21e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6973880.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 6.97e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1907          |\n",
            "|    time_elapsed         | 731           |\n",
            "|    total_timesteps      | 190700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00082875224 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.972        |\n",
            "|    explained_variance   | 9.54e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.1e+05       |\n",
            "|    n_updates            | 19060         |\n",
            "|    policy_gradient_loss | -0.000674     |\n",
            "|    value_loss           | 4.1e+05       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 6977194.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.98e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1908         |\n",
            "|    time_elapsed         | 732          |\n",
            "|    total_timesteps      | 190800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011108089 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.877       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.51e+05     |\n",
            "|    n_updates            | 19070        |\n",
            "|    policy_gradient_loss | -0.00612     |\n",
            "|    value_loss           | 2.34e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6979622.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.98e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1909         |\n",
            "|    time_elapsed         | 732          |\n",
            "|    total_timesteps      | 190900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044721058 |\n",
            "|    clip_fraction        | 0.00547      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.989       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.26e+05     |\n",
            "|    n_updates            | 19080        |\n",
            "|    policy_gradient_loss | -0.0126      |\n",
            "|    value_loss           | 2.32e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6982929.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.98e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1910         |\n",
            "|    time_elapsed         | 732          |\n",
            "|    total_timesteps      | 191000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028482596 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.35e+04     |\n",
            "|    n_updates            | 19090        |\n",
            "|    policy_gradient_loss | -0.00556     |\n",
            "|    value_loss           | 8.95e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6986136.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.99e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1911        |\n",
            "|    time_elapsed         | 733         |\n",
            "|    total_timesteps      | 191100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002047827 |\n",
            "|    clip_fraction        | 0.00373     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.77e+04    |\n",
            "|    n_updates            | 19100       |\n",
            "|    policy_gradient_loss | -0.00663    |\n",
            "|    value_loss           | 1.42e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6988874.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.99e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1912        |\n",
            "|    time_elapsed         | 733         |\n",
            "|    total_timesteps      | 191200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000591846 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.976      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.35e+04    |\n",
            "|    n_updates            | 19110       |\n",
            "|    policy_gradient_loss | -0.0022     |\n",
            "|    value_loss           | 1.52e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6991890.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 6.99e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1913         |\n",
            "|    time_elapsed         | 733          |\n",
            "|    total_timesteps      | 191300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068316213 |\n",
            "|    clip_fraction        | 0.0226       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.968       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.61e+04     |\n",
            "|    n_updates            | 19120        |\n",
            "|    policy_gradient_loss | -0.0147      |\n",
            "|    value_loss           | 8.79e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 6994363.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 6.99e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1914        |\n",
            "|    time_elapsed         | 734         |\n",
            "|    total_timesteps      | 191400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003167293 |\n",
            "|    clip_fraction        | 0.00313     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.955      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.77e+04    |\n",
            "|    n_updates            | 19130       |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    value_loss           | 1.56e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 6997507.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7e+06        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1915         |\n",
            "|    time_elapsed         | 734          |\n",
            "|    total_timesteps      | 191500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040385462 |\n",
            "|    clip_fraction        | 0.0116       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.926       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.74e+04     |\n",
            "|    n_updates            | 19140        |\n",
            "|    policy_gradient_loss | -0.0104      |\n",
            "|    value_loss           | 1.21e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7001327.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 7e+06      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1916       |\n",
            "|    time_elapsed         | 735        |\n",
            "|    total_timesteps      | 191600     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00585683 |\n",
            "|    clip_fraction        | 0.0272     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.99      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.42e+04   |\n",
            "|    n_updates            | 19150      |\n",
            "|    policy_gradient_loss | -0.00888   |\n",
            "|    value_loss           | 1.51e+05   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 7005446.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.01e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1917         |\n",
            "|    time_elapsed         | 735          |\n",
            "|    total_timesteps      | 191700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014372449 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.965       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.08e+05     |\n",
            "|    n_updates            | 19160        |\n",
            "|    policy_gradient_loss | -0.00383     |\n",
            "|    value_loss           | 2.18e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7009672.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.01e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1918          |\n",
            "|    time_elapsed         | 735           |\n",
            "|    total_timesteps      | 191800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038964843 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.915        |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.43e+05      |\n",
            "|    n_updates            | 19170         |\n",
            "|    policy_gradient_loss | -0.0021       |\n",
            "|    value_loss           | 4.05e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7013864.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.01e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1919         |\n",
            "|    time_elapsed         | 736          |\n",
            "|    total_timesteps      | 191900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012458249 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.945       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.89e+05     |\n",
            "|    n_updates            | 19180        |\n",
            "|    policy_gradient_loss | -0.00547     |\n",
            "|    value_loss           | 3.28e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7018122.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.02e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1920          |\n",
            "|    time_elapsed         | 736           |\n",
            "|    total_timesteps      | 192000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00062097306 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.936        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.46e+05      |\n",
            "|    n_updates            | 19190         |\n",
            "|    policy_gradient_loss | -0.00285      |\n",
            "|    value_loss           | 2.78e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7021500.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.02e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1921          |\n",
            "|    time_elapsed         | 736           |\n",
            "|    total_timesteps      | 192100        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00096122734 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.951        |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.41e+05      |\n",
            "|    n_updates            | 19200         |\n",
            "|    policy_gradient_loss | -0.0033       |\n",
            "|    value_loss           | 3.5e+05       |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7024598.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.02e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1922         |\n",
            "|    time_elapsed         | 737          |\n",
            "|    total_timesteps      | 192200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025835894 |\n",
            "|    clip_fraction        | 0.00556      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.28e+05     |\n",
            "|    n_updates            | 19210        |\n",
            "|    policy_gradient_loss | -0.00943     |\n",
            "|    value_loss           | 2.37e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7027736.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.03e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1923          |\n",
            "|    time_elapsed         | 737           |\n",
            "|    total_timesteps      | 192300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00090889423 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.07         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.53e+04      |\n",
            "|    n_updates            | 19220         |\n",
            "|    policy_gradient_loss | -0.0025       |\n",
            "|    value_loss           | 1.42e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7030246.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.03e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1924         |\n",
            "|    time_elapsed         | 738          |\n",
            "|    total_timesteps      | 192400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021226078 |\n",
            "|    clip_fraction        | 0.00278      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.978       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.14e+04     |\n",
            "|    n_updates            | 19230        |\n",
            "|    policy_gradient_loss | -0.00474     |\n",
            "|    value_loss           | 1.53e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7032768.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.03e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1925         |\n",
            "|    time_elapsed         | 738          |\n",
            "|    total_timesteps      | 192500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017517229 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.21e+04     |\n",
            "|    n_updates            | 19240        |\n",
            "|    policy_gradient_loss | -0.0059      |\n",
            "|    value_loss           | 5.47e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7036690.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.04e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1926         |\n",
            "|    time_elapsed         | 738          |\n",
            "|    total_timesteps      | 192600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037713402 |\n",
            "|    clip_fraction        | 0.00885      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.76e+04     |\n",
            "|    n_updates            | 19250        |\n",
            "|    policy_gradient_loss | -0.0095      |\n",
            "|    value_loss           | 9.35e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7039989.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.04e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1927         |\n",
            "|    time_elapsed         | 739          |\n",
            "|    total_timesteps      | 192700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011712273 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.42e+05     |\n",
            "|    n_updates            | 19260        |\n",
            "|    policy_gradient_loss | -0.00378     |\n",
            "|    value_loss           | 2.96e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7042566.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.04e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1928         |\n",
            "|    time_elapsed         | 739          |\n",
            "|    total_timesteps      | 192800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033073723 |\n",
            "|    clip_fraction        | 0.00295      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.981       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.86e+04     |\n",
            "|    n_updates            | 19270        |\n",
            "|    policy_gradient_loss | -0.0108      |\n",
            "|    value_loss           | 1.61e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7045060.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.05e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1929        |\n",
            "|    time_elapsed         | 739         |\n",
            "|    total_timesteps      | 192900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004152146 |\n",
            "|    clip_fraction        | 0.00434     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.962      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.89e+04    |\n",
            "|    n_updates            | 19280       |\n",
            "|    policy_gradient_loss | -0.00784    |\n",
            "|    value_loss           | 8.61e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7047999.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.05e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1930         |\n",
            "|    time_elapsed         | 740          |\n",
            "|    total_timesteps      | 193000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065287147 |\n",
            "|    clip_fraction        | 0.026        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.15e+04     |\n",
            "|    n_updates            | 19290        |\n",
            "|    policy_gradient_loss | -0.0151      |\n",
            "|    value_loss           | 9.75e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7050658.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.05e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1931         |\n",
            "|    time_elapsed         | 740          |\n",
            "|    total_timesteps      | 193100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018405416 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.95        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.29e+04     |\n",
            "|    n_updates            | 19300        |\n",
            "|    policy_gradient_loss | -0.00356     |\n",
            "|    value_loss           | 1.28e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7053446.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.05e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1932         |\n",
            "|    time_elapsed         | 741          |\n",
            "|    total_timesteps      | 193200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014997636 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.67e+04     |\n",
            "|    n_updates            | 19310        |\n",
            "|    policy_gradient_loss | -0.00467     |\n",
            "|    value_loss           | 1.01e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7057971.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.06e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1933          |\n",
            "|    time_elapsed         | 741           |\n",
            "|    total_timesteps      | 193300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00043801058 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.934        |\n",
            "|    explained_variance   | 3.81e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.63e+04      |\n",
            "|    n_updates            | 19320         |\n",
            "|    policy_gradient_loss | -0.000938     |\n",
            "|    value_loss           | 1.84e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7061560.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.06e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1934         |\n",
            "|    time_elapsed         | 741          |\n",
            "|    total_timesteps      | 193400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014266955 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.43e+05     |\n",
            "|    n_updates            | 19330        |\n",
            "|    policy_gradient_loss | -0.0081      |\n",
            "|    value_loss           | 2.89e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7064315.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.06e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1935         |\n",
            "|    time_elapsed         | 742          |\n",
            "|    total_timesteps      | 193500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025809682 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.04e+05     |\n",
            "|    n_updates            | 19340        |\n",
            "|    policy_gradient_loss | -0.0076      |\n",
            "|    value_loss           | 2.22e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7066946.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.07e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1936         |\n",
            "|    time_elapsed         | 742          |\n",
            "|    total_timesteps      | 193600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018020377 |\n",
            "|    clip_fraction        | 0.00217      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.961       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.4e+04      |\n",
            "|    n_updates            | 19350        |\n",
            "|    policy_gradient_loss | -0.00608     |\n",
            "|    value_loss           | 1.01e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7070346.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.07e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1937         |\n",
            "|    time_elapsed         | 742          |\n",
            "|    total_timesteps      | 193700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009619821 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.922       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.98e+04     |\n",
            "|    n_updates            | 19360        |\n",
            "|    policy_gradient_loss | -0.00317     |\n",
            "|    value_loss           | 1.16e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7073263.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.07e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1938         |\n",
            "|    time_elapsed         | 743          |\n",
            "|    total_timesteps      | 193800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020305347 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.04e+04     |\n",
            "|    n_updates            | 19370        |\n",
            "|    policy_gradient_loss | -0.00642     |\n",
            "|    value_loss           | 2.01e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7077631.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.08e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1939         |\n",
            "|    time_elapsed         | 743          |\n",
            "|    total_timesteps      | 193900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022876244 |\n",
            "|    clip_fraction        | 0.00295      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.964       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.67e+04     |\n",
            "|    n_updates            | 19380        |\n",
            "|    policy_gradient_loss | -0.00644     |\n",
            "|    value_loss           | 1.19e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7081175.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.08e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1940         |\n",
            "|    time_elapsed         | 743          |\n",
            "|    total_timesteps      | 194000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010863126 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.96        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.05e+05     |\n",
            "|    n_updates            | 19390        |\n",
            "|    policy_gradient_loss | -0.00345     |\n",
            "|    value_loss           | 3.81e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7085583.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.09e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1941         |\n",
            "|    time_elapsed         | 744          |\n",
            "|    total_timesteps      | 194100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011455452 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.971       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16e+05     |\n",
            "|    n_updates            | 19400        |\n",
            "|    policy_gradient_loss | -0.00388     |\n",
            "|    value_loss           | 2.1e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7089062.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.09e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1942         |\n",
            "|    time_elapsed         | 744          |\n",
            "|    total_timesteps      | 194200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026351132 |\n",
            "|    clip_fraction        | 0.0079       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.35e+05     |\n",
            "|    n_updates            | 19410        |\n",
            "|    policy_gradient_loss | -0.00743     |\n",
            "|    value_loss           | 2.88e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7090507.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.09e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1943         |\n",
            "|    time_elapsed         | 745          |\n",
            "|    total_timesteps      | 194300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016646743 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 4.17e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.49e+04     |\n",
            "|    n_updates            | 19420        |\n",
            "|    policy_gradient_loss | -0.00544     |\n",
            "|    value_loss           | 1.84e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7093865.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 7.09e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1944       |\n",
            "|    time_elapsed         | 745        |\n",
            "|    total_timesteps      | 194400     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02060494 |\n",
            "|    clip_fraction        | 0.14       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.984     |\n",
            "|    explained_variance   | 7.15e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.41e+04   |\n",
            "|    n_updates            | 19430      |\n",
            "|    policy_gradient_loss | -0.0249    |\n",
            "|    value_loss           | 2e+04      |\n",
            "----------------------------------------\n",
            "Episode done — reward = 7097412.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.1e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1945         |\n",
            "|    time_elapsed         | 745          |\n",
            "|    total_timesteps      | 194500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032642349 |\n",
            "|    clip_fraction        | 0.00278      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.39e+04     |\n",
            "|    n_updates            | 19440        |\n",
            "|    policy_gradient_loss | -0.00407     |\n",
            "|    value_loss           | 1.68e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7099744.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.1e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1946         |\n",
            "|    time_elapsed         | 746          |\n",
            "|    total_timesteps      | 194600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012839746 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.83e+05     |\n",
            "|    n_updates            | 19450        |\n",
            "|    policy_gradient_loss | -0.0049      |\n",
            "|    value_loss           | 3.29e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7103122.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.1e+06     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1947        |\n",
            "|    time_elapsed         | 746         |\n",
            "|    total_timesteps      | 194700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004983354 |\n",
            "|    clip_fraction        | 0.0258      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.77e+04    |\n",
            "|    n_updates            | 19460       |\n",
            "|    policy_gradient_loss | -0.01       |\n",
            "|    value_loss           | 9.49e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7107097.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.11e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1948         |\n",
            "|    time_elapsed         | 746          |\n",
            "|    total_timesteps      | 194800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024429022 |\n",
            "|    clip_fraction        | 0.00278      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.954       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7e+04        |\n",
            "|    n_updates            | 19470        |\n",
            "|    policy_gradient_loss | -0.00874     |\n",
            "|    value_loss           | 1.57e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7109744.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.11e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1949         |\n",
            "|    time_elapsed         | 747          |\n",
            "|    total_timesteps      | 194900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023095915 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.42e+05     |\n",
            "|    n_updates            | 19480        |\n",
            "|    policy_gradient_loss | -0.00588     |\n",
            "|    value_loss           | 2.71e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7112467.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.11e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1950        |\n",
            "|    time_elapsed         | 747         |\n",
            "|    total_timesteps      | 195000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004523702 |\n",
            "|    clip_fraction        | 0.0053      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.09e+04    |\n",
            "|    n_updates            | 19490       |\n",
            "|    policy_gradient_loss | -0.00957    |\n",
            "|    value_loss           | 1.24e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7117261.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.12e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1951        |\n",
            "|    time_elapsed         | 748         |\n",
            "|    total_timesteps      | 195100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009995673 |\n",
            "|    clip_fraction        | 0.0342      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.02e+04    |\n",
            "|    n_updates            | 19500       |\n",
            "|    policy_gradient_loss | -0.0174     |\n",
            "|    value_loss           | 7.6e+04     |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7119681.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.12e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1952         |\n",
            "|    time_elapsed         | 748          |\n",
            "|    total_timesteps      | 195200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019045782 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.976       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.42e+05     |\n",
            "|    n_updates            | 19510        |\n",
            "|    policy_gradient_loss | -0.00179     |\n",
            "|    value_loss           | 6.33e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7122928.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.12e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1953        |\n",
            "|    time_elapsed         | 748         |\n",
            "|    total_timesteps      | 195300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013341928 |\n",
            "|    clip_fraction        | 0.0667      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.983      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.34e+04    |\n",
            "|    n_updates            | 19520       |\n",
            "|    policy_gradient_loss | -0.0225     |\n",
            "|    value_loss           | 7.27e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7125205.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.13e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1954         |\n",
            "|    time_elapsed         | 749          |\n",
            "|    total_timesteps      | 195400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032819984 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.945       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.87e+04     |\n",
            "|    n_updates            | 19530        |\n",
            "|    policy_gradient_loss | -0.00549     |\n",
            "|    value_loss           | 1.3e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7129347.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.13e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1955         |\n",
            "|    time_elapsed         | 749          |\n",
            "|    total_timesteps      | 195500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035532487 |\n",
            "|    clip_fraction        | 0.00512      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.997       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.25e+04     |\n",
            "|    n_updates            | 19540        |\n",
            "|    policy_gradient_loss | -0.0134      |\n",
            "|    value_loss           | 6.58e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7131323.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.13e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1956        |\n",
            "|    time_elapsed         | 749         |\n",
            "|    total_timesteps      | 195600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003349403 |\n",
            "|    clip_fraction        | 0.00868     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.969      |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.05e+05    |\n",
            "|    n_updates            | 19550       |\n",
            "|    policy_gradient_loss | -0.0114     |\n",
            "|    value_loss           | 2.03e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7134561.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.13e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1957        |\n",
            "|    time_elapsed         | 750         |\n",
            "|    total_timesteps      | 195700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013676723 |\n",
            "|    clip_fraction        | 0.0895      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.941      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.75e+04    |\n",
            "|    n_updates            | 19560       |\n",
            "|    policy_gradient_loss | -0.0178     |\n",
            "|    value_loss           | 3.87e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7137704.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.14e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1958         |\n",
            "|    time_elapsed         | 750          |\n",
            "|    total_timesteps      | 195800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023686474 |\n",
            "|    clip_fraction        | 0.00373      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.973       |\n",
            "|    explained_variance   | 1.43e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.48e+04     |\n",
            "|    n_updates            | 19570        |\n",
            "|    policy_gradient_loss | -0.00615     |\n",
            "|    value_loss           | 1.66e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7140310.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.14e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1959        |\n",
            "|    time_elapsed         | 751         |\n",
            "|    total_timesteps      | 195900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004731993 |\n",
            "|    clip_fraction        | 0.0207      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.43e+05    |\n",
            "|    n_updates            | 19580       |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    value_loss           | 2.32e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7143435.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.14e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1960         |\n",
            "|    time_elapsed         | 751          |\n",
            "|    total_timesteps      | 196000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053329985 |\n",
            "|    clip_fraction        | 0.00495      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.38e+04     |\n",
            "|    n_updates            | 19590        |\n",
            "|    policy_gradient_loss | -0.0132      |\n",
            "|    value_loss           | 1.04e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7147682.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.15e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1961         |\n",
            "|    time_elapsed         | 751          |\n",
            "|    total_timesteps      | 196100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018275748 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.937       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.8e+04      |\n",
            "|    n_updates            | 19600        |\n",
            "|    policy_gradient_loss | -0.00253     |\n",
            "|    value_loss           | 1.25e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7150534.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.15e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1962         |\n",
            "|    time_elapsed         | 752          |\n",
            "|    total_timesteps      | 196200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016748813 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.984       |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.69e+05     |\n",
            "|    n_updates            | 19610        |\n",
            "|    policy_gradient_loss | -0.00852     |\n",
            "|    value_loss           | 4.08e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7154145.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.15e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1963        |\n",
            "|    time_elapsed         | 752         |\n",
            "|    total_timesteps      | 196300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003087799 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.12e+04    |\n",
            "|    n_updates            | 19620       |\n",
            "|    policy_gradient_loss | -0.00778    |\n",
            "|    value_loss           | 1.5e+05     |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7156909.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.16e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1964         |\n",
            "|    time_elapsed         | 752          |\n",
            "|    total_timesteps      | 196400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018284284 |\n",
            "|    clip_fraction        | 0.00278      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.966       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16e+05     |\n",
            "|    n_updates            | 19630        |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    value_loss           | 2.1e+05      |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7160101.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.16e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1965         |\n",
            "|    time_elapsed         | 753          |\n",
            "|    total_timesteps      | 196500       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036171738 |\n",
            "|    clip_fraction        | 0.00495      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 1.25e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.25e+04     |\n",
            "|    n_updates            | 19640        |\n",
            "|    policy_gradient_loss | -0.00917     |\n",
            "|    value_loss           | 1.04e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7163257.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.16e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1966         |\n",
            "|    time_elapsed         | 753          |\n",
            "|    total_timesteps      | 196600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024684723 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.982       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.78e+04     |\n",
            "|    n_updates            | 19650        |\n",
            "|    policy_gradient_loss | -0.00487     |\n",
            "|    value_loss           | 1.43e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7165964.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.17e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1967         |\n",
            "|    time_elapsed         | 753          |\n",
            "|    total_timesteps      | 196700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011931355 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.895       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.08e+04     |\n",
            "|    n_updates            | 19660        |\n",
            "|    policy_gradient_loss | -0.00359     |\n",
            "|    value_loss           | 1.61e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7170350.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.17e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1968        |\n",
            "|    time_elapsed         | 754         |\n",
            "|    total_timesteps      | 196800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003242517 |\n",
            "|    clip_fraction        | 0.0079      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.6e+04     |\n",
            "|    n_updates            | 19670       |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    value_loss           | 1.47e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7174343.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.17e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1969         |\n",
            "|    time_elapsed         | 754          |\n",
            "|    total_timesteps      | 196900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023350073 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.982       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.49e+05     |\n",
            "|    n_updates            | 19680        |\n",
            "|    policy_gradient_loss | -0.00583     |\n",
            "|    value_loss           | 2.67e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7177662.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.18e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1970          |\n",
            "|    time_elapsed         | 755           |\n",
            "|    total_timesteps      | 197000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00046781797 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.975        |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.58e+05      |\n",
            "|    n_updates            | 19690         |\n",
            "|    policy_gradient_loss | -0.00188      |\n",
            "|    value_loss           | 2.79e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7182421.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.18e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1971         |\n",
            "|    time_elapsed         | 755          |\n",
            "|    total_timesteps      | 197100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027622345 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1           |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.32e+04     |\n",
            "|    n_updates            | 19700        |\n",
            "|    policy_gradient_loss | -0.00857     |\n",
            "|    value_loss           | 1.53e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7186498.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.19e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1972         |\n",
            "|    time_elapsed         | 755          |\n",
            "|    total_timesteps      | 197200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007251408 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.925       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.86e+05     |\n",
            "|    n_updates            | 19710        |\n",
            "|    policy_gradient_loss | -0.00158     |\n",
            "|    value_loss           | 4.09e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7189743.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.19e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1973         |\n",
            "|    time_elapsed         | 756          |\n",
            "|    total_timesteps      | 197300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004209173 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.922       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.52e+05     |\n",
            "|    n_updates            | 19720        |\n",
            "|    policy_gradient_loss | -0.00232     |\n",
            "|    value_loss           | 3.36e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7194049.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.19e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1974         |\n",
            "|    time_elapsed         | 756          |\n",
            "|    total_timesteps      | 197400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013221661 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.93        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.31e+04     |\n",
            "|    n_updates            | 19730        |\n",
            "|    policy_gradient_loss | -0.00549     |\n",
            "|    value_loss           | 1.19e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7197378.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.2e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1975          |\n",
            "|    time_elapsed         | 756           |\n",
            "|    total_timesteps      | 197500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00061003055 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.946        |\n",
            "|    explained_variance   | 4.17e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.2e+05       |\n",
            "|    n_updates            | 19740         |\n",
            "|    policy_gradient_loss | -0.00212      |\n",
            "|    value_loss           | 3.93e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7200402.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.2e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1976         |\n",
            "|    time_elapsed         | 757          |\n",
            "|    total_timesteps      | 197600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010087634 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.912       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.33e+04     |\n",
            "|    n_updates            | 19750        |\n",
            "|    policy_gradient_loss | -0.00332     |\n",
            "|    value_loss           | 1.53e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7204190.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.2e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1977         |\n",
            "|    time_elapsed         | 757          |\n",
            "|    total_timesteps      | 197700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020597805 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.911       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.18e+04     |\n",
            "|    n_updates            | 19760        |\n",
            "|    policy_gradient_loss | -0.00561     |\n",
            "|    value_loss           | 1.09e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7207344.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.21e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1978         |\n",
            "|    time_elapsed         | 758          |\n",
            "|    total_timesteps      | 197800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011146825 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.985       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.26e+05     |\n",
            "|    n_updates            | 19770        |\n",
            "|    policy_gradient_loss | -0.00288     |\n",
            "|    value_loss           | 2.74e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7211430.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.21e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1979        |\n",
            "|    time_elapsed         | 758         |\n",
            "|    total_timesteps      | 197900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008098352 |\n",
            "|    clip_fraction        | 0.0241      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.925      |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.46e+04    |\n",
            "|    n_updates            | 19780       |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    value_loss           | 1.13e+05    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7213547.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.21e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1980         |\n",
            "|    time_elapsed         | 758          |\n",
            "|    total_timesteps      | 198000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030796665 |\n",
            "|    clip_fraction        | 0.0101       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.932       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.41e+05     |\n",
            "|    n_updates            | 19790        |\n",
            "|    policy_gradient_loss | -0.00938     |\n",
            "|    value_loss           | 3.24e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7217970.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.22e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1981         |\n",
            "|    time_elapsed         | 759          |\n",
            "|    total_timesteps      | 198100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061001983 |\n",
            "|    clip_fraction        | 0.0347       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.887       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.66e+04     |\n",
            "|    n_updates            | 19800        |\n",
            "|    policy_gradient_loss | -0.0145      |\n",
            "|    value_loss           | 7.92e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7221847.0\n",
            "----------------------------------------\n",
            "| episode_reward          | 7.22e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 260        |\n",
            "|    iterations           | 1982       |\n",
            "|    time_elapsed         | 759        |\n",
            "|    total_timesteps      | 198200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00130768 |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.97      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.52e+05   |\n",
            "|    n_updates            | 19810      |\n",
            "|    policy_gradient_loss | -0.00327   |\n",
            "|    value_loss           | 3.26e+05   |\n",
            "----------------------------------------\n",
            "Episode done — reward = 7223858.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.22e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1983          |\n",
            "|    time_elapsed         | 759           |\n",
            "|    total_timesteps      | 198300        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033594336 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.03         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.47e+05      |\n",
            "|    n_updates            | 19820         |\n",
            "|    policy_gradient_loss | -0.00149      |\n",
            "|    value_loss           | 2.62e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7227404.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.23e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 1984        |\n",
            "|    time_elapsed         | 760         |\n",
            "|    total_timesteps      | 198400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011342295 |\n",
            "|    clip_fraction        | 0.0695      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.88e+04    |\n",
            "|    n_updates            | 19830       |\n",
            "|    policy_gradient_loss | -0.0141     |\n",
            "|    value_loss           | 3.94e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7230085.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.23e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1985          |\n",
            "|    time_elapsed         | 760           |\n",
            "|    total_timesteps      | 198500        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00071858533 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.949        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.56e+05      |\n",
            "|    n_updates            | 19840         |\n",
            "|    policy_gradient_loss | -0.00186      |\n",
            "|    value_loss           | 2.88e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7234121.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.23e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1986         |\n",
            "|    time_elapsed         | 761          |\n",
            "|    total_timesteps      | 198600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044002538 |\n",
            "|    clip_fraction        | 0.0119       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.82e+04     |\n",
            "|    n_updates            | 19850        |\n",
            "|    policy_gradient_loss | -0.00935     |\n",
            "|    value_loss           | 1.04e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7238354.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.24e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1987          |\n",
            "|    time_elapsed         | 761           |\n",
            "|    total_timesteps      | 198700        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00081076706 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.969        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.36e+05      |\n",
            "|    n_updates            | 19860         |\n",
            "|    policy_gradient_loss | -0.00196      |\n",
            "|    value_loss           | 2.95e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7241404.0\n",
            "-------------------------------------------\n",
            "| episode_reward          | 7.24e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 1988          |\n",
            "|    time_elapsed         | 761           |\n",
            "|    total_timesteps      | 198800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00071907893 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.899        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.4e+05       |\n",
            "|    n_updates            | 19870         |\n",
            "|    policy_gradient_loss | -0.00372      |\n",
            "|    value_loss           | 2.97e+05      |\n",
            "-------------------------------------------\n",
            "Episode done — reward = 7243874.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.24e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1989         |\n",
            "|    time_elapsed         | 762          |\n",
            "|    total_timesteps      | 198900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017944521 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.984       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.97e+04     |\n",
            "|    n_updates            | 19880        |\n",
            "|    policy_gradient_loss | -0.00621     |\n",
            "|    value_loss           | 1.45e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7247228.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.25e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1990         |\n",
            "|    time_elapsed         | 762          |\n",
            "|    total_timesteps      | 199000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052688005 |\n",
            "|    clip_fraction        | 0.0177       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.94e+04     |\n",
            "|    n_updates            | 19890        |\n",
            "|    policy_gradient_loss | -0.0143      |\n",
            "|    value_loss           | 7.62e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7250329.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.25e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1991         |\n",
            "|    time_elapsed         | 762          |\n",
            "|    total_timesteps      | 199100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016430966 |\n",
            "|    clip_fraction        | 0.00139      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.47e+04     |\n",
            "|    n_updates            | 19900        |\n",
            "|    policy_gradient_loss | -0.00248     |\n",
            "|    value_loss           | 1.65e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7253498.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.25e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 260          |\n",
            "|    iterations           | 1992         |\n",
            "|    time_elapsed         | 763          |\n",
            "|    total_timesteps      | 199200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037814754 |\n",
            "|    clip_fraction        | 0.013        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.889       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.36e+04     |\n",
            "|    n_updates            | 19910        |\n",
            "|    policy_gradient_loss | -0.0114      |\n",
            "|    value_loss           | 1.44e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7257099.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.26e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 1993         |\n",
            "|    time_elapsed         | 763          |\n",
            "|    total_timesteps      | 199300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027779462 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.972       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.73e+04     |\n",
            "|    n_updates            | 19920        |\n",
            "|    policy_gradient_loss | -0.00907     |\n",
            "|    value_loss           | 1.99e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7259875.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.26e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 1994         |\n",
            "|    time_elapsed         | 763          |\n",
            "|    total_timesteps      | 199400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008115969 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.909       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1e+05        |\n",
            "|    n_updates            | 19930        |\n",
            "|    policy_gradient_loss | -0.00197     |\n",
            "|    value_loss           | 2.04e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7262024.0\n",
            "-----------------------------------------\n",
            "| episode_reward          | 7.26e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 261         |\n",
            "|    iterations           | 1995        |\n",
            "|    time_elapsed         | 764         |\n",
            "|    total_timesteps      | 199500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001220243 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.977      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.51e+04    |\n",
            "|    n_updates            | 19940       |\n",
            "|    policy_gradient_loss | -0.00277    |\n",
            "|    value_loss           | 7.54e+04    |\n",
            "-----------------------------------------\n",
            "Episode done — reward = 7265756.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.27e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 1996         |\n",
            "|    time_elapsed         | 764          |\n",
            "|    total_timesteps      | 199600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023846468 |\n",
            "|    clip_fraction        | 0.00217      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.862       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.94e+04     |\n",
            "|    n_updates            | 19950        |\n",
            "|    policy_gradient_loss | -0.00571     |\n",
            "|    value_loss           | 4.25e+04     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7270082.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.27e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 1997         |\n",
            "|    time_elapsed         | 765          |\n",
            "|    total_timesteps      | 199700       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013664623 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.08e+05     |\n",
            "|    n_updates            | 19960        |\n",
            "|    policy_gradient_loss | -0.00157     |\n",
            "|    value_loss           | 1.71e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7273276.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.27e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 1998         |\n",
            "|    time_elapsed         | 765          |\n",
            "|    total_timesteps      | 199800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012480892 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.953       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.51e+05     |\n",
            "|    n_updates            | 19970        |\n",
            "|    policy_gradient_loss | -0.00479     |\n",
            "|    value_loss           | 3.13e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7276956.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.28e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 1999         |\n",
            "|    time_elapsed         | 765          |\n",
            "|    total_timesteps      | 199900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023843711 |\n",
            "|    clip_fraction        | 0.00295      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.967       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.31e+04     |\n",
            "|    n_updates            | 19980        |\n",
            "|    policy_gradient_loss | -0.00663     |\n",
            "|    value_loss           | 1.68e+05     |\n",
            "------------------------------------------\n",
            "Episode done — reward = 7279982.0\n",
            "------------------------------------------\n",
            "| episode_reward          | 7.28e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 261          |\n",
            "|    iterations           | 2000         |\n",
            "|    time_elapsed         | 766          |\n",
            "|    total_timesteps      | 200000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026270105 |\n",
            "|    clip_fraction        | 0.0152       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.19e+05     |\n",
            "|    n_updates            | 19990        |\n",
            "|    policy_gradient_loss | -0.00597     |\n",
            "|    value_loss           | 2.25e+05     |\n",
            "------------------------------------------\n",
            "Running LOOK benchmark...\n",
            "36\n",
            "\n",
            "Total reward for LOOK benchmark over 100 steps: 760.0\n",
            "info[\"done\"]: 50\n",
            "Average reward: 1051.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_test_ppo(SB3_PPO, 3, 1)"
      ],
      "metadata": {
        "id": "j2gylDFqzoSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9c52f9df-8b5d-4057-a8b4-55ebd7cd4ee7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 133\n",
            "info[\"done\"]: 103\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 118\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 93\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 90\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 95\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 87\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 91\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 80\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 97\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 136\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 80\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 96\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 95\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 87\n",
            "info[\"done\"]: 96\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 122\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 87\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 110\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 110\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 88\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 87\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 86\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 121\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 90\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 102\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 115\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 102\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 80\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 85\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 131\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 114\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 90\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 103\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 94\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 115\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 92\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 102\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 89\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 90\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 103\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 88\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 86\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 82\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 80\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 77\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 90\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 78\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 94\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 84\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 84\n",
            "info[\"done\"]: 62\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 71\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 99\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 63\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 99\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 82\n",
            "info[\"done\"]: 79\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 105\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 95\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 66\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 74\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 64\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 69\n",
            "info[\"done\"]: 83\n",
            "info[\"done\"]: 84\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 92\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 53\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 47\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 67\n",
            "info[\"done\"]: 114\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 55\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 75\n",
            "info[\"done\"]: 50\n",
            "info[\"done\"]: 76\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 51\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 118\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 72\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 73\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 60\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 81\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 92\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 58\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 61\n",
            "info[\"done\"]: 59\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 110\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 70\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 46\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_test_ppo(SB3_PPO, 6, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rEK80IgpIekF",
        "outputId": "3422385a-79b8-4d15-b52b-dd8104248551"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 56\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 52\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 44\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 57\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 42\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 68\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 38\n",
            "info[\"done\"]: 45\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 65\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 41\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 49\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 32\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 48\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 37\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 35\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 36\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 43\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 1\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 34\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 40\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 39\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 54\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 24\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 33\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 0\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 29\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 2\n",
            "info[\"done\"]: 22\n",
            "info[\"done\"]: 23\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 9\n",
            "info[\"done\"]: 25\n",
            "info[\"done\"]: 26\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 10\n",
            "info[\"done\"]: 30\n",
            "info[\"done\"]: 3\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 18\n",
            "info[\"done\"]: 8\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 15\n",
            "info[\"done\"]: 4\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 7\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 13\n",
            "info[\"done\"]: 14\n",
            "info[\"done\"]: 17\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 27\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 12\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 31\n",
            "info[\"done\"]: 11\n",
            "info[\"done\"]: 20\n",
            "info[\"done\"]: 6\n",
            "info[\"done\"]: 28\n",
            "info[\"done\"]: 5\n",
            "info[\"done\"]: 19\n",
            "info[\"done\"]: 16\n",
            "info[\"done\"]: 21\n",
            "info[\"done\"]: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "render_test_ppo(SB3_PPO, 3, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vju24PIZNJsh",
        "outputId": "d2199c27-92ac-430c-947e-d6c6bb6d6cd0"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=44.7s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=47.0s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=49.0s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=51.3s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.3s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=1.51 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=58.0s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=60.0s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=60.0s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=62.0s | waiting=24\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=64.3s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=27\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=66.3s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=66.3s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=68.3s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=7 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=0.31 dir=-1 load=7 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=70.7s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=7 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=72.7s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=0.53 dir=+1 load=7 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=75.2s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=77.2s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=7 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=0.75 dir=+1 load=7 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=80.6s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=82.6s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=87.3s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=89.3s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=89.3s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=31 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=91.3s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=31 it=2\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=9\n",
            "event: SPAWN\n",
            " Car0 floor=1.15 dir=-1 load=31 it=0\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=95.3s | waiting=9\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=31 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=97.3s | waiting=9\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=31 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=99.6s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=18 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=101.6s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=18 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 40\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.44 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=7.8s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=9.8s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=12.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=14.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=14.1s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=16.1s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=16.1s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=18.1s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=18.1s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=20.1s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=1.31 dir=+1 load=6 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.47 dir=+1 load=6 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=24.8s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=26.8s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=12\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=33.4s | waiting=12\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=35.4s | waiting=12\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=1.35 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=37.7s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=39.7s | waiting=26\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=42.0s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=33\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=44.0s | waiting=33\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=44.0s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=46.0s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=46.0s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=48.0s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=50.4s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=52.4s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=1.43 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.8s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=59.8s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=59.8s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=61.8s | waiting=24\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=15 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=64.1s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=27\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=66.1s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=0.52 dir=-1 load=7 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=68.4s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=7 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=70.4s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=1.88 dir=+1 load=7 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=1.24 dir=-1 load=7 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=76.8s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.8s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=0.04 dir=+1 load=7 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=79.0s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=81.0s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=85.7s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=87.7s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=92.3s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=94.3s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=96.7s | waiting=36\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=98.7s | waiting=36\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=4 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=101.0s | waiting=36\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 26\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=0\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.0s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=6.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=4\n",
            "event: SPAWN\n",
            " Car0 floor=0.33 dir=+1 load=6 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=7.5s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=9.5s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=6 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=9.5s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=11.5s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=11.5s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=17.5s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=22.2s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=24.2s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=24.2s | waiting=1\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=14 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=26.2s | waiting=1\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=14 it=0\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=28.5s | waiting=1\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=30.5s | waiting=1\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=30.5s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=5 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=1\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=5 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=32.5s | waiting=1\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=5 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=34.9s | waiting=1\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=36.9s | waiting=5\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=1.34 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=40.0s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=42.0s | waiting=15\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=1.27 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=46.6s | waiting=22\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=48.6s | waiting=22\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=0.57 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.8s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.8s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.8s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=59.8s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=15 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=64.5s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=66.5s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=8 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=1.66 dir=-1 load=8 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=0.51 dir=-1 load=8 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=1.57 dir=+1 load=8 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=24\n",
            "event: SPAWN\n",
            " Car0 floor=0.94 dir=-1 load=8 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=76.1s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.1s | waiting=24\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=8 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=0.34 dir=+1 load=8 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=82.8s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=84.8s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=8 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=89.4s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=91.4s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=0.80 dir=+1 load=8 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=93.8s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=95.8s | waiting=26\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=95.8s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=97.8s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=9 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=100.1s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 37\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=2.3s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.7s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=10.7s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=15.3s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=17.3s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=22.0s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=24.0s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=26.3s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=28.3s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=1 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=30.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=32.7s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=1.50 dir=+1 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=37.3s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=39.3s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=0.15 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=48.0s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=50.0s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=52.3s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.3s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.3s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=56.3s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=56.3s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=58.3s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=60.7s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.7s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.0s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.0s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=0.88 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.5s | waiting=44\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.5s | waiting=44\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=1.17 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=71.9s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=30 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=30 it=1\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=75.9s | waiting=19\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=30 it=0\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=0.70 dir=-1 load=30 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=80.5s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=17 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=82.5s | waiting=20\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=17 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=87.2s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=17 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=89.2s | waiting=20\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=17 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=0.25 dir=-1 load=17 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=95.1s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=97.1s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=99.4s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=101.4s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 41\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=5.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=7.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=7.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=9.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=12.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=14.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=18.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=20.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.06 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=23.3s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=25.3s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=27.7s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=29.7s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=1.42 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=32.0s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=34.0s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=1.93 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.3s | waiting=22\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=38.3s | waiting=22\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=38.3s | waiting=12\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=40.3s | waiting=22\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=10 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=42.7s | waiting=22\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=44.7s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=44.7s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=5 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=46.7s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=5 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=49.0s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=51.0s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=51.0s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=14 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=53.0s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=14 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=1.94 dir=-1 load=14 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=1.37 dir=-1 load=14 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.3s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.3s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=59.7s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=61.7s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=0.20 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=0.80 dir=+1 load=7 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.7s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.7s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=0.91 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.98 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=72.5s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=74.5s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=36\n",
            "event: SPAWN\n",
            " Car0 floor=0.11 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=83.3s | waiting=36\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=85.3s | waiting=36\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=85.3s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=87.3s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=87.3s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=89.3s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=89.3s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=91.3s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=18 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=91.3s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=1\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=18 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=98.0s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=100.0s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=5 it=0\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=100.0s | waiting=9\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=26 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=102.0s | waiting=9\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=26 it=0\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 27\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=5.4s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=2\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=4\n",
            "event: SPAWN\n",
            " Car0 floor=0.59 dir=+1 load=6 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=10.1s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=12.1s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=16.7s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=18.7s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=18.7s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=20.7s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=4 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=23.1s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=25.1s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=25.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=27.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=29.4s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=31.4s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=33.7s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=35.7s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=1.19 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.1s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=40.1s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=0.46 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=44.9s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=46.9s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=49.3s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=51.3s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=1.19 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=0.62 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.9s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.9s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=60.3s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.3s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=64.6s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=66.6s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=66.6s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=68.6s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=15 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=68.6s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=70.6s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=15 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=70.6s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=72.6s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=15 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=72.6s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=15 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=74.6s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=15 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=1.85 dir=+1 load=15 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=83.2s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=15 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=85.2s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=15 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=89.9s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=91.9s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=1.39 dir=-1 load=8 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=94.2s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=96.2s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=96.2s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=98.2s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=98.2s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=100.2s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 22\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.96 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=6.9s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.9s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.9s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=10.9s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=13.2s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=15.2s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=17.5s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=19.5s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=1.57 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=1.42 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=26.9s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=28.9s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=0.93 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.2s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=33.2s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=1 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=35.5s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=37.5s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=37.5s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=39.5s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=12 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=39.5s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=41.5s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=23\n",
            "event: SPAWN\n",
            " Car0 floor=0.91 dir=+1 load=12 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=43.9s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=45.9s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=5 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=48.2s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=50.2s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=0.74 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.8s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.8s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.8s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.8s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=60.1s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.1s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.1s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=64.1s | waiting=20\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=64.1s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=23\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=66.1s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=0.50 dir=+1 load=10 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=68.4s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=5 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=70.4s | waiting=26\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=5 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=27\n",
            "event: SPAWN\n",
            " Car0 floor=1.86 dir=+1 load=5 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.22 dir=-1 load=5 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=76.8s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.8s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=5 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=0.06 dir=+1 load=5 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=83.4s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=85.4s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=36\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=95.6s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=97.6s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=5 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=97.6s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=99.6s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=9 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=102.0s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 30\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=0\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.0s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.3s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.3s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=10.3s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=12.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=14.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=14.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=16.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=21.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.3s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.3s | waiting=1\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=25.3s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=25.3s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=27.3s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=27.3s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=29.3s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=8\n",
            "event: SPAWN\n",
            " Car0 floor=0.72 dir=+1 load=10 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.7s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=33.7s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=3 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.0s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.0s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.0s | waiting=5\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=6 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=40.0s | waiting=15\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=6 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=40.0s | waiting=5\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=42.0s | waiting=5\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=16 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=12\n",
            "event: SPAWN\n",
            " Car0 floor=1.29 dir=-1 load=16 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=46.7s | waiting=12\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=9 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=48.7s | waiting=12\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=9 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=48.7s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=21 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=50.7s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=21 it=0\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=50.7s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=21 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=52.7s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=21 it=0\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=52.7s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=21 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=3\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=21 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=21 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.7s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=21 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=59.3s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=15 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=61.3s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=15 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=61.3s | waiting=3\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=19 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=63.3s | waiting=3\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=19 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.91 dir=-1 load=19 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=8\n",
            "event: SPAWN\n",
            " Car0 floor=1.51 dir=+1 load=19 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=68.5s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=9\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=70.5s | waiting=9\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.84 dir=+1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=72.8s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=74.8s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=74.8s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=76.8s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=8\n",
            "event: SPAWN\n",
            " Car0 floor=1.10 dir=-1 load=9 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=81.0s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=83.0s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=9 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=87.7s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=89.7s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=89.7s | waiting=5\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=91.7s | waiting=5\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=0.70 dir=+1 load=6 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=94.9s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=96.9s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=99.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=101.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 51\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=2.3s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.19 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=7.2s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=9.2s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=11.5s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=17.5s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=22.2s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=24.2s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=24.2s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=26.2s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=26.2s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=28.2s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=8\n",
            "event: SPAWN\n",
            " Car0 floor=1.21 dir=+1 load=10 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=32.9s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=34.9s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=8 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=34.9s | waiting=2\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=14 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=14 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=36.9s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=14 it=0\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=1.34 dir=-1 load=14 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=39.2s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=41.2s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.5s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=23\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=45.5s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=50.2s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=52.2s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.43 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=55.5s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=57.5s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=57.5s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=59.5s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=59.5s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=61.5s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=61.5s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=63.5s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=63.5s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.5s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.6s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=71.6s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=71.6s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=20 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=20 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=73.6s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=20 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=73.6s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=20 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=20 it=0\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=75.6s | waiting=19\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=20 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=0.58 dir=-1 load=20 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=79.9s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=12 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=81.9s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=12 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=81.9s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=83.9s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=86.2s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=88.2s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=92.9s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=94.9s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=97.2s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=99.2s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=101.5s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 45\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.60 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.8s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=8.8s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=17.8s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=19.8s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=22.1s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=24.1s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=28.8s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=30.8s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=12\n",
            "event: SPAWN\n",
            " Car0 floor=0.10 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=35.5s | waiting=12\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=37.5s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=37.5s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=10 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=39.5s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=10 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=23\n",
            "event: SPAWN\n",
            " Car0 floor=0.20 dir=-1 load=10 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=44.1s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=46.1s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=48.5s | waiting=22\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=50.5s | waiting=22\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=52.8s | waiting=22\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.8s | waiting=29\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.8s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=56.8s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=16 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=56.8s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=58.8s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=16 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=58.8s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=60.8s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=16 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=60.8s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=62.8s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=16 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.32 dir=+1 load=16 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=1.91 dir=+1 load=16 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=0.77 dir=-1 load=16 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=71.7s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=16 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=16 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.7s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=16 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=24\n",
            "event: SPAWN\n",
            " Car0 floor=0.08 dir=+1 load=16 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=76.1s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=11 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.1s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=11 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.1s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=12 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=80.1s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=12 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=82.4s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=11 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=84.4s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=11 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=86.7s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=11 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=88.7s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=11 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=88.7s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=90.7s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=90.7s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=92.7s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=11 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=0.75 dir=-1 load=11 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=95.1s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=11 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=97.1s | waiting=26\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=11 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=97.1s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=99.1s | waiting=26\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=11 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=99.1s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=101.1s | waiting=26\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=11 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 25\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.60 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=25.2s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=33.4s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=35.4s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=0.65 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=37.7s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=39.7s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=39.7s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=41.7s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=11 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=24\n",
            "event: SPAWN\n",
            " Car0 floor=0.84 dir=+1 load=11 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=44.0s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=46.0s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=5 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=48.4s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=50.4s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=50.4s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=52.4s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=16 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.66 dir=-1 load=16 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=1.09 dir=-1 load=16 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=56.6s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=16 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=58.6s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=16 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=60.9s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.9s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.9s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=64.9s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=7 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=64.9s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=66.9s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=7 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=1.15 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.3s | waiting=19\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=71.3s | waiting=20\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=1.49 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=24\n",
            "event: SPAWN\n",
            " Car0 floor=0.86 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=74.2s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=10 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=76.2s | waiting=21\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=10 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=78.6s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=80.6s | waiting=22\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=80.6s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=82.6s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 28\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=5.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=7.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=7.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=9.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=12.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=14.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.84 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=27.9s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=29.9s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=0.50 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=34.5s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.5s | waiting=22\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.19 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=41.2s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.2s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.2s | waiting=21\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=11 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=45.2s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=11 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=49.9s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=51.9s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=1.45 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=0.88 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=56.5s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=58.5s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=60.9s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.9s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=0.40 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=68.7s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=70.7s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=0.24 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.0s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=75.0s | waiting=44\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=1.67 dir=+1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=79.7s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=81.7s | waiting=45\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=84.0s | waiting=44\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=86.0s | waiting=44\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=1 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=86.0s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=88.0s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=4 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=88.0s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=90.0s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=4 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=90.0s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=92.0s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=0.44 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=94.3s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=96.3s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=96.3s | waiting=36\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=98.3s | waiting=36\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=103.0s | waiting=36\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 21\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=2\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.60 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.19 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=10.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=10.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=12.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=17.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=19.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=21.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=23.7s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=26.0s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=28.0s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=30.3s | waiting=12\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=32.3s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=1 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=34.7s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.7s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=38.7s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=12 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=38.7s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=40.7s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=23\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=46.0s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=48.0s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=5 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=50.3s | waiting=23\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=52.3s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=1.65 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.0s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=56.0s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=56.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=20 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=58.0s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=20 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=60.3s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=10 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=62.3s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=10 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=64.6s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=10 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=10 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=66.6s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=10 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=1.73 dir=-1 load=10 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.0s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=10 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=10 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=71.0s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=10 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=71.0s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=10 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.0s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=10 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.0s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=11 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=75.0s | waiting=19\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=11 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=75.0s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=77.0s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=1.83 dir=+1 load=13 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=83.2s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=85.2s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=87.5s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=89.5s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=91.8s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=23\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=93.8s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=93.8s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=95.8s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=13 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=95.8s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=13 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=97.8s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=13 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=102.5s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 44\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.96 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=6.9s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.9s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=20.2s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=22.2s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=22.2s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=24.2s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=35.7s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=37.7s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=0.31 dir=+1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=39.1s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=41.1s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=1.08 dir=+1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=43.8s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=45.8s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=48.2s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=50.2s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=0.73 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=53.8s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.8s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=1 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.8s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.8s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=60.1s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=62.1s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=0.39 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=0.99 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.3s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=1.28 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=71.6s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.6s | waiting=48\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=1.87 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=74.2s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=76.2s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=78.5s | waiting=50\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=80.5s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=82.9s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=84.9s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=84.9s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=28 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=86.9s | waiting=24\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=28 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=28 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=95.6s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=18 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=97.6s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=18 it=1\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=100.0s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=102.0s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 37\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=11.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=13.0s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=15.3s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=17.3s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=17.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=19.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=1.84 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=27.9s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=29.9s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=29.9s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=8\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=10 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.9s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=10 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=31.9s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=33.9s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=11 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=0.99 dir=+1 load=11 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.2s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=3 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.2s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=3 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=40.2s | waiting=20\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=3 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=42.5s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=27\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=44.5s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=49.2s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=51.2s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.16 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=1.73 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=55.1s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.1s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=57.1s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=20 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=59.1s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=20 it=1\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=61.4s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=10 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=63.4s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=10 it=0\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=65.8s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.8s | waiting=19\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=0.93 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.1s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=21\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=24\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=74.1s | waiting=24\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=74.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=14 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=76.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=14 it=1\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.19 dir=+1 load=14 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=80.8s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=82.8s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=85.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=87.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=1 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=87.1s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=89.1s | waiting=7\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=4 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=91.5s | waiting=7\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=12\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=93.5s | waiting=12\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=98.1s | waiting=12\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=100.1s | waiting=12\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 50\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=2\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.60 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=0.81 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=8.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=10.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=10.7s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=12.7s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=15.0s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=17.0s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=2 it=2\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=17.0s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=2 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=19.0s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=2 it=1\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=21.3s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.3s | waiting=5\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=2 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=0.10 dir=+1 load=2 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=25.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=27.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=3 it=0\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=30.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=32.0s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=1.79 dir=+1 load=3 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=36.7s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=38.7s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=43.3s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=45.3s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=45.3s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=47.3s | waiting=20\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=12 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=52.0s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [2]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=23\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.0s | waiting=23\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=6 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=27\n",
            "event: SPAWN\n",
            " Car0 floor=1.80 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=54.9s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=56.9s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=6 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=61.6s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=63.6s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=0.97 dir=+1 load=6 it=1\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.57 dir=+1 load=6 it=2\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=33\n",
            "event: SPAWN\n",
            " Car0 floor=0.42 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=70.9s | waiting=33\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 2]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=2\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=72.9s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2]\n",
            "----------------------------------------\n",
            "t=72.9s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=9 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=74.9s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=9 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=1.71 dir=+1 load=9 it=2\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=79.6s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=81.6s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=8 it=0\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=81.6s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=39 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=83.6s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=39 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=88.3s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=26 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=90.3s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=26 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=92.6s | waiting=1\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=3 it=2\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=94.6s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=3 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=94.6s | waiting=5\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=96.6s | waiting=5\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=4 it=2\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=96.6s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=9 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=98.6s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=9 it=2\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=100.9s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "info[\"done\"]: 58\n",
            "[1222.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "render_test_ppo(SB3_PPO, 6, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWurU-NMNZ49",
        "outputId": "01befe9c-f77b-4088-c8aa-152ec08f364e"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=1 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=4.74 dir=+1 load=1 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.8s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=55.8s | waiting=45\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=0.67 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=1.27 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=0.12 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=1.18 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.9s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.9s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=2.73 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=81.9s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=83.9s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.2s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=54\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=95.2s | waiting=54\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=97.5s | waiting=43\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=12 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=99.5s | waiting=43\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=12 it=5\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=101.9s | waiting=43\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=12 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 7\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.90 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=11.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+0 load=4 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=20.3s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=22.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=3 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=3.37 dir=+1 load=3 it=4\n",
            " Hall‑calls: [1, 2]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=3.53 dir=+1 load=3 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=24.8s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=26.8s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=3 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.1s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.1s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=1 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.8s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.8s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.8s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+0 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=39.8s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.1s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=44.1s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=1.13 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=1.70 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=55.2s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=57.2s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=61.8s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=63.8s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=1 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=3.13 dir=-1 load=1 it=3\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=3.73 dir=+1 load=1 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.58 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=1.52 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.6s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=2 it=3\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=75.6s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=75.6s | waiting=50\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=2 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.6s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=2 it=2\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=2.57 dir=+1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=80.2s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=82.2s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=89.2s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=91.2s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=91.2s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+0 load=24 it=None\n",
            " Hall‑calls: [0, 1, 3, 4]\n",
            "----------------------------------------\n",
            "t=93.2s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+0 load=24 it=3\n",
            " Hall‑calls: [0, 1, 3, 4]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=33\n",
            "event: SPAWN\n",
            " Car0 floor=4.97 dir=-1 load=24 it=0\n",
            " Hall‑calls: [0, 1, 3, 4]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=0.65 dir=-1 load=24 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 5\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.5s | waiting=2\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.5s | waiting=2\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.13 dir=+1 load=4 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.8s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=17.8s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.1s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=22.1s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=4 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=3.54 dir=-1 load=4 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=3.39 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.1s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.1s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=3.16 dir=+1 load=4 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.5s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=-1 load=4 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.5s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=0.81 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=4.88 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=4.31 dir=-1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=55.2s | waiting=42\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=57.2s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=4 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=59.5s | waiting=42\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=61.5s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=4.40 dir=-1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=3.25 dir=-1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=70.5s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=-1 load=4 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.5s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=4 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=2.41 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.2s | waiting=50\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.2s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=88.5s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=90.5s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=56\n",
            "event: SPAWN\n",
            " Car0 floor=3.81 dir=-1 load=6 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=97.5s | waiting=56\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=99.5s | waiting=56\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=61\n",
            "event: SPAWN\n",
            " Car0 floor=3.66 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 0\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.04 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=4.84 dir=-1 load=0 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=1.65 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=32.5s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=34.5s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=5 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.70 dir=+1 load=5 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=27\n",
            "event: SPAWN\n",
            " Car0 floor=2.66 dir=+1 load=5 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=40.0s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.0s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=5 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=2.73 dir=+1 load=5 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=47.7s | waiting=33\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=49.7s | waiting=33\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=52.0s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.0s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.0s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=56.0s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=7 it=0\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=58.4s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=60.4s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=7 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.0s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.0s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=7 it=3\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=1.11 dir=+1 load=7 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=2.25 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=3.32 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=2.68 dir=-1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.0s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=8 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.0s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=8 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.0s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=81.0s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=11 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=81.0s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=83.0s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=11 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=83.0s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=85.0s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=11 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=92.0s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=11 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=11 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=94.0s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=11 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=96.3s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=98.3s | waiting=51\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=56\n",
            "event: SPAWN\n",
            " Car0 floor=2.18 dir=+1 load=3 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 8\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.44 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=10.4s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=12.4s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=14.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=16.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=16.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=18.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=0 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=3.91 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=4.07 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=0.87 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=3.08 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=2.12 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=41.0s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.0s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=5 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=0.73 dir=-1 load=5 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=49.0s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=51.0s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=3.94 dir=+1 load=5 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=3.37 dir=-1 load=5 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=60.0s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=62.0s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=64.3s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.3s | waiting=43\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=0.40 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=1.54 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.61 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=3.25 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=1.11 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.7s | waiting=50\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.7s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=1 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.7s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=89.7s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=92.0s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+1 load=3 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=94.0s | waiting=52\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=103.3s | waiting=52\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 7\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=0.01 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=11.4s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.4s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=18.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.66 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=2.81 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=24.0s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=26.0s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=28.3s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=30.3s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=3.70 dir=-1 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.7s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=33.7s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=2.95 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.99 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.0s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=45.0s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=3.47 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=4.04 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=61.6s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=63.6s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=1.99 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=1.39 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=68.7s | waiting=44\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=1\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=70.7s | waiting=45\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=70.7s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=7 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.7s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=7 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=2.53 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.5s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.5s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.5s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=81.5s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=18 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=83.8s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=12 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.8s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=12 it=0\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.8s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.8s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.8s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=89.8s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=16 it=3\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=89.8s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=16 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=91.8s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=16 it=5\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=0.64 dir=+1 load=16 it=2\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=101.1s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=12 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 11\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.04 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=9.2s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=11.2s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.5s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=22.5s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=4.72 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=4.56 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=24.9s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=26.9s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=2.22 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.2s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.2s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=3.47 dir=-1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.0s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=44.0s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=46.3s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=48.3s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=50.7s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=52.7s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=2.79 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=2.22 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=59.7s | waiting=42\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=61.7s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=1.81 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.41 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=1.26 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=70.5s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.5s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=2 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=1.59 dir=+1 load=2 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.2s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.2s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.2s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+0 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=81.2s | waiting=48\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+0 load=5 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.9s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.9s | waiting=48\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=53\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=102.6s | waiting=53\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 5\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.44 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=12.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=14.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=17.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=19.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.23 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.38 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=26.8s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=28.8s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=0.96 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.1s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=33.1s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.96 dir=+1 load=2 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=40.7s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.7s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=7 it=3\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.43 dir=+1 load=7 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=47.3s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=49.3s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=51.7s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=-1 load=7 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.7s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=1.65 dir=-1 load=7 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=62.3s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=64.3s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=4.32 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.6s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=68.6s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=3.43 dir=-1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=4.50 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=3.86 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.2s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=76.2s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=8 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.86 dir=-1 load=8 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=83.9s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.9s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=1.83 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=96.0s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=10 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=98.0s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=10 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=102.7s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=11 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 3\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.96 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.9s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.9s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.5s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=22.5s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=3.28 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=3.44 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=26.9s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=28.9s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=1.10 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=33.6s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.6s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=0.25 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.9s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=39.9s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=2.60 dir=+1 load=2 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=49.3s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=51.3s | waiting=37\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=1 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=4.19 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.6s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=-1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=55.6s | waiting=44\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=62.6s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=64.6s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=1.55 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=2.15 dir=+1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.6s | waiting=43\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.6s | waiting=43\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=2.14 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=3.21 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=3.85 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=76.6s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.6s | waiting=48\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=81.2s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=83.2s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=9 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.6s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.6s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.6s | waiting=19\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+0 load=36 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=89.6s | waiting=19\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+0 load=36 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=24\n",
            "event: SPAWN\n",
            " Car0 floor=3.40 dir=-1 load=36 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=94.2s | waiting=19\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=37 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=96.2s | waiting=19\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=37 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=98.6s | waiting=18\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=32 it=None\n",
            " Hall‑calls: [0, 1, 2, 3]\n",
            "----------------------------------------\n",
            "t=100.6s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=32 it=1\n",
            " Hall‑calls: [0, 1, 2, 3]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 12\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=5.7s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=8\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=7.7s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=2 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=10.1s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=12.1s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=2 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=12.1s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=2 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=14.1s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=2 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=16.4s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=18.4s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=2 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.1s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=3\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=25.1s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=2\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=27.4s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [5]\n",
            "----------------------------------------\n",
            "t=29.4s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=5\n",
            " Hall‑calls: [5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.30 dir=-1 load=7 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=3.51 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=2.55 dir=-1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=41.8s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=43.8s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=46.1s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=48.1s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=50.5s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=52.5s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=3.71 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.8s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=55.8s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=62.8s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=64.8s | waiting=37\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=8 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=1.44 dir=+1 load=8 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=0.84 dir=-1 load=8 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=1.99 dir=+1 load=8 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=0.93 dir=-1 load=8 it=0\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.6s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=17 it=None\n",
            " Hall‑calls: [0, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=17 it=3\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.6s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=17 it=1\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=76.9s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=11 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=11 it=4\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=11 it=5\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=90.6s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=92.6s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=4.70 dir=-1 load=8 it=0\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=0.37 dir=-1 load=8 it=0\n",
            " Hall‑calls: [0, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 10\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.90 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=17.9s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=19.9s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=2 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=22.2s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=9\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=24.2s | waiting=15\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=2 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=28.9s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=30.9s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=3 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=1.93 dir=-1 load=3 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=33.5s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=35.5s | waiting=15\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=2.72 dir=-1 load=3 it=0\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=29\n",
            "event: SPAWN\n",
            " Car0 floor=1.76 dir=-1 load=3 it=0\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=42.5s | waiting=29\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=36\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=44.5s | waiting=36\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=44.5s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=46.5s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=7 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=46.5s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=48.5s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=7 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=48.5s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=50.5s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=1.13 dir=+1 load=7 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=1.70 dir=+1 load=7 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=59.8s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=61.8s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=2.28 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=1.68 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=2.83 dir=+1 load=4 it=5\n",
            " Hall‑calls: [1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=1.76 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=1.12 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.2s | waiting=43\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=10 it=None\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=76.2s | waiting=43\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=10 it=4\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=2.16 dir=+1 load=10 it=4\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=83.9s | waiting=44\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.9s | waiting=44\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=3.16 dir=+1 load=4 it=4\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=98.3s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=100.3s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 9\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.44 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=10.4s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=12.4s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=19.4s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=21.4s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=0.77 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=0.93 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=4.12 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=22\n",
            "event: SPAWN\n",
            " Car0 floor=1.92 dir=-1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=2.88 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=0.63 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=46.9s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=48.9s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=1 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=48.9s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=50.9s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=1 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=50.9s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=52.9s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=1 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=2.13 dir=+1 load=1 it=4\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=2.69 dir=+1 load=1 it=4\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=57.5s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=59.5s | waiting=45\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=1 it=4\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=59.5s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=1 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=61.5s | waiting=45\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=1 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=2.14 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.2s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=-1 load=1 it=3\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=68.2s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=1 it=2\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=1.25 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=71.7s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=1 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.7s | waiting=52\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=1 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=55\n",
            "event: SPAWN\n",
            " Car0 floor=1.91 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.8s | waiting=54\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=55\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=80.8s | waiting=55\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=2 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=90.1s | waiting=55\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=92.1s | waiting=55\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=2 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=92.1s | waiting=51\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=56\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=5\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=94.1s | waiting=56\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=4\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=61\n",
            "event: SPAWN\n",
            " Car0 floor=3.97 dir=+1 load=6 it=5\n",
            " Hall‑calls: [1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 0\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=0.01 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=18.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.66 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=2.50 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=27.1s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.1s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=3.16 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.1s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.1s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=5 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=27\n",
            "event: SPAWN\n",
            " Car0 floor=1.15 dir=+1 load=5 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.8s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=40.8s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=5 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=40.8s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.8s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=7 it=3\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.38 dir=+1 load=7 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=44.5s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=46.5s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=8 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=48.9s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=50.9s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=9 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=33\n",
            "event: SPAWN\n",
            " Car0 floor=1.03 dir=-1 load=9 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=1.59 dir=+1 load=9 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=57.8s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=59.8s | waiting=37\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=9 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=62.1s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=64.1s | waiting=37\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=9 it=0\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=1.24 dir=-1 load=9 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=1.84 dir=+1 load=9 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=2.99 dir=+1 load=9 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.2s | waiting=43\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=-1 load=9 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=-1 load=9 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.2s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=9 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=9 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=88.2s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=90.2s | waiting=48\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=53\n",
            "event: SPAWN\n",
            " Car0 floor=1.32 dir=+1 load=3 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=99.6s | waiting=52\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=101.6s | waiting=52\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 7\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=1.44 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=12.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=14.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=17.1s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=19.1s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.23 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=1.38 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=24.5s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=26.5s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=28.8s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=30.8s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=30.8s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=32.8s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=1.45 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=2.41 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=4.66 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=49.9s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=51.9s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=2.55 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.4s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=-1 load=0 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=56.4s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=0 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=58.8s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=60.8s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=2 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=63.1s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.1s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=0.33 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=0.93 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=46\n",
            "event: SPAWN\n",
            " Car0 floor=2.07 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=3.14 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.8s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.8s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=1.22 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=80.7s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=82.7s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=2 it=1\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.4s | waiting=48\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=89.4s | waiting=48\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=91.7s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=-1 load=3 it=1\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.7s | waiting=52\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=3 it=1\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=98.4s | waiting=52\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=100.4s | waiting=52\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=3 it=0\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 7\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=2.3s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=2 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.3s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=2 it=5\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=8\n",
            "event: SPAWN\n",
            " Car0 floor=0.81 dir=-1 load=2 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=14.2s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=1 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=16.2s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=1 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=18.5s | waiting=8\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.5s | waiting=8\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=9\n",
            "event: SPAWN\n",
            " Car0 floor=3.86 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=3.70 dir=-1 load=0 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=24.3s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=26.3s | waiting=15\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=33.0s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=20\n",
            "event: SPAWN\n",
            " Car0 floor=2.65 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.0s | waiting=20\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=39.0s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=50.7s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=52.7s | waiting=37\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=3.79 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.6s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=55.6s | waiting=44\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=62.6s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=64.6s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=1.53 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=2.13 dir=+1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=0.98 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=2.04 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=1.41 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=75.3s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.3s | waiting=46\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=8 it=2\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.3s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=8 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=8 it=5\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.3s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=8 it=4\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=84.0s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=86.0s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=2 it=2\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.0s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=50\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=95.0s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=97.3s | waiting=50\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=99.3s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=55\n",
            "event: SPAWN\n",
            " Car0 floor=3.76 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 8\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.90 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=7.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=9.0s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [5]\n",
            "----------------------------------------\n",
            "t=17.7s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=6 it=2\n",
            " Hall‑calls: [5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=3.37 dir=+1 load=6 it=4\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=3.22 dir=-1 load=6 it=0\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=28.7s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=30.7s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=12\n",
            "event: SPAWN\n",
            " Car0 floor=1.12 dir=+1 load=6 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.4s | waiting=12\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.4s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=2.56 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=39.4s | waiting=26\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=41.4s | waiting=26\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=33\n",
            "event: SPAWN\n",
            " Car0 floor=3.95 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=50.6s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=52.6s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=1.26 dir=+1 load=8 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.7s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=8 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=55.7s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=8 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=58.1s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=60.1s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=2 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=60.1s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=62.1s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=6 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=1.62 dir=+1 load=6 it=5\n",
            " Hall‑calls: [1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=2.22 dir=+1 load=6 it=4\n",
            " Hall‑calls: [1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=3.37 dir=+1 load=6 it=4\n",
            " Hall‑calls: [1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=71.4s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=2\n",
            " Hall‑calls: [1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.4s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=4 it=3\n",
            " Hall‑calls: [1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.4s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+0 load=7 it=2\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=75.4s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+0 load=7 it=5\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=2.50 dir=-1 load=7 it=2\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=84.7s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=86.7s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=91.4s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=-1 load=6 it=3\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=93.4s | waiting=45\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=6 it=3\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=98.1s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "t=100.1s | waiting=45\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [1, 2, 3, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 11\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=5\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.60 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.04 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=9.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=11.0s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=17.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=5\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=3.37 dir=+1 load=4 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=3.53 dir=+1 load=4 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=27.0s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.0s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=4.13 dir=-1 load=4 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=1.93 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=2.89 dir=+1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.3s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=45.3s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=52.3s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=-1 load=4 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.3s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=2.05 dir=+1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=59.0s | waiting=42\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=61.0s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=4 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=63.3s | waiting=42\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.3s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=2.78 dir=-1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.4s | waiting=45\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=68.4s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=2.33 dir=-1 load=4 it=2\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=49\n",
            "event: SPAWN\n",
            " Car0 floor=3.40 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=2.76 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.5s | waiting=50\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=76.5s | waiting=50\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=51\n",
            "event: SPAWN\n",
            " Car0 floor=4.04 dir=+1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=86.0s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=88.0s | waiting=49\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=8 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=92.7s | waiting=49\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=54\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=8 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=94.7s | waiting=54\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=8 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=97.0s | waiting=54\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=99.0s | waiting=54\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=8 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=103.4s | waiting=59\n",
            "event: SPAWN\n",
            " Car0 floor=3.88 dir=+1 load=8 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 0\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=0.0s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=2.0s | waiting=0\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=3\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.60 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.3s | waiting=4\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.3s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=2 it=0\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.3s | waiting=0\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: []\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=4\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [5]\n",
            "----------------------------------------\n",
            "t=8.3s | waiting=4\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=5\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=1.16 dir=+1 load=6 it=2\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=23.9s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=25.9s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=0\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=25.9s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=27.9s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=6 it=4\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=27.9s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=29.9s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=6 it=0\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=12\n",
            "event: SPAWN\n",
            " Car0 floor=1.47 dir=+1 load=6 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=32.1s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=34.1s | waiting=11\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=7 it=1\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=0.12 dir=-1 load=7 it=0\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.5s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=2 it=3\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.5s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=2 it=3\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=2.23 dir=+1 load=2 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=44.2s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=46.2s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=46.2s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=3 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=48.2s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=3 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=52.9s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.9s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=64.2s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.2s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=3.54 dir=-1 load=2 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=4.69 dir=+1 load=2 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=45\n",
            "event: SPAWN\n",
            " Car0 floor=3.62 dir=-1 load=2 it=0\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.3s | waiting=44\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=3 it=4\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=75.3s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=48\n",
            "event: SPAWN\n",
            " Car0 floor=2.46 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=82.3s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=84.3s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=4.85 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=102.3s | waiting=52\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 6\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=4.5s | waiting=2\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.5s | waiting=2\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=3\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=0.87 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=14.1s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=16.1s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=4 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.7s | waiting=6\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=22.7s | waiting=6\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=4 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=7\n",
            "event: SPAWN\n",
            " Car0 floor=1.80 dir=-1 load=4 it=0\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=13\n",
            "event: SPAWN\n",
            " Car0 floor=1.65 dir=-1 load=4 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=27.4s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.4s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.4s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.4s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=2.05 dir=+1 load=0 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=1.08 dir=-1 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=40.9s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.9s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.9s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=4 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=44.9s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=4 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=51.9s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.9s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=4 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.9s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+0 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+0 load=4 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=55.9s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+0 load=4 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=60.6s | waiting=36\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=62.6s | waiting=36\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=62.6s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=17 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=64.6s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=17 it=1\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=64.6s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=17 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=17 it=4\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.6s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=17 it=2\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=1.28 dir=+1 load=17 it=5\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.9s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=17 it=None\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=30\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=17 it=1\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=30\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=17 it=None\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=17 it=1\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=71.9s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=17 it=1\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=0.78 dir=-1 load=17 it=0\n",
            " Hall‑calls: [2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=1.41 dir=+1 load=17 it=5\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=36\n",
            "event: SPAWN\n",
            " Car0 floor=3.55 dir=+1 load=17 it=4\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.9s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=13 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=81.9s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=13 it=4\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=86.6s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=7 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=88.6s | waiting=35\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=7 it=3\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=90.9s | waiting=33\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=92.9s | waiting=33\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=7 it=2\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=2.85 dir=-1 load=7 it=1\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=98.3s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=3 it=None\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=100.3s | waiting=38\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=3 it=5\n",
            " Hall‑calls: [2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 21\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.90 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.5s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.5s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=3.28 dir=+1 load=0 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=3.13 dir=-1 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=28.5s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=30.5s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=30.5s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=12\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=6 it=5\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=32.5s | waiting=12\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=6 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=2.56 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=26\n",
            "event: SPAWN\n",
            " Car0 floor=1.60 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=39.3s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=41.3s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=2.99 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=48.3s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=50.3s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=5 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=3.80 dir=-1 load=5 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.6s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=4 it=4\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=55.6s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=4 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=58.0s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=60.0s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=64.6s | waiting=37\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=-1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.6s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=1.28 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=43\n",
            "event: SPAWN\n",
            " Car0 floor=2.43 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=3.49 dir=+1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.86 dir=-1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=75.9s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.9s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=6 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.9s | waiting=46\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=47\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=7 it=1\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.9s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=7 it=2\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=84.6s | waiting=47\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=86.6s | waiting=47\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=1 it=1\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=52\n",
            "event: SPAWN\n",
            " Car0 floor=2.88 dir=+1 load=1 it=4\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=97.7s | waiting=52\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=99.7s | waiting=52\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=1 it=0\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=102.0s | waiting=52\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=1 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 9\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.90 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=9.3s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=11.3s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=2\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=18.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.0s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=22.3s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=24.3s | waiting=17\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.0s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=-1 load=4 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=13\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=-1 load=4 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=1.01 dir=+1 load=4 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=33.4s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.4s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.4s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=+0 load=0 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.4s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=+0 load=0 it=3\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=0.44 dir=+1 load=0 it=4\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=39.5s | waiting=28\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=41.5s | waiting=28\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=0.95 dir=+1 load=0 it=5\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=43.8s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=45.8s | waiting=27\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=8 it=2\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=52.8s | waiting=27\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=30\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=34\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=1\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=54.8s | waiting=34\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=64.1s | waiting=34\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=6 it=5\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.1s | waiting=37\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=0.49 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=1.64 dir=+1 load=6 it=3\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=70.8s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=40\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=5 it=4\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.8s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=5 it=2\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.8s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+0 load=6 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=42\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+0 load=6 it=1\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=74.8s | waiting=42\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+0 load=6 it=3\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=77.1s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=41\n",
            "event: SPAWN\n",
            " Car0 floor=3.00 dir=+1 load=6 it=2\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.1s | waiting=41\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=6 it=2\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.1s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=81.1s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+0 load=7 it=0\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=81.1s | waiting=40\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=83.1s | waiting=40\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+0 load=7 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=85.5s | waiting=39\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=87.5s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=+1 load=6 it=4\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=44\n",
            "event: SPAWN\n",
            " Car0 floor=1.49 dir=-1 load=6 it=0\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=96.8s | waiting=44\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=5 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=98.8s | waiting=44\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=5 it=5\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=101.1s | waiting=41\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 1, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 13\n",
            "Using cuda:0 device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "t=3.4s | waiting=6\n",
            "event: SPAWN\n",
            " Car0 floor=1.46 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1]\n",
            "----------------------------------------\n",
            "t=6.8s | waiting=10\n",
            "event: SPAWN\n",
            " Car0 floor=2.90 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=7.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=9.0s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=3.00 dir=+1 load=0 it=4\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=13.7s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=15.7s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=18.0s | waiting=10\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=20.0s | waiting=10\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=0 it=3\n",
            " Hall‑calls: [1, 5]\n",
            "----------------------------------------\n",
            "t=23.2s | waiting=11\n",
            "event: SPAWN\n",
            " Car0 floor=2.63 dir=-1 load=0 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=23.6s | waiting=17\n",
            "event: SPAWN\n",
            " Car0 floor=2.47 dir=-1 load=0 it=2\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=29.3s | waiting=17\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.0s | waiting=18\n",
            "event: SPAWN\n",
            " Car0 floor=0.00 dir=-1 load=0 it=1\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=31.3s | waiting=18\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=0 it=3\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=33.7s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=2 it=None\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.7s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=2 it=4\n",
            " Hall‑calls: [1, 2, 5]\n",
            "----------------------------------------\n",
            "t=35.7s | waiting=11\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [2, 5]\n",
            "----------------------------------------\n",
            "t=36.2s | waiting=15\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=1\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.7s | waiting=15\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=7 it=1\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=37.7s | waiting=15\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=None\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=38.4s | waiting=25\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+0 load=7 it=2\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=39.7s | waiting=25\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+0 load=7 it=1\n",
            " Hall‑calls: [0, 2, 5]\n",
            "----------------------------------------\n",
            "t=42.0s | waiting=24\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=None\n",
            " Hall‑calls: [0, 5]\n",
            "----------------------------------------\n",
            "t=43.7s | waiting=31\n",
            "event: SPAWN\n",
            " Car0 floor=2.00 dir=+1 load=8 it=0\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=44.0s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=8 it=4\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=48.7s | waiting=31\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=3 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=50.7s | waiting=31\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=3 it=2\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.0s | waiting=25\n",
            "event: OPEN\n",
            " Car0 floor=1.00 dir=+1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=53.1s | waiting=28\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=9 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=54.5s | waiting=32\n",
            "event: SPAWN\n",
            " Car0 floor=1.00 dir=+1 load=9 it=3\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=55.0s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=1.00 dir=+1 load=9 it=4\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=57.3s | waiting=32\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=+1 load=7 it=None\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=59.3s | waiting=32\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 5]\n",
            "----------------------------------------\n",
            "t=65.9s | waiting=35\n",
            "event: SPAWN\n",
            " Car0 floor=4.80 dir=+1 load=7 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=66.3s | waiting=35\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=67.3s | waiting=37\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=5\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=68.3s | waiting=37\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=69.9s | waiting=38\n",
            "event: SPAWN\n",
            " Car0 floor=4.31 dir=-1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=71.6s | waiting=38\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=None\n",
            " Hall‑calls: [0, 1, 4, 5]\n",
            "----------------------------------------\n",
            "t=72.4s | waiting=39\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.6s | waiting=39\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+1 load=4 it=1\n",
            " Hall‑calls: [0, 1, 2, 4, 5]\n",
            "----------------------------------------\n",
            "t=73.6s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=5.00 dir=+0 load=30 it=None\n",
            " Hall‑calls: [0, 1, 2, 4]\n",
            "----------------------------------------\n",
            "t=73.9s | waiting=16\n",
            "event: SPAWN\n",
            " Car0 floor=5.00 dir=+0 load=30 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=75.6s | waiting=16\n",
            "event: CLOSE\n",
            " Car0 floor=5.00 dir=+0 load=30 it=3\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=77.9s | waiting=13\n",
            "event: OPEN\n",
            " Car0 floor=4.00 dir=-1 load=25 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4]\n",
            "----------------------------------------\n",
            "t=78.9s | waiting=14\n",
            "event: SPAWN\n",
            " Car0 floor=4.00 dir=-1 load=25 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=79.9s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=4.00 dir=-1 load=25 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=84.6s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=2.00 dir=-1 load=18 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=86.6s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=2.00 dir=-1 load=18 it=0\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=91.2s | waiting=14\n",
            "event: OPEN\n",
            " Car0 floor=0.00 dir=-1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.2s | waiting=14\n",
            "event: CLOSE\n",
            " Car0 floor=0.00 dir=-1 load=9 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=93.3s | waiting=19\n",
            "event: SPAWN\n",
            " Car0 floor=0.03 dir=+1 load=9 it=2\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "t=100.2s | waiting=16\n",
            "event: OPEN\n",
            " Car0 floor=3.00 dir=+1 load=6 it=None\n",
            " Hall‑calls: [0, 1, 2, 3, 4, 5]\n",
            "----------------------------------------\n",
            "info[\"done\"]: 40\n",
            "[846.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Policy-Gradient **with a baseline**\n",
        "\n",
        "* **Signal used:** $g_t = \\nabla_\\theta\\log\\pi_\\theta(a_t|s_t)\\,[R_t - b]$\n",
        "  \\*$R_t$ is the **full Monte-Carlo return** from $t$ to the end; $b$ is a baseline (often a moving average or a learned **state-value $V_\\phi(s_t)$**).\n",
        "* **Variance control:** The baseline subtracts a constant (or $V_\\phi$) to reduce variance, but the target is **still the complete return**, so the method remains Monte-Carlo.\n",
        "* **When it breaks:** Long horizons ⇒ high variance and slow learning; still no explicit constraint on step size.\n",
        "\n",
        "---\n",
        "\n",
        "### **A2C – Advantage Actor-Critic (synchronous)**\n",
        "\n",
        "* **Signal used:** $g_t = \\nabla_\\theta\\log\\pi_\\theta(a_t|s_t)\\,[\\,R_t^{(\\lambda)} - V_\\phi(s_t)\\,]$\n",
        "  where $R_t^{(\\lambda)}$ is an **n-step / GAE bootstrapped target**.\n",
        "* **What’s new vs PG + baseline:**\n",
        "\n",
        "  1. **Bootstrapping**: mixes immediate rewards with the critic’s estimate instead of waiting for the episode to finish ⇒ much lower variance & quicker feedback.\n",
        "  2. **Multiple workers, one update:** “*A2C*” is simply A3C run **synchronously**; workers collect batches, then the central learner averages the gradients—hardware-friendly and stable.\n",
        "* **Limitation:** Still a pure gradient step; if the learning rate is too high the policy can move far and collapse.\n",
        "\n",
        "---\n",
        "\n",
        "### **PPO – Proximal Policy Optimisation**\n",
        "\n",
        "* **Signal used:** same advantage estimate as A2C, but the loss is\n",
        "\n",
        "  $$\n",
        "  L(\\theta)=\\mathbb{E}\\big[\\;\n",
        "  \\min\\big(r_t(\\theta)A_t,\\;\\text{clip}(r_t(\\theta),1\\!\\pm\\!\\epsilon)\\,A_t\\big)\\big]\n",
        "  $$\n",
        "\n",
        "  with $r_t=\\pi_\\theta/\\pi_{\\theta_{\\text{old}}}$.\n",
        "* **What’s new vs A2C:**\n",
        "\n",
        "  1. **Trust-region by clipping (or KL penalty):** prevents $r_t$ from straying too far from 1, so each update is **safely bounded**.\n",
        "  2. **Large-batch optimisation:** can reuse the same trajectories for multiple epochs, making PPO markedly **more sample-efficient**.\n",
        "* **Result:** Typically the most robust of the three—good performance with minimal tuning.\n",
        "\n",
        "---\n",
        "\n",
        "#### Quick takeaway\n",
        "\n",
        "| Method        | Variance control                              | Bootstrapping   | Step-size safeguard | Typical stability |\n",
        "| ------------- | --------------------------------------------- | --------------- | ------------------- | ----------------- |\n",
        "| PG + baseline | subtract baseline                             | ✗ (full return) | ✗                   | lowest            |\n",
        "| A2C           | subtract baseline **and** bootstrapped n-step | ✓               | ✗                   | moderate          |\n",
        "| PPO           | same as A2C                                   | ✓               | ✓ (clip/KL)         | highest           |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J6WbthuG5PPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generalized Advantage Estimation (GAE) inside PPO\n",
        "\n",
        "1. **Goal — a low-variance, low-bias “advantage”**\n",
        "   PPO’s policy-loss needs an estimate $A_t$ of how much better (or worse) an action was than the critic’s prediction.\n",
        "\n",
        "   * Full-trajectory returns ⇒ low bias, **high variance**\n",
        "   * 1-step TD errors ⇒ low variance, **high bias**\n",
        "     GAE smoothly interpolates between the two with a single hyper-parameter $\\lambda\\in[0,1]$.\n",
        "\n",
        "2. **Compute the *temporal-difference residuals***\n",
        "\n",
        "$$\n",
        "\\delta_t = r_t + \\gamma\\,V_{\\phi}(s_{t+1}) - V_{\\phi}(s_t)\n",
        "$$\n",
        "\n",
        "3. **Exponentially-weighted TD-sum**\n",
        "\n",
        "$$\n",
        "A^{(\\gamma,\\lambda)}_t\n",
        "= \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^{\\,l}\\;\\delta_{t+l}\n",
        "$$\n",
        "\n",
        "* If $\\lambda=1$ you get the Monte-Carlo (n-step → ∞) advantage.\n",
        "* If $\\lambda=0$ you get the 1-step TD residual.\n",
        "\n",
        "4. **Efficient backward recursion (what you code)**\n",
        "\n",
        "```python\n",
        "adv = 0\n",
        "for t in reversed(range(T)):\n",
        "    delta = r[t] + γ * V[t+1] - V[t]\n",
        "    adv   = delta + γ * λ * adv\n",
        "    gae[t] = adv\n",
        "```\n",
        "\n",
        "5. **Normalization (optional but standard)**\n",
        "   GAE values are often **whitened** (subtract mean, divide by std) before entering the PPO clipped objective to keep magnitudes comparable across batches.\n",
        "\n",
        "6. **Why it fits PPO so well**\n",
        "\n",
        "| Property                       | Benefit for PPO                                                                                                              |\n",
        "| ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Single λ knob**              | Lets you dial bias–variance without extra structures.                                                                        |\n",
        "| **Bootstrapping** (when λ < 1) | Shorter credit-assignment paths ⇒ can reuse trajectories for **multiple epochs**, which PPO relies on for sample efficiency. |\n",
        "| **Smooth estimates**           | Clipped ratio $r_t(\\theta)$ · $A_t$ is less jittery, so the clip threshold ε meaningfully controls step size.                |\n",
        "\n",
        "**Typical hyper-parameters**\n",
        "\n",
        "* γ = 0.99 (discount)\n",
        "* λ = 0.95 (good bias–variance trade-off for many continuous-control tasks)\n",
        "\n",
        "---\n",
        "\n",
        "**In short:** GAE feeds PPO a *smoothed* advantage computed as an exponentially-weighted sum of TD errors; the λ parameter decides how Monte-Carlo-like or TD-like the signal is. This keeps variance low enough for PPO’s multiple-epoch updates while introducing only a small, tunable bias.\n"
      ],
      "metadata": {
        "id": "zlmvRdS76HJp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bhu-1Ovn6Hps"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}